{"overall": {"Bleu_1": 0.2612668524859011, "Bleu_2": 0.17749918711790802, "Bleu_3": 0.11489623404780092, "Bleu_4": 0.07425067452408537, "METEOR": 0.2171189987720795, "ROUGE_L": 0.2576576927395042, "CIDEr": 2.590891827494705e-05, "SPICE": 0.18407169805933518}, "imgToEval": {"281533": {"image_id": 281533, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.221323371633511, "Bleu_3": 0.15662032595896783, "Bleu_4": 0.12321792904178375, "METEOR": 0.2917892738425711, "ROUGE_L": 0.2959369314736203, "CIDEr": 3.053338303106672e-10, "SPICE": {"All": {"pr": 0.22857142857142856, "re": 0.36363636363636365, "f": 0.2807017543859649, "fn": 14.0, "numImages": 1.0, "fp": 27.0, "tp": 8.0}, "Relation": {"pr": 0.07142857142857142, "re": 0.09090909090909091, "f": 0.08, "fn": 10.0, "numImages": 1.0, "fp": 13.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.5, "f": 0.22222222222222224, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.6666666666666666, "f": 0.5217391304347826, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The image shows a dog sitting in front of a television. The dog is looking at the television with its ears perked up. The television is showing a nature documentary with a herd of deer running across the screen. The room is decorated with plants and a rug.\n\nThe dog is a golden"}, "397773": {"image_id": 397773, "Bleu_1": 0.22727272726756204, "Bleu_2": 0.16256402218685123, "Bleu_3": 0.08569060201339629, "Bleu_4": 1.1130224679490435e-05, "METEOR": 0.20531239887113278, "ROUGE_L": 0.29985955056179775, "CIDEr": 1.6913525369968577e-07, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.23076923076923078, "f": 0.24489795918367346, "fn": 20.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.1, "re": 0.14285714285714285, "f": 0.11764705882352941, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.1, "f": 0.15384615384615383, "fn": 9.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.4444444444444444, "f": 0.4210526315789474, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "This image shows a bowl of pasta with meatballs and broccoli. The pasta is cooked and has a golden brown color. The meatballs are made of ground beef and have a crispy exterior. The broccoli is steamed and has a green color. The bow"}, "371250": {"image_id": 371250, "Bleu_1": 0.1960784313687044, "Bleu_2": 0.0885614885522557, "Bleu_3": 5.429559270864282e-07, "Bleu_4": 1.351335308585343e-09, "METEOR": 0.18076969799852258, "ROUGE_L": 0.24702071040148096, "CIDEr": 3.3085991581711767e-10, "SPICE": {"All": {"pr": 0.22580645161290322, "re": 0.28, "f": 0.25, "fn": 18.0, "numImages": 1.0, "fp": 24.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.16666666666666666, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.6, "f": 0.4615384615384615, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 6.0}}, "caption": "This is a living room with a couch, coffee table, and television. The couch is brown and has two pillows on it. The coffee table has a lamp on it and there is a remote control on the table. The television is on and there is a cushion on the couch."}, "573877": {"image_id": 573877, "Bleu_1": 0.2727272727223141, "Bleu_2": 0.1880253582691385, "Bleu_3": 0.13869867869725044, "Bleu_4": 0.10064933408714859, "METEOR": 0.2852324155881566, "ROUGE_L": 0.22297650130548302, "CIDEr": 7.795281056311326e-11, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.22727272727272727, "f": 0.2, "fn": 17.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5, "f": 0.380952380952381, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The cat is sitting on the laptop, looking at the screen.\n\nThe laptop is open and the cat is looking at the screen. The laptop is on a desk in front of a window. The window is open and there is a view of the outside. The cat is white and has blue eyes. The"}, "181859": {"image_id": 181859, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.22117152024581002, "Bleu_3": 0.09798321550063377, "Bleu_4": 1.1653898241136168e-05, "METEOR": 0.24617212936929816, "ROUGE_L": 0.24710648148148148, "CIDEr": 1.0717506538255631e-11, "SPICE": {"All": {"pr": 0.08, "re": 0.10526315789473684, "f": 0.0909090909090909, "fn": 17.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.1, "re": 0.16666666666666666, "f": 0.125, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}}, "caption": "The image is of a cat lying on a sink in a bathroom. The cat is gray and white with a pink nose and green eyes. The sink is made of white marble and has a faucet on the right side. There is a towel hanging on the left side of the sink and"}, "119939": {"image_id": 119939, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.11215443081631235, "Bleu_3": 0.06230790884875894, "Bleu_4": 8.29879120277201e-06, "METEOR": 0.1650224215246637, "ROUGE_L": 0.1737891737891738, "CIDEr": 1.6966503046589084e-12, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.3076923076923077, "f": 0.2285714285714286, "fn": 9.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3, "re": 0.5, "f": 0.37499999999999994, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a group of people standing on the sidewalk in front of a building. They are all wearing black and white clothing and have skateboards in their hands. The building is a large, modern structure with large windows and a large sign on the front that reads `The Building'. The street is"}, "385320": {"image_id": 385320, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.1927248223279925, "Bleu_3": 0.14573527583425525, "Bleu_4": 0.1185575905830453, "METEOR": 0.27967917475572174, "ROUGE_L": 0.2901307966706302, "CIDEr": 1.072677887459634e-09, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.23809523809523808, "f": 0.20408163265306123, "fn": 16.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.2857142857142857, "f": 0.26666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.6, "f": 0.33333333333333337, "fn": 2.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a young girl sitting on a couch, holding a toothbrush in her mouth. She is wearing a striped shirt and white shorts. The room appears to be a bedroom, with a bed in the background and a window in the background. The girl is looking directly at"}, "490415": {"image_id": 490415, "Bleu_1": 0.1730769230735947, "Bleu_2": 0.13026252642719696, "Bleu_3": 0.06975194623395738, "Bleu_4": 9.122591486542633e-06, "METEOR": 0.20207424054223436, "ROUGE_L": 0.26293103448275856, "CIDEr": 2.258769039386878e-11, "SPICE": {"All": {"pr": 0.10526315789473684, "re": 0.06896551724137931, "f": 0.08333333333333333, "fn": 27.0, "numImages": 1.0, "fp": 17.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 17.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.2222222222222222, "f": 0.25, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The image shows a man standing in a park, holding a kite and looking up at the sky. The sky is clear and blue, with a few white clouds scattered across it. The man is wearing a black shirt and jeans, and has a backpack slung over his shoulder. The grass is"}, "432293": {"image_id": 432293, "Bleu_1": 0.2765957446749661, "Bleu_2": 0.20516007603198386, "Bleu_3": 0.09779674410658105, "Bleu_4": 1.2074805060760116e-05, "METEOR": 0.2279542076485407, "ROUGE_L": 0.31063017186505404, "CIDEr": 4.8673687349073675e-09, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.12, "f": 0.12499999999999997, "fn": 22.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.1111111111111111, "f": 0.125, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2222222222222222, "f": 0.19999999999999998, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a pizza on a wooden cutting board with various toppings such as shrimp, tomatoes, and cheese. There is a knife on the board and a spatula nearby. The pizza appears to be almost finished, with the crust being rolled out and the toppings being"}, "256301": {"image_id": 256301, "Bleu_1": 0.32142857142283165, "Bleu_2": 0.21622499104303794, "Bleu_3": 0.13746108160935094, "Bleu_4": 0.0836692450235854, "METEOR": 0.20495581765453108, "ROUGE_L": 0.20795454545454545, "CIDEr": 9.179660725656066e-11, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.1724137931034483, "f": 0.18867924528301888, "fn": 24.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.1, "re": 0.08333333333333333, "f": 0.0909090909090909, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.36363636363636365, "f": 0.36363636363636365, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a large neon sign with the words \"public market\" written in red letters on a black background. The sign is surrounded by people standing on the sidewalk, looking at it. The sign is located in front of a building with a large window on the side. The building appears to be a restaurant"}, "361103": {"image_id": 361103, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.161880977056518, "Bleu_3": 0.07957856377253618, "Bleu_4": 9.970215112770837e-06, "METEOR": 0.1809253219452407, "ROUGE_L": 0.24710648148148148, "CIDEr": 2.1534169437557835e-08, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.21739130434782608, "f": 0.21739130434782608, "fn": 18.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.14285714285714285, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.6666666666666666, "f": 0.4444444444444444, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a street with tall buildings on either side. There are people walking on the sidewalk and cars driving down the street. The buildings are made of brick and have large windows on the upper floors. There are also streetlights on the corners of the street.\n\nThe image is taken from the"}, "567562": {"image_id": 567562, "Bleu_1": 0.19999999999600004, "Bleu_2": 0.11065666703226194, "Bleu_3": 0.0799063530041829, "Bleu_4": 0.05739995047256522, "METEOR": 0.20300681325775677, "ROUGE_L": 0.22235722964763066, "CIDEr": 3.2696630252916707e-09, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.19047619047619047, "f": 0.19047619047619047, "fn": 17.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.125, "f": 0.15384615384615385, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.42857142857142855, "f": 0.375, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a group of people sitting around a table eating pizza and drinking soda. The table is made of wood and has a white tablecloth on it. There are several chairs around the table, and the people are all wearing casual clothing. The walls in the background are"}, "448320": {"image_id": 448320, "Bleu_1": 0.31372549018992696, "Bleu_2": 0.23763541030969573, "Bleu_3": 0.15121069009849425, "Bleu_4": 0.09212480089236008, "METEOR": 0.25826225982866613, "ROUGE_L": 0.3285457809694794, "CIDEr": 5.4668399972174146e-09, "SPICE": {"All": {"pr": 0.12, "re": 0.10714285714285714, "f": 0.1132075471698113, "fn": 25.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.25, "f": 0.2727272727272727, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image is of a bathroom with a sink, toilet, and mirror. The sink is made of wood and has a large bowl in the center. The toilet is a standard toilet with a seat and lid. The mirror is mounted on the wall above the sink and has a light"}, "14874": {"image_id": 14874, "Bleu_1": 0.27999999999440006, "Bleu_2": 0.19999999999595922, "Bleu_3": 0.14938015821552278, "Bleu_4": 0.12077457989654793, "METEOR": 0.2628846424089891, "ROUGE_L": 0.30118144947980957, "CIDEr": 4.1949084036839816e-10, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.21052631578947367, "f": 0.1904761904761905, "fn": 15.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.6, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a man standing in the snow with skis on his feet. He is wearing a blue jacket and black pants, and has a pair of goggles on his face. The background is a mountain range with snow covered peaks and trees.\n\nThe image is in black and"}, "373713": {"image_id": 373713, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.1447907475849832, "Bleu_3": 0.10654502541910188, "Bleu_4": 0.0829879120277201, "METEOR": 0.19264253315757746, "ROUGE_L": 0.21863799283154117, "CIDEr": 9.310415898446529e-13, "SPICE": {"All": {"pr": 0.25, "re": 0.18181818181818182, "f": 0.2105263157894737, "fn": 18.0, "numImages": 1.0, "fp": 12.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.16666666666666666, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.375, "re": 0.42857142857142855, "f": 0.39999999999999997, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a group of people sitting around a table in a room. They are all wearing black shirts and jeans, and one of them is holding a laptop. The room is dimly lit, and there are several computers and other electronic devices on the table. The people are all looking at the"}, "539326": {"image_id": 539326, "Bleu_1": 0.19999999999600004, "Bleu_2": 0.18070158057739938, "Bleu_3": 0.16823908657056308, "Bleu_4": 0.15702128401926801, "METEOR": 0.37776039641035014, "ROUGE_L": 0.34879288437102923, "CIDEr": 1.0491004707590501e-10, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.21428571428571427, "f": 0.15, "fn": 11.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.5, "f": 0.3529411764705882, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a herd of cows grazing in a green field. The cows are brown and white and have long, curly horns. They are standing in a line, with their heads down and their tails swishing back and forth. In the background, there is a hill with trees on"}, "20059": {"image_id": 20059, "Bleu_1": 0.21999999999560005, "Bleu_2": 0.14982983544985165, "Bleu_3": 0.09779765794978403, "Bleu_4": 1.1877413502967068e-05, "METEOR": 0.19009207908100145, "ROUGE_L": 0.23252858958068615, "CIDEr": 1.1243060980919353e-10, "SPICE": {"All": {"pr": 0.375, "re": 0.0967741935483871, "f": 0.15384615384615383, "fn": 28.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}, "Relation": {"pr": 1.0, "re": 0.1111111111111111, "f": 0.19999999999999998, "fn": 8.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.2, "f": 0.26666666666666666, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}}, "caption": "The image shows a group of zebras grazing in a grassy area. They are standing in a clearing surrounded by trees and rocks. The zebras are black and white with long manes and tails. They are standing in a line, looking at each other. The grass is green and the"}, "530520": {"image_id": 530520, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.18890811128268284, "Bleu_3": 0.14281978481313226, "Bleu_4": 0.0882634647352183, "METEOR": 0.17139006587083444, "ROUGE_L": 0.22889305816135083, "CIDEr": 1.99868484970243e-11, "SPICE": {"All": {"pr": 0.1, "re": 0.125, "f": 0.11111111111111112, "fn": 21.0, "numImages": 1.0, "fp": 27.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.375, "f": 0.2857142857142857, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a group of people sitting on the grass outside a building, using laptops and other devices. They are all wearing casual clothing and appear to be working on their laptops. The building in the background is a multi-story structure with windows and a flat roof. The image is"}, "117337": {"image_id": 117337, "Bleu_1": 0.16981132075151303, "Bleu_2": 0.11429089766174176, "Bleu_3": 0.08001308872460662, "Bleu_4": 0.05657548369713708, "METEOR": 0.17830276125440744, "ROUGE_L": 0.17722254503195814, "CIDEr": 6.633217414010692e-11, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.19047619047619047, "f": 0.16326530612244897, "fn": 17.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.6666666666666666, "f": 0.4444444444444444, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "This is a collage of images that show different colors and patterns. The images are arranged in a collage style, with different colors and patterns. The images are arranged in a collage style, with different colors and patterns. The images are arranged in a collage style, with different colors and patterns. The images"}, "256504": {"image_id": 256504, "Bleu_1": 0.31249999999348965, "Bleu_2": 0.19973386548003494, "Bleu_3": 0.15138000825437872, "Bleu_4": 0.12331859801407646, "METEOR": 0.2303974777931056, "ROUGE_L": 0.31282051282051276, "CIDEr": 1.024158736445461e-09, "SPICE": {"All": {"pr": 0.12903225806451613, "re": 0.13793103448275862, "f": 0.13333333333333333, "fn": 25.0, "numImages": 1.0, "fp": 27.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.4, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows two people sitting on a bed with laptops in front of them. The bed is covered in a colorful tapestry with a large mandala design on it. The people are both wearing pajamas and are looking at their laptops. The room is dimly lit and"}, "265472": {"image_id": 265472, "Bleu_1": 0.20454545454080583, "Bleu_2": 0.18247766233173443, "Bleu_3": 0.14691857457136862, "Bleu_4": 0.09378028870453133, "METEOR": 0.22057662252930274, "ROUGE_L": 0.2636887608069164, "CIDEr": 6.651609204542462e-08, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.2777777777777778, "f": 0.21739130434782608, "fn": 13.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3076923076923077, "re": 0.6666666666666666, "f": 0.42105263157894735, "fn": 2.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a plate of food with bacon, bananas, and bacon. The bacon is cooked and crispy, while the bananas are sliced and placed on top of the bacon. The plate is on a white tablecloth, and there is a fork and knife"}, "441083": {"image_id": 441083, "Bleu_1": 0.2264150943353507, "Bleu_2": 0.18663625434394557, "Bleu_3": 0.08806579930933402, "Bleu_4": 1.0810927622781055e-05, "METEOR": 0.20227721301231358, "ROUGE_L": 0.2334609075997813, "CIDEr": 8.462892543209724e-10, "SPICE": {"All": {"pr": 0.13793103448275862, "re": 0.16666666666666666, "f": 0.1509433962264151, "fn": 20.0, "numImages": 1.0, "fp": 25.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.09090909090909091, "re": 0.16666666666666666, "f": 0.11764705882352942, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The dog is sitting in the back seat of a car, looking out the window. The car is parked on the side of the road, and there are trees and houses visible in the background. The dog's collar is visible, and it appears to be a small breed. The car's wind"}, "126958": {"image_id": 126958, "Bleu_1": 0.2727272727223141, "Bleu_2": 0.1880253582691385, "Bleu_3": 0.08737469244163576, "Bleu_4": 1.0642371354817975e-05, "METEOR": 0.1862555268418366, "ROUGE_L": 0.26478567552902876, "CIDEr": 6.38553278767666e-11, "SPICE": {"All": {"pr": 0.2413793103448276, "re": 0.30434782608695654, "f": 0.2692307692307692, "fn": 16.0, "numImages": 1.0, "fp": 22.0, "tp": 7.0}, "Relation": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4444444444444444, "f": 0.3636363636363637, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a room with a blue wall, a white ceiling, and a white floor. There is a mirror on the wall opposite the door, and a window on the left side of the room. The room is empty except for a small table with a vase on it. The vase has a small"}, "484075": {"image_id": 484075, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.16496470132732644, "Bleu_3": 0.1169798453599748, "Bleu_4": 0.08945369410433844, "METEOR": 0.21651722461147815, "ROUGE_L": 0.3100818977689918, "CIDEr": 1.343360696088902e-10, "SPICE": {"All": {"pr": 0.4090909090909091, "re": 0.47368421052631576, "f": 0.43902439024390244, "fn": 10.0, "numImages": 1.0, "fp": 13.0, "tp": 9.0}, "Relation": {"pr": 0.3333333333333333, "re": 0.25, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5555555555555556, "f": 0.5263157894736842, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "The image is of a desk with a computer, keyboard, mouse, and other office supplies on it. The desk is made of wood and has a small lamp on it. There is a window in the background with a view of the city. The room is cluttered with various items such as books,"}, "274528": {"image_id": 274528, "Bleu_1": 0.2826086956460303, "Bleu_2": 0.19411635536098795, "Bleu_3": 0.11964664509646934, "Bleu_4": 1.4127271129810476e-05, "METEOR": 0.1806291498874332, "ROUGE_L": 0.2367399741267788, "CIDEr": 2.474414368460413e-08, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.14285714285714285, "f": 0.14814814814814817, "fn": 24.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.36363636363636365, "f": 0.36363636363636365, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a group of people standing in front of a truck with a banner on it that reads \"the race is on\"\n\nThe people in the image are wearing cycling jerseys and helmets, and some of them are holding bicycles. The truck has a"}, "286820": {"image_id": 286820, "Bleu_1": 0.18181818181487608, "Bleu_2": 0.1297498240245396, "Bleu_3": 0.068230605987166, "Bleu_4": 8.840637545603441e-06, "METEOR": 0.14111788073975587, "ROUGE_L": 0.17579250720461098, "CIDEr": 8.078291166058013e-14, "SPICE": {"All": {"pr": 0.25, "re": 0.4, "f": 0.3076923076923077, "fn": 9.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.5, "f": 0.36363636363636365, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.4, "re": 1.0, "f": 0.5714285714285715, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Object": {"pr": 0.3, "re": 0.6, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows two iPhones on a green carpet. One is white and the other is red. The white iPhone has a black screen and the red iPhone has a white screen. The white iPhone has a black bezel and the red iPhone has a white bezel. The white iPhone has a black home button"}, "69236": {"image_id": 69236, "Bleu_1": 0.2181818181778513, "Bleu_2": 0.08989331499344942, "Bleu_3": 5.342275830329977e-07, "Bleu_4": 1.3085607656499974e-09, "METEOR": 0.13592832197978322, "ROUGE_L": 0.17951736315479697, "CIDEr": 2.5867392261353783e-13, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.10714285714285714, "f": 0.12244897959183672, "fn": 25.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3, "f": 0.3, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a park at night with a bench and trees. The sky is clear and there are no clouds. The park is well lit with streetlights and there are no people in the image. The bench is made of metal and has a backrest. The trees are tall and have leaves on them"}, "333237": {"image_id": 333237, "Bleu_1": 0.3599999999928, "Bleu_2": 0.2842821248818621, "Bleu_3": 0.21617379142815762, "Bleu_4": 0.17123510841196515, "METEOR": 0.3051705296120279, "ROUGE_L": 0.38255416191562136, "CIDEr": 7.593317170402739e-09, "SPICE": {"All": {"pr": 0.08108108108108109, "re": 0.15, "f": 0.10526315789473685, "fn": 17.0, "numImages": 1.0, "fp": 34.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.375, "f": 0.2727272727272727, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image is a bedroom with a bed, a dresser, and a window. The walls are covered in red and white striped wallpaper. There is a white bedspread on the bed and a white dresser with a mirror on it. The window has white curtains and a white table with"}, "285258": {"image_id": 285258, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.13736056394607243, "Bleu_3": 0.07178791139239354, "Bleu_4": 9.274617069102212e-06, "METEOR": 0.1955832167110596, "ROUGE_L": 0.21721068249258166, "CIDEr": 3.3761970222381994e-12, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.21428571428571427, "f": 0.21052631578947367, "fn": 22.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.1, "f": 0.0909090909090909, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.38461538461538464, "f": 0.38461538461538464, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a group of dogs playing in a grassy area. The dogs are of different breeds and sizes, and they are all wearing collars and leashes. The dogs are standing in a line, with one dog in the front and the others behind it. The dogs are all looking at each"}, "574454": {"image_id": 574454, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.15357377920556234, "Bleu_3": 0.0974311856710336, "Bleu_4": 0.06558144622470677, "METEOR": 0.1324490181063188, "ROUGE_L": 0.18908865468071917, "CIDEr": 1.1964256291475027e-12, "SPICE": {"All": {"pr": 0.13793103448275862, "re": 0.21052631578947367, "f": 0.16666666666666666, "fn": 15.0, "numImages": 1.0, "fp": 25.0, "tp": 4.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.14285714285714285, "f": 0.1111111111111111, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.42857142857142855, "f": 0.2857142857142857, "fn": 4.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a group of people on the beach, with kite surfers in the foreground and Table Mountain in the background. The sky is clear and blue, with a few clouds in the distance. The waves are crashing on the shore, and there are some rocks and sand in the foreground. The"}, "57703": {"image_id": 57703, "Bleu_1": 0.29787234041919425, "Bleu_2": 0.24141120717629194, "Bleu_3": 0.1730297489554192, "Bleu_4": 0.12387535527350019, "METEOR": 0.2661286530056607, "ROUGE_L": 0.2848565710473649, "CIDEr": 1.2864233007174085e-08, "SPICE": {"All": {"pr": 0.12, "re": 0.2, "f": 0.15, "fn": 12.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.5, "f": 0.3529411764705882, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a group of people standing in a forest with their dogs. They are all wearing hiking boots and backpacks, and one person is holding a leash. The dogs are all different breeds and sizes, and they are all wearing collars and leashes. The forest"}, "70294": {"image_id": 70294, "Bleu_1": 0.18965517241052324, "Bleu_2": 0.1631510643898356, "Bleu_3": 0.07804239677504014, "Bleu_4": 9.641781305219008e-06, "METEOR": 0.2318892492259866, "ROUGE_L": 0.25539041239271504, "CIDEr": 3.224909507614565e-13, "SPICE": {"All": {"pr": 0.07407407407407407, "re": 0.08, "f": 0.07692307692307691, "fn": 23.0, "numImages": 1.0, "fp": 25.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2222222222222222, "f": 0.23529411764705882, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows a blue bus parked on the side of a road. The bus has a large window on the front and a small window on the side. There are no people in the bus. The bus is parked in front of a building with a large window on the front. The building has a large sign"}, "279769": {"image_id": 279769, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.20866567588950136, "Bleu_3": 0.15059039854381842, "Bleu_4": 0.09090870283563952, "METEOR": 0.21801227958910283, "ROUGE_L": 0.24811156304474144, "CIDEr": 1.74378844251893e-11, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.3125, "f": 0.2380952380952381, "fn": 11.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.5, "f": 0.25, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.8, "f": 0.5333333333333333, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a banana on a table with a piece of paper on top of it. The banana is peeled and has a few brown spots on it. The paper has a few notes written on it. There are also some other objects on the table, including a pen and a pencil"}, "541474": {"image_id": 541474, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.11895773785545558, "Bleu_3": 6.522364605376366e-07, "Bleu_4": 1.5348336186916813e-09, "METEOR": 0.2090821373281298, "ROUGE_L": 0.21721068249258166, "CIDEr": 2.0876125154874283e-12, "SPICE": {"All": {"pr": 0.25, "re": 0.2608695652173913, "f": 0.2553191489361702, "fn": 17.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.125, "re": 0.1, "f": 0.11111111111111112, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.5555555555555556, "f": 0.4545454545454546, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a person skiing down a snowy slope. The person is wearing a blue jacket and black pants, and has a black helmet on their head. They are holding onto a snowboard and are in the process of turning. The sky is blue and there are some clouds in the background"}, "217561": {"image_id": 217561, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.10301575072546125, "Bleu_3": 6.04674993673567e-07, "Bleu_4": 1.4727093431133163e-09, "METEOR": 0.11234211381867758, "ROUGE_L": 0.18944099378881987, "CIDEr": 4.587470242517544e-10, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.21428571428571427, "f": 0.21428571428571427, "fn": 22.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2727272727272727, "f": 0.3, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}, "Size": {"pr": 0.5, "re": 0.3333333333333333, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Color": {"pr": 0.5, "re": 0.3333333333333333, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a table set with white plates, silverware, and glasses. There are several white napkins on the table. The tablecloth is white and has a pattern of small, white flowers on it. There are several white vases on the table with flowers in them. The flowers are white"}, "303778": {"image_id": 303778, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.19062587708942288, "Bleu_3": 0.12798543247670327, "Bleu_4": 1.4238946596951668e-05, "METEOR": 0.3127925472242976, "ROUGE_L": 0.3210526315789473, "CIDEr": 2.4356297798073072e-12, "SPICE": {"All": {"pr": 0.2, "re": 0.2777777777777778, "f": 0.23255813953488372, "fn": 13.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.125, "re": 0.2, "f": 0.15384615384615385, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.6, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a baseball player in a white uniform, holding a bat and standing on the field. The player is wearing cleats and has a helmet on his head. The field is green and there are no other players or spectators in the background.\n\nThe image is in black and white and has"}, "40426": {"image_id": 40426, "Bleu_1": 0.22727272726756204, "Bleu_2": 0.16256402218685123, "Bleu_3": 0.10796339325484196, "Bleu_4": 1.3236142380428904e-05, "METEOR": 0.1949848741742763, "ROUGE_L": 0.2889039242219215, "CIDEr": 3.4158185429921764e-07, "SPICE": {"All": {"pr": 0.25, "re": 0.14285714285714285, "f": 0.18181818181818182, "fn": 24.0, "numImages": 1.0, "fp": 12.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.16666666666666666, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a kitchen countertop with a blender, a bowl of strawberries, and a spoon. The blender is turned on and has a small amount of strawberries in it. The bowl of strawberries has a spoon in it and is sitting on the"}, "324291": {"image_id": 324291, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.1628822035823003, "Bleu_3": 0.11836112830783165, "Bleu_4": 0.07706946743067312, "METEOR": 0.19496463699766176, "ROUGE_L": 0.2494158878504673, "CIDEr": 5.9629788633209936e-09, "SPICE": {"All": {"pr": 0.15625, "re": 0.22727272727272727, "f": 0.18518518518518515, "fn": 17.0, "numImages": 1.0, "fp": 27.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.14285714285714285, "re": 0.25, "f": 0.18181818181818182, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5, "f": 0.380952380952381, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a woman riding a horse in a green field. The woman is wearing a red shirt and black pants, and the horse is wearing a brown saddle and bridle. The background is a clear blue sky with some clouds.\n\nThe image is taken from a bird's"}, "96241": {"image_id": 96241, "Bleu_1": 0.2916666666605903, "Bleu_2": 0.11140622746151968, "Bleu_3": 6.461802818873081e-07, "Bleu_4": 1.5648119269441605e-09, "METEOR": 0.21375324768500636, "ROUGE_L": 0.23036253776435048, "CIDEr": 3.783165003619035e-09, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.06666666666666667, "f": 0.07692307692307691, "fn": 28.0, "numImages": 1.0, "fp": 20.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.18181818181818182, "f": 0.1904761904761905, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a group of people standing next to a train that is parked on the tracks. The train is black and has a number on the side. There are people standing on the platform and looking at the train. The image is in black and white."}, "326911": {"image_id": 326911, "Bleu_1": 0.15999999999680004, "Bleu_2": 0.11428571428340528, "Bleu_3": 0.09345903743205189, "Bleu_4": 0.07677132076930955, "METEOR": 0.20736047343604827, "ROUGE_L": 0.2594167679222357, "CIDEr": 1.831034948122571e-10, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.22727272727272727, "f": 0.20408163265306123, "fn": 17.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.7142857142857143, "f": 0.5555555555555556, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a black and white cat sitting on the sidewalk next to a bicycle. The cat is looking at the bicycle with its head tilted to the side. The bicycle has a basket on the front and a bell on the handlebars. The cat is wearing a col"}, "209222": {"image_id": 209222, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.18504536999744126, "Bleu_3": 0.14876737580907046, "Bleu_4": 0.12676612954445451, "METEOR": 0.2566047200220614, "ROUGE_L": 0.29151732377538825, "CIDEr": 2.1621235609434385e-12, "SPICE": {"All": {"pr": 0.25, "re": 0.3157894736842105, "f": 0.27906976744186046, "fn": 13.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.7142857142857143, "f": 0.5263157894736842, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a man sitting on a bench in the middle of a city street. The man is wearing a black hoodie and has his arms crossed over his chest. The bench is made of concrete and has a small table on top of it. There are trees and buildings in the background."}, "362293": {"image_id": 362293, "Bleu_1": 0.31481481480898493, "Bleu_2": 0.24371914504394585, "Bleu_3": 0.15076449164519362, "Bleu_4": 0.09053817930683167, "METEOR": 0.23800821128528787, "ROUGE_L": 0.24124293785310735, "CIDEr": 4.4101973917892855e-10, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.2, "f": 0.15789473684210528, "fn": 12.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.125, "f": 0.18181818181818182, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.5, "f": 0.28571428571428575, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a horse and carriage on a city street. The horse is pulling a carriage with two people inside. The carriage is decorated with flowers and ribbons. The street is lined with buildings and there are cars parked on the side of the road. The sky is cloudy and there are trees"}, "144481": {"image_id": 144481, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.09901475429574468, "Bleu_3": 5.809560444813124e-07, "Bleu_4": 1.414355019067261e-09, "METEOR": 0.18853121195507397, "ROUGE_L": 0.18780788177339902, "CIDEr": 8.817038573044137e-12, "SPICE": {"All": {"pr": 0.125, "re": 0.13636363636363635, "f": 0.13043478260869565, "fn": 19.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 0.3333333333333333, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.07692307692307693, "re": 0.125, "f": 0.09523809523809525, "fn": 7.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.2, "f": 0.23529411764705882, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The image is of a museum exhibit with several ancient Greek vases on display. The vases are made of clay and have intricate designs on them. They are displayed on a blue carpet in a dimly lit room with white walls and a high ceiling. There are several other artifacts on display"}, "433804": {"image_id": 433804, "Bleu_1": 0.22807017543459532, "Bleu_2": 0.1276351315918384, "Bleu_3": 6.665907105567284e-07, "Bleu_4": 1.5303684471240379e-09, "METEOR": 0.17722818263013843, "ROUGE_L": 0.22994076467420568, "CIDEr": 1.2624233092282405e-13, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.25, "f": 0.20408163265306123, "fn": 15.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.125, "f": 0.10526315789473685, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a small boat traveling down a river surrounded by tall trees and palm trees. The boat is white and has a blue stripe on the side. There are people on the boat, but they are not visible in the image. The water is calm and there are no other boats or people in the"}, "142815": {"image_id": 142815, "Bleu_1": 0.2105263157857803, "Bleu_2": 0.1501878522938693, "Bleu_3": 0.0742966059776262, "Bleu_4": 9.3353002593842e-06, "METEOR": 0.23174297301215646, "ROUGE_L": 0.2335630674980855, "CIDEr": 1.666031861023468e-12, "SPICE": {"All": {"pr": 0.375, "re": 0.25, "f": 0.3, "fn": 18.0, "numImages": 1.0, "fp": 10.0, "tp": 6.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.09090909090909091, "f": 0.1111111111111111, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.6666666666666666, "re": 0.5, "f": 0.5714285714285715, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 4.0}}, "caption": "The image shows a man sitting on a bed, looking at a camera. He is wearing a red shirt and has a serious expression on his face. The bed is made of wood and has a white sheet on it. There is a window in the background with a view of the city. The room is dim"}, "85292": {"image_id": 85292, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.14793756076371775, "Bleu_3": 0.07446639778133946, "Bleu_4": 9.43994412057466e-06, "METEOR": 0.20164999684793433, "ROUGE_L": 0.24110671936758893, "CIDEr": 7.118029597860436e-13, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.375, "f": 0.33333333333333326, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "This is an image of a train traveling along a track with other trains in the background. The train is made up of several cars, including a caboose and a tank car. The train is traveling at a moderate speed and appears to be carrying cargo. The sky is clear and there are no clouds"}, "500423": {"image_id": 500423, "Bleu_1": 0.23214285713871177, "Bleu_2": 0.1125270530222903, "Bleu_3": 6.166518568255149e-07, "Bleu_4": 1.4503107473430172e-09, "METEOR": 0.16237908864984163, "ROUGE_L": 0.2096700274977085, "CIDEr": 8.156172512907473e-14, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.16, "f": 0.17391304347826086, "fn": 21.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5, "f": 0.4210526315789474, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The train is a red and orange train with a white and black stripe on the side. It is parked in a train station with a large window on the front. There are people standing in front of the train, looking at it.\n\nThe train is a red and orange train with a white and black"}, "196280": {"image_id": 196280, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.17149858513917848, "Bleu_3": 0.10556671919572966, "Bleu_4": 1.2447904522737402e-05, "METEOR": 0.15420639700645242, "ROUGE_L": 0.2544696066746126, "CIDEr": 9.518666637230684e-11, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.07692307692307693, "f": 0.08, "fn": 24.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2222222222222222, "f": 0.19999999999999998, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a young boy standing in front of a kitchen counter with several pots and pans on it. The boy is wearing a white shirt and black pants, and has a spoon in his hand. The background is a dark brown color with a few scattered objects on it.\n\nThe"}, "84752": {"image_id": 84752, "Bleu_1": 0.2727272727223141, "Bleu_2": 0.15891043153801604, "Bleu_3": 0.1126462485588081, "Bleu_4": 0.0861081409020589, "METEOR": 0.20234115789145404, "ROUGE_L": 0.28126801152737757, "CIDEr": 3.4862071655190856e-12, "SPICE": {"All": {"pr": 0.13513513513513514, "re": 0.18518518518518517, "f": 0.15625, "fn": 22.0, "numImages": 1.0, "fp": 32.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.3125, "re": 0.4166666666666667, "f": 0.35714285714285715, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 5.0}}, "caption": "The image shows a large, white jet plane sitting on the ground in front of a building. The plane has a black nose and a red tail. There are several people standing around the plane, looking at it. The sky is cloudy and there are some trees in the background.\n\nThe image is taken from"}, "222317": {"image_id": 222317, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.1940285000252239, "Bleu_3": 0.1566155905072522, "Bleu_4": 0.13376143914338529, "METEOR": 0.27155735192744673, "ROUGE_L": 0.33577981651376143, "CIDEr": 7.969200659984552e-11, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.2777777777777778, "f": 0.22222222222222224, "fn": 13.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a small dog sitting on a couch in a living room. The dog is wearing a red collar and has its paws tucked under its body. The couch is covered in a blue and white striped blanket. There is a lamp on the table next to the couch"}, "544421": {"image_id": 544421, "Bleu_1": 0.29999999999400007, "Bleu_2": 0.15649215928402857, "Bleu_3": 0.0799063530041829, "Bleu_4": 1.0207315006261666e-05, "METEOR": 0.20353448317468484, "ROUGE_L": 0.2692307692307692, "CIDEr": 5.676609224736308e-08, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.15789473684210525, "f": 0.13043478260869565, "fn": 16.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.25, "f": 0.18181818181818182, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 1.0, "f": 0.33333333333333337, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Object": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image is a cake with a waterfall on top of it. The cake is made of chocolate and has green icing on top. There are trees and rocks on the sides of the cake. The waterfall is made of white frosting and has a small stream running through it."}, "526827": {"image_id": 526827, "Bleu_1": 0.32608695651465036, "Bleu_2": 0.17025130614800754, "Bleu_3": 8.701138391057639e-07, "Bleu_4": 1.9784038258954024e-09, "METEOR": 0.23874355500338706, "ROUGE_L": 0.2794502617801047, "CIDEr": 3.205537547895726e-08, "SPICE": {"All": {"pr": 0.08, "re": 0.11764705882352941, "f": 0.09523809523809526, "fn": 15.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.4, "f": 0.25000000000000006, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a pair of scissors, a pair of scissors, and a pair of scissors on a table. The scissors are made of plastic and have red, yellow, and green handles. The scissors are on a green table with a blue background. The image is"}, "527529": {"image_id": 527529, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.13479285035956426, "Bleu_3": 0.070433097826065, "Bleu_4": 9.097876739853199e-06, "METEOR": 0.16313880325070984, "ROUGE_L": 0.1862026862026862, "CIDEr": 6.890998179238082e-13, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.3125, "f": 0.25, "fn": 11.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.14285714285714285, "f": 0.10526315789473685, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 1.0, "re": 0.3333333333333333, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.2727272727272727, "re": 0.5, "f": 0.3529411764705882, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a white cat sitting on top of a black bag. The cat is looking up at the camera with its eyes. The bag appears to be a black leather bag with a zipper on the top. There are several items on the desk, including a laptop, a pen, and a not"}, "152785": {"image_id": 152785, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.20603150145092247, "Bleu_3": 0.15236855057487514, "Bleu_4": 0.11076550621858337, "METEOR": 0.22478572330462376, "ROUGE_L": 0.2776332899869961, "CIDEr": 2.7078516878812037e-10, "SPICE": {"All": {"pr": 0.25925925925925924, "re": 0.30434782608695654, "f": 0.27999999999999997, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 7.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.16666666666666666, "f": 0.13333333333333333, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Object": {"pr": 0.5454545454545454, "re": 0.5454545454545454, "f": 0.5454545454545454, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 6.0}}, "caption": "The image shows a group of elephants walking across a dry, dusty field. The sun is setting in the background, casting a warm orange glow over the scene. The elephants are walking in a line, with their trunks held high and their ears flapping in the wind. They are all"}, "516212": {"image_id": 516212, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.23431167444696221, "Bleu_3": 0.2077274647589439, "Bleu_4": 0.1829565422412541, "METEOR": 0.3687945642170665, "ROUGE_L": 0.40155595451825254, "CIDEr": 2.244732568567648e-09, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.35294117647058826, "f": 0.31578947368421056, "fn": 11.0, "numImages": 1.0, "fp": 15.0, "tp": 6.0}, "Relation": {"pr": 0.2, "re": 0.2857142857142857, "f": 0.23529411764705882, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.6666666666666666, "f": 0.5333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a cat sitting on top of a microwave oven in a kitchen. The cat is looking directly at the camera with its eyes wide open. The microwave oven is white and has a digital display on the front. There are several kitchen utensils on the countertop, including a"}, "403378": {"image_id": 403378, "Bleu_1": 0.16363636363338846, "Bleu_2": 0.1100963765106158, "Bleu_3": 0.07704895020757056, "Bleu_4": 0.0544595700666424, "METEOR": 0.2465391803986196, "ROUGE_L": 0.2513243084167157, "CIDEr": 3.19720450259899e-13, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.18181818181818182, "f": 0.16326530612244897, "fn": 18.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The woman in the image is holding a mirror and looking at herself in it. She is wearing a white dress and has long blonde hair. The background is dark and there are candles on the table.\n\nThe woman in the image is holding a mirror and looking at herself in it. She is we"}, "216051": {"image_id": 216051, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.17458222992646102, "Bleu_3": 0.13370981749106947, "Bleu_4": 0.109436018609632, "METEOR": 0.28889273115066516, "ROUGE_L": 0.2959369314736203, "CIDEr": 1.11438324532336e-11, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.13793103448275862, "f": 0.1509433962264151, "fn": 25.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a woman sitting on a bench in a park, holding a dog. The woman is wearing a purple shirt and pants, and the dog is a small black and white dog. The bench is made of wood and has a metal frame. The trees in the background are tall and"}, "543043": {"image_id": 543043, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.16823164622440862, "Bleu_3": 0.13044729210752723, "Bleu_4": 0.1074271647268228, "METEOR": 0.2537897112825308, "ROUGE_L": 0.2896142433234421, "CIDEr": 4.704120846897736e-11, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.23076923076923078, "f": 0.24489795918367346, "fn": 20.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.4166666666666667, "f": 0.4, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a red sports car parked in front of a small, rustic cabin. The cabin has a large wooden porch with a wooden bench and a small table on it. There are several old cars parked in front of the cabin, including a blue pickup truck and a yellow school bus"}, "392493": {"image_id": 392493, "Bleu_1": 0.23999999999520005, "Bleu_2": 0.12121830534381622, "Bleu_3": 0.06739562828886043, "Bleu_4": 8.983577738158223e-06, "METEOR": 0.18354979006920163, "ROUGE_L": 0.1852976913730255, "CIDEr": 1.4494335600810052e-10, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.20689655172413793, "f": 0.24000000000000002, "fn": 23.0, "numImages": 1.0, "fp": 15.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.1111111111111111, "f": 0.13333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5555555555555556, "re": 0.45454545454545453, "f": 0.5, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 5.0}}, "caption": "The image shows a group of people standing on a grassy field, holding colorful kites. The kites are flying in the sky, with the sun setting in the background. The people are wearing casual clothing and are standing in a line, with their kites held high. The image is taken"}, "524681": {"image_id": 524681, "Bleu_1": 0.30357142856600766, "Bleu_2": 0.1965613482731822, "Bleu_3": 0.11268795394901665, "Bleu_4": 0.07208404560789433, "METEOR": 0.2798282775597069, "ROUGE_L": 0.2717149220489977, "CIDEr": 1.7897375443762758e-13, "SPICE": {"All": {"pr": 0.13636363636363635, "re": 0.14285714285714285, "f": 0.1395348837209302, "fn": 18.0, "numImages": 1.0, "fp": 19.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows a group of people standing on a beach with a kite flying in the sky. The people are wearing casual clothing and are looking up at the kite. The sky is clear and blue with a few clouds scattered about. The beach is covered in sand and there are some rocks in the"}, "265816": {"image_id": 265816, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.20982172726164103, "Bleu_3": 0.13643933968476482, "Bleu_4": 0.09990095999345465, "METEOR": 0.20530971194954648, "ROUGE_L": 0.2814302191464821, "CIDEr": 3.13952842531648e-11, "SPICE": {"All": {"pr": 0.24, "re": 0.21428571428571427, "f": 0.2264150943396226, "fn": 22.0, "numImages": 1.0, "fp": 19.0, "tp": 6.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.09090909090909091, "f": 0.09999999999999999, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.38461538461538464, "f": 0.41666666666666663, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a horse and carriage in front of a building with a balcony and windows. The building has a red roof and white walls. The horse is wearing a black and white harness and has a red ribbon tied around its neck. The carriage is black and has a white canopy. There"}, "528984": {"image_id": 528984, "Bleu_1": 0.29999999999400007, "Bleu_2": 0.17496355305240635, "Bleu_3": 0.08607650943307255, "Bleu_4": 1.0792921564506728e-05, "METEOR": 0.21228365904232266, "ROUGE_L": 0.2434435575826682, "CIDEr": 1.9032304951221317e-08, "SPICE": {"All": {"pr": 0.25, "re": 0.19230769230769232, "f": 0.2173913043478261, "fn": 21.0, "numImages": 1.0, "fp": 15.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.36363636363636365, "f": 0.380952380952381, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "This is a ski resort with a lot of people on the slopes. There are several people on the slopes, including some in ski gear and others in snowboarding gear. The slopes are covered in snow and there are several trees and rocks in the background. The sky is cloud"}, "565776": {"image_id": 565776, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.20550493309104514, "Bleu_3": 0.13633224439757732, "Bleu_4": 0.0848007939389795, "METEOR": 0.24859447603918522, "ROUGE_L": 0.2784479947831757, "CIDEr": 1.6176470679040546e-10, "SPICE": {"All": {"pr": 0.10714285714285714, "re": 0.13636363636363635, "f": 0.11999999999999998, "fn": 19.0, "numImages": 1.0, "fp": 25.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a kitchen with a large island in the center of the room. The island has a wooden top and is surrounded by stools. There is a refrigerator, oven, and sink on the walls. The floor is made of hardwood and there are no windows in the room. The walls"}, "208132": {"image_id": 208132, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.17131872291347974, "Bleu_3": 0.1413162504531272, "Bleu_4": 0.12197379410072966, "METEOR": 0.3056050920992618, "ROUGE_L": 0.308080808080808, "CIDEr": 1.7322330353927186e-11, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.18518518518518517, "f": 0.19607843137254902, "fn": 22.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.6666666666666666, "re": 0.2, "f": 0.30769230769230765, "fn": 8.0, "numImages": 1.0, "fp": 1.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a table with a plate of food on it. There are two glasses of red wine on the table, one with a straw in it and the other with a spoon in it. There is also a bottle of ketchup on the table. The table is covered with a red and"}, "37017": {"image_id": 37017, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.221323371633511, "Bleu_3": 0.16871413170505073, "Bleu_4": 0.09899678591567462, "METEOR": 0.23997444944508994, "ROUGE_L": 0.25894481503941785, "CIDEr": 8.82454464131137e-12, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.10714285714285714, "f": 0.12244897959183672, "fn": 25.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3, "f": 0.3157894736842105, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a black dog sitting on the floor in front of a kitchen counter. The dog is looking up at the camera with its tongue hanging out of its mouth. The kitchen counter has a stove, sink, and refrigerator on it. There is a white tile floor in the kitchen. The"}, "20536": {"image_id": 20536, "Bleu_1": 0.2608695652117203, "Bleu_2": 0.15227739752202907, "Bleu_3": 8.077421359130519e-07, "Bleu_4": 1.8710588175603566e-09, "METEOR": 0.18822274214921628, "ROUGE_L": 0.2378476735118274, "CIDEr": 4.004461336048479e-08, "SPICE": {"All": {"pr": 0.25, "re": 0.18181818181818182, "f": 0.2105263157894737, "fn": 18.0, "numImages": 1.0, "fp": 12.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.4, "re": 0.2222222222222222, "f": 0.2857142857142857, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.4, "re": 0.2857142857142857, "f": 0.3333333333333333, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}}, "caption": "The image shows a toilet in a bathroom with a sink and a toilet paper holder on the wall.\n\nThe toilet is white and has a seat and lid. The sink is stainless steel and has a faucet on the right side. The toilet paper holder"}, "289264": {"image_id": 289264, "Bleu_1": 0.2075471698074048, "Bleu_2": 0.12635334149896824, "Bleu_3": 0.06789968544843664, "Bleu_4": 8.89525332437836e-06, "METEOR": 0.21188014861834262, "ROUGE_L": 0.25894481503941785, "CIDEr": 1.5399759337905803e-11, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.17647058823529413, "f": 0.15789473684210528, "fn": 14.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.42857142857142855, "f": 0.39999999999999997, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a brown dog sitting on a windowsill looking out at the view. The dog is wearing a collar and has its head resting on its paws. The background is a rainy day with trees and buildings visible through the window.\n\nThe dog is looking out at the view with its"}, "18014": {"image_id": 18014, "Bleu_1": 0.41860465115305573, "Bleu_2": 0.2823724831943621, "Bleu_3": 0.18002274574304641, "Bleu_4": 1.9542518789387487e-05, "METEOR": 0.2463121645787314, "ROUGE_L": 0.29756097560975614, "CIDEr": 1.5690338562067015e-07, "SPICE": {"All": {"pr": 0.10344827586206896, "re": 0.13043478260869565, "f": 0.11538461538461538, "fn": 20.0, "numImages": 1.0, "fp": 26.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.3333333333333333, "f": 0.2608695652173913, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image is a pizza box with a pizza inside. The pizza has cheese, pepperoni, and vegetables on it. There is a knife and a fork on the side of the box. The box is sitting on a table with a blue tablecloth."}, "381123": {"image_id": 381123, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.17377018855424126, "Bleu_3": 0.12273221526854444, "Bleu_4": 1.4009047908622088e-05, "METEOR": 0.14972806309002615, "ROUGE_L": 0.2544392801811465, "CIDEr": 9.507429615424949e-10, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.16666666666666666, "f": 0.17391304347826086, "fn": 20.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.4, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a group of people standing on a beach with kayaks in the water. The sky is cloudy and there are mountains in the background.\n\nThe image is taken from a high vantage point, looking down on the beach and the kayaks in the water. The beach is sandy"}, "19608": {"image_id": 19608, "Bleu_1": 0.4038461538383876, "Bleu_2": 0.30825739963656773, "Bleu_3": 0.1786457781290674, "Bleu_4": 1.846909213573168e-05, "METEOR": 0.25577512214493464, "ROUGE_L": 0.24349908759124086, "CIDEr": 4.270860486394993e-09, "SPICE": {"All": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a bird standing on the edge of a pond, looking at its reflection in the water. The bird is a heron, with a long neck and legs. The pond is surrounded by trees and grass, and there is a bicycle parked on the side of the road. The sky"}, "497348": {"image_id": 497348, "Bleu_1": 0.28571428570918367, "Bleu_2": 0.2038588765713768, "Bleu_3": 0.11546007350729219, "Bleu_4": 1.3054339553369079e-05, "METEOR": 0.2614844572103489, "ROUGE_L": 0.2663755458515284, "CIDEr": 1.944922384181766e-12, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.2777777777777778, "f": 0.25, "fn": 13.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.14285714285714285, "f": 0.16666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.5714285714285714, "f": 0.5, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a road with no traffic on it. The road is lined with trees on both sides and there is a sign on the left side of the road that says \"no through road\". The sky is cloudy and there are some trees in the background.\n\nThe image is taken from the perspective of"}, "437594": {"image_id": 437594, "Bleu_1": 0.30357142856600766, "Bleu_2": 0.1965613482731822, "Bleu_3": 0.14197792524995015, "Bleu_4": 0.08572285991508859, "METEOR": 0.2619947903427714, "ROUGE_L": 0.27566171723692706, "CIDEr": 1.7225850819574674e-11, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.21052631578947367, "f": 0.186046511627907, "fn": 15.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4444444444444444, "f": 0.4444444444444444, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a woman sitting at a desk in front of a laptop computer. She is wearing a blue shirt and has a blue scarf around her neck. There are several photos on the wall behind her, including one of a man and one of a woman. The wall is made of brick and has"}, "413404": {"image_id": 413404, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.1577557537051747, "Bleu_3": 0.09984894236626275, "Bleu_4": 0.06713648224794187, "METEOR": 0.1832157607756865, "ROUGE_L": 0.21580188679245285, "CIDEr": 5.15349583024974e-11, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.21739130434782608, "f": 0.19607843137254902, "fn": 18.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of people standing in a park. They are all wearing casual clothing and are looking at something on their phones. There are several trees in the background, and a building with a clock tower in the distance. The sky is cloudy and there is a breeze blowing"}, "332775": {"image_id": 332775, "Bleu_1": 0.1960784313687044, "Bleu_2": 0.15339299776643633, "Bleu_3": 0.12430587656291647, "Bleu_4": 0.10467398290586188, "METEOR": 0.26599269758384414, "ROUGE_L": 0.3051907442151345, "CIDEr": 2.78259143553393e-11, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.06896551724137931, "f": 0.0784313725490196, "fn": 27.0, "numImages": 1.0, "fp": 20.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 15.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a cat sitting in a suitcase on a bed. The cat is wearing a collar and has its paws tucked under its body. The suitcase is black and has wheels on the bottom. The bed is made of wood and has a blanket on it. The room is"}, "530624": {"image_id": 530624, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.15339299776643633, "Bleu_3": 0.07830779525362613, "Bleu_4": 1.0001000249872884e-05, "METEOR": 0.23491543590473113, "ROUGE_L": 0.26116207951070336, "CIDEr": 5.320693391895277e-11, "SPICE": {"All": {"pr": 0.1, "re": 0.14285714285714285, "f": 0.11764705882352941, "fn": 18.0, "numImages": 1.0, "fp": 27.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a brown and white dog lying under a floral patterned blanket on a bed. The dog is curled up and appears to be sleeping. The bed is covered in a floral patterned blanket and has a pillow on it. The room is dimly lit and there are cur"}, "139113": {"image_id": 139113, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.13026252642719696, "Bleu_3": 0.06975194623395738, "Bleu_4": 9.122591486542633e-06, "METEOR": 0.18218236241068217, "ROUGE_L": 0.22536945812807885, "CIDEr": 5.462270310712163e-10, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.10344827586206896, "f": 0.10714285714285715, "fn": 26.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 15.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2727272727272727, "f": 0.2608695652173913, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of people playing soccer on a field. They are wearing red and blue jerseys and black shorts. One player is kicking the ball with his left foot while another player is running towards the goal. There are trees in the background and a fence surrounding the field."}, "192858": {"image_id": 192858, "Bleu_1": 0.3333333333263889, "Bleu_2": 0.2062842492474154, "Bleu_3": 0.14052875118914693, "Bleu_4": 0.08861775869596063, "METEOR": 0.25058080630679375, "ROUGE_L": 0.3221830985915493, "CIDEr": 1.5254738234610904e-08, "SPICE": {"All": {"pr": 0.25, "re": 0.21428571428571427, "f": 0.23076923076923075, "fn": 22.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.125, "re": 0.08333333333333333, "f": 0.1, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.5, "f": 0.47619047619047616, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a group of people sitting at a table in a restaurant, eating pizza. The table is covered in white tablecloths and there are several pizzas on the table. The people are all smiling and enjoying their meals. The atmosphere is lively and the lighting is"}, "482742": {"image_id": 482742, "Bleu_1": 0.37254901960053827, "Bleu_2": 0.228378770342237, "Bleu_3": 0.1021029899855528, "Bleu_4": 1.220305407563223e-05, "METEOR": 0.23659055626909212, "ROUGE_L": 0.24113054649668939, "CIDEr": 1.3543610083284922e-06, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.18181818181818182, "f": 0.21428571428571427, "fn": 27.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.46153846153846156, "f": 0.48000000000000004, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 6.0}}, "caption": "The image shows a bicycle parked on the sidewalk next to a building with a red roof. There are trees and bushes in the background.\n\nThe image is taken from a low angle, looking down the street. The sun is shining and there are no other people in the image. The"}, "398818": {"image_id": 398818, "Bleu_1": 0.24999999999431824, "Bleu_2": 0.1704985848636984, "Bleu_3": 0.12757664411350078, "Bleu_4": 0.08435919294635907, "METEOR": 0.24614217688751072, "ROUGE_L": 0.214185393258427, "CIDEr": 2.88261977407549e-08, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.16, "f": 0.15094339622641512, "fn": 21.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.125, "re": 0.16666666666666666, "f": 0.14285714285714288, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows three bananas on a countertop. The bananas are yellow and have blue stickers on them. The stickers have the words \"Banana\" and \"Banana\" written on them. The bananas are arranged in a row on the countertop. The countertop is made of"}, "305871": {"image_id": 305871, "Bleu_1": 0.18867924527945892, "Bleu_2": 0.08518740546037545, "Bleu_3": 5.220677531965598e-07, "Bleu_4": 1.298831014470151e-09, "METEOR": 0.19520242622975245, "ROUGE_L": 0.2083096186681844, "CIDEr": 5.096512202557439e-12, "SPICE": {"All": {"pr": 0.10714285714285714, "re": 0.12, "f": 0.1132075471698113, "fn": 22.0, "numImages": 1.0, "fp": 25.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a street with a sign on the sidewalk that reads, \"San Carlos Street.\" There are buildings on either side of the street, and a blue sky in the background.\n\nThe image is taken from a bird's eye view, with the camera facing the street. The buildings are made of"}, "443818": {"image_id": 443818, "Bleu_1": 0.20370370369993146, "Bleu_2": 0.10737969231852026, "Bleu_3": 0.07625886213125337, "Bleu_4": 9.656621220138897e-06, "METEOR": 0.18798159973859507, "ROUGE_L": 0.1862026862026862, "CIDEr": 4.2074320862943776e-13, "SPICE": {"All": {"pr": 0.11764705882352941, "re": 0.08695652173913043, "f": 0.09999999999999999, "fn": 21.0, "numImages": 1.0, "fp": 15.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.2857142857142857, "f": 0.25, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a brown dog lying on its back in a small, furry bed. The dog is wearing a collar and tag on its neck. The bed is made of a soft, fluffy material and has a small opening on the top for the dog to stick its head through. The dog is"}, "421109": {"image_id": 421109, "Bleu_1": 0.2075471698074048, "Bleu_2": 0.10942520359115704, "Bleu_3": 0.06169095833758949, "Bleu_4": 8.277965966610606e-06, "METEOR": 0.20008983657426724, "ROUGE_L": 0.2896142433234421, "CIDEr": 6.245084217325404e-12, "SPICE": {"All": {"pr": 0.125, "re": 0.15789473684210525, "f": 0.13953488372093023, "fn": 16.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.1111111111111111, "f": 0.125, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.2222222222222222, "re": 0.4, "f": 0.2857142857142857, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a brown horse standing in a green field with mountains in the background. The horse is wearing a halter and has a long mane. There are two other horses in the background, one of which is also wearing a halter. The horses are grazing on the grass in the field."}, "416660": {"image_id": 416660, "Bleu_1": 0.34693877550312374, "Bleu_2": 0.2550510152998425, "Bleu_3": 0.17690444644743003, "Bleu_4": 0.10474049059633278, "METEOR": 0.21332863210354994, "ROUGE_L": 0.27566171723692706, "CIDEr": 1.4331315671415216e-09, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.38461538461538464, "f": 0.24390243902439027, "fn": 8.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.5, "f": 0.22222222222222224, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5714285714285714, "f": 0.4210526315789474, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of people standing in front of a store with a sign that reads \"johnson's jewelry\". The people are dressed in clothing from the 1950s and are looking at the display in the window. The store is located on a busy street with other"}, "322845": {"image_id": 322845, "Bleu_1": 0.24999999999553577, "Bleu_2": 0.19069251784568234, "Bleu_3": 0.14988143767147738, "Bleu_4": 0.11749577059992224, "METEOR": 0.25372272525164824, "ROUGE_L": 0.3265524625267667, "CIDEr": 2.4989703304507062e-12, "SPICE": {"All": {"pr": 0.10714285714285714, "re": 0.17647058823529413, "f": 0.13333333333333333, "fn": 14.0, "numImages": 1.0, "fp": 25.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.75, "f": 0.39999999999999997, "fn": 1.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a red stop sign on a metal pole in front of a rusty building. The sign is leaning against the pole and has a white background with red letters. The building is made of metal and has a large window on the side. There is a tree in the background.\n\nThe image is"}, "304361": {"image_id": 304361, "Bleu_1": 0.1666666666635803, "Bleu_2": 0.11215443081631235, "Bleu_3": 0.06230790884875894, "Bleu_4": 8.29879120277201e-06, "METEOR": 0.14966065223522226, "ROUGE_L": 0.21048999309868874, "CIDEr": 3.0610129051613947e-12, "SPICE": {"All": {"pr": 0.1935483870967742, "re": 0.2727272727272727, "f": 0.22641509433962262, "fn": 16.0, "numImages": 1.0, "fp": 25.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a young child sitting on the floor in a room with a bed and a table. The child is holding a tablet in their hands and looking at it intently. The room is dimly lit and there are some toys and books on the floor. The child is wearing a pajama"}, "446917": {"image_id": 446917, "Bleu_1": 0.2916666666605903, "Bleu_2": 0.19296124624292718, "Bleu_3": 0.11741874968995038, "Bleu_4": 1.3772093427778417e-05, "METEOR": 0.2109386889357491, "ROUGE_L": 0.2952973720608575, "CIDEr": 7.91469887290827e-08, "SPICE": {"All": {"pr": 0.25, "re": 0.3157894736842105, "f": 0.27906976744186046, "fn": 13.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.125, "f": 0.11764705882352941, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.5555555555555556, "f": 0.5, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a person wearing a yellow and blue cycling jersey and holding a banana on their handlebars. The person is standing next to a bicycle with a yellow and blue seat and handlebars. The background is a city street with buildings and cars in the distance."}, "234676": {"image_id": 234676, "Bleu_1": 0.31249999999348965, "Bleu_2": 0.21573723181113055, "Bleu_3": 0.1264855137960998, "Bleu_4": 0.08188935746398157, "METEOR": 0.2058593385277737, "ROUGE_L": 0.29256594724220625, "CIDEr": 1.0089265545280484e-08, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.13043478260869565, "f": 0.14634146341463414, "fn": 20.0, "numImages": 1.0, "fp": 15.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.09090909090909091, "f": 0.11764705882352942, "fn": 10.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.2222222222222222, "f": 0.25, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The image shows a group of people standing on a beach next to a surfboard. The sky is blue and there are waves in the ocean. The people are wearing swimsuits and sunglasses. There are palm trees in the background.\n\nThe image is taken from a low angle"}, "343692": {"image_id": 343692, "Bleu_1": 0.339622641503026, "Bleu_2": 0.2138186907385403, "Bleu_3": 0.15305953245123383, "Bleu_4": 0.12111086315926764, "METEOR": 0.25240140661681665, "ROUGE_L": 0.27774615822424586, "CIDEr": 2.0487223353297717e-11, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.23809523809523808, "f": 0.20833333333333334, "fn": 16.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4444444444444444, "f": 0.380952380952381, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a scooter parked in front of a yellow building with a sign that reads \"traffic ahead.\" The scooter is a silver and black color and has a small mirror on the side. The building is a yellow color with a large window on the side. There is a small parking"}, "293011": {"image_id": 293011, "Bleu_1": 0.39583333332508686, "Bleu_2": 0.29020657841007313, "Bleu_3": 0.22229821466056154, "Bleu_4": 0.1486470694363979, "METEOR": 0.2932331535548616, "ROUGE_L": 0.36023622047244097, "CIDEr": 4.850327099900543e-07, "SPICE": {"All": {"pr": 0.10714285714285714, "re": 0.1111111111111111, "f": 0.10909090909090909, "fn": 24.0, "numImages": 1.0, "fp": 25.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.1, "f": 0.11764705882352941, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a birthday cake with an airplane on it. The cake is blue and has a number 6 on it. There are candles on the cake and it appears to be on a table.\n\nThe image is of a birthday cake with an airplane on it."}, "104625": {"image_id": 104625, "Bleu_1": 0.24561403508341031, "Bleu_2": 0.17521916100951418, "Bleu_3": 0.13070338390996192, "Bleu_4": 0.09536174349646075, "METEOR": 0.2975943223346375, "ROUGE_L": 0.30148270181219106, "CIDEr": 6.27298732383916e-14, "SPICE": {"All": {"pr": 0.28, "re": 0.22580645161290322, "f": 0.25, "fn": 24.0, "numImages": 1.0, "fp": 18.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.5, "f": 0.4615384615384615, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The image shows a cat sitting on a table in front of a television. The cat is looking at the television with its ears perked up. The television is showing a soccer match with players in blue and white uniforms playing on the field. The cat appears to be interested in the game and is watching intently"}, "175612": {"image_id": 175612, "Bleu_1": 0.21739130434310025, "Bleu_2": 0.15541746803663614, "Bleu_3": 0.08188084718265139, "Bleu_4": 1.0629666321559288e-05, "METEOR": 0.2127576082777393, "ROUGE_L": 0.19728331177231562, "CIDEr": 7.439314958600873e-09, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.16, "f": 0.15384615384615383, "fn": 21.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4444444444444444, "f": 0.380952380952381, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image is of a skateboarder standing on a sidewalk with a skateboard in his hand. The sun is shining behind him, casting a shadow on the ground. The skateboarder is wearing a red and blue striped shirt and blue jeans. He is looking down at"}, "43448": {"image_id": 43448, "Bleu_1": 0.3584905660309719, "Bleu_2": 0.24909123653704068, "Bleu_3": 0.16946066413438396, "Bleu_4": 0.11811816077501729, "METEOR": 0.23621027242862253, "ROUGE_L": 0.29651633810423983, "CIDEr": 1.2343267451862919e-06, "SPICE": {"All": {"pr": 0.23529411764705882, "re": 0.2857142857142857, "f": 0.2580645161290323, "fn": 10.0, "numImages": 1.0, "fp": 13.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 1.0, "f": 0.25, "fn": 0.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.5, "re": 0.42857142857142855, "f": 0.4615384615384615, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 3.0}}, "caption": "The image shows two elephants standing on a rocky beach with trees in the background. The elephants are brown with large ears and tusks. They are standing next to each other and appear to be looking at something in the distance. The sky is blue and there are clouds in the background. The"}, "528705": {"image_id": 528705, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.15861031714066393, "Bleu_3": 0.07850304593348242, "Bleu_4": 9.868981544258463e-06, "METEOR": 0.15021058554161182, "ROUGE_L": 0.21403508771929822, "CIDEr": 3.7139987069244684e-12, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.2857142857142857, "f": 0.24000000000000002, "fn": 15.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.1, "re": 0.16666666666666666, "f": 0.125, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5714285714285714, "f": 0.4210526315789474, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a woman wearing a red coat and holding a stuffed animal. The woman is standing on a bench in front of a large stone building with a clock tower on top. There are people standing around the bench, looking at the woman and the stuffed animal. The sky is cloudy and"}, "319221": {"image_id": 319221, "Bleu_1": 0.32142857142283165, "Bleu_2": 0.18725633517633355, "Bleu_3": 8.659505513046919e-07, "Bleu_4": 1.8709011949882287e-09, "METEOR": 0.17287092530331777, "ROUGE_L": 0.20460644007155634, "CIDEr": 6.8420979657218e-13, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.26666666666666666, "f": 0.19047619047619047, "fn": 11.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5714285714285714, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a tray of food on a cart with several plates of food on it. There are several people in the background, including a man in a white shirt and black pants, and a woman in a black dress. The walls are painted a light blue color and there are several windows on the"}, "338903": {"image_id": 338903, "Bleu_1": 0.2608695652117203, "Bleu_2": 0.18650096164396338, "Bleu_3": 0.13335528843258257, "Bleu_4": 0.08617684067542819, "METEOR": 0.2984628457256137, "ROUGE_L": 0.32317880794701986, "CIDEr": 7.135513900049625e-09, "SPICE": {"All": {"pr": 0.12903225806451613, "re": 0.19047619047619047, "f": 0.15384615384615383, "fn": 17.0, "numImages": 1.0, "fp": 27.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.07142857142857142, "re": 0.14285714285714285, "f": 0.09523809523809523, "fn": 6.0, "numImages": 1.0, "fp": 13.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.42857142857142855, "f": 0.375, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a bowl of cereal with bananas and a spoon. The bowl is on a table with a white background. The lighting is bright and the image is in focus.\n\nThe image is of a bowl of cereal with bananas and a spoon. The"}, "364993": {"image_id": 364993, "Bleu_1": 0.29999999999400007, "Bleu_2": 0.17496355305240635, "Bleu_3": 0.08607650943307255, "Bleu_4": 1.0792921564506728e-05, "METEOR": 0.20119085041610765, "ROUGE_L": 0.23252858958068615, "CIDEr": 1.7572228134280281e-09, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.1724137931034483, "f": 0.19607843137254902, "fn": 24.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.38461538461538464, "f": 0.4347826086956522, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "The image shows a person holding a sandwich in their hand. The sandwich appears to be made of meat, cheese, and vegetables. The person's hand is visible, and the sandwich is on a white paper plate. There is a fork and knife on the plate, but they are not"}, "37616": {"image_id": 37616, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.20180183819497494, "Bleu_3": 0.16969817063172157, "Bleu_4": 0.1413265796065327, "METEOR": 0.2525198402532587, "ROUGE_L": 0.28773584905660377, "CIDEr": 1.7837713831974418e-10, "SPICE": {"All": {"pr": 0.28, "re": 0.2413793103448276, "f": 0.25925925925925924, "fn": 22.0, "numImages": 1.0, "fp": 18.0, "tp": 7.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.46153846153846156, "re": 0.46153846153846156, "f": 0.46153846153846156, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a man standing in a living room with a couch, a coffee table, and a television. The man is wearing a white shirt and black pants. There is a window in the background with curtains. The room is well lit and there are no other objects in the room."}, "157756": {"image_id": 157756, "Bleu_1": 0.31481481480898493, "Bleu_2": 0.2556147957905549, "Bleu_3": 0.18452117548297922, "Bleu_4": 0.13865081575452137, "METEOR": 0.27486969869708916, "ROUGE_L": 0.34231200897867564, "CIDEr": 2.0317332371299717e-11, "SPICE": {"All": {"pr": 0.03571428571428571, "re": 0.047619047619047616, "f": 0.04081632653061224, "fn": 20.0, "numImages": 1.0, "fp": 27.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.09090909090909091, "re": 0.125, "f": 0.10526315789473685, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}}, "caption": "The image shows a large clock tower in the middle of a city street. The clock face is visible on the tower, and there are people walking on the sidewalk in front of it. The sky is dark and there are streetlights on the corners of the street.\n\nThe image is taken at night,"}, "516508": {"image_id": 516508, "Bleu_1": 0.17999999999640004, "Bleu_2": 0.06060915267190813, "Bleu_3": 4.245658537601285e-07, "Bleu_4": 1.129625096611556e-09, "METEOR": 0.13116717557500449, "ROUGE_L": 0.18944099378881987, "CIDEr": 5.213206694744401e-11, "SPICE": {"All": {"pr": 0.3157894736842105, "re": 0.3, "f": 0.3076923076923077, "fn": 14.0, "numImages": 1.0, "fp": 13.0, "tp": 6.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4444444444444444, "f": 0.4444444444444444, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a large, ornate clock hanging from the ceiling of a church. The clock has two hands and a face with Roman numerals. The clock is surrounded by intricate carvings and has a large, ornate frame. The walls of the church are made of stone and have stained"}, "520528": {"image_id": 520528, "Bleu_1": 0.32727272726677686, "Bleu_2": 0.2059714602139953, "Bleu_3": 0.1169829962474529, "Bleu_4": 1.324619730130697e-05, "METEOR": 0.26944465307224774, "ROUGE_L": 0.2872277810476751, "CIDEr": 2.1120756512243073e-13, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.25, "f": 0.1739130434782609, "fn": 12.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}}, "caption": "The image shows a young boy in a baseball uniform holding a baseball bat and standing on the mound of a baseball field. The boy is wearing a blue and white jersey with the number 23 on the back, and has a white baseball cap on his head. The background of the image is a"}, "37675": {"image_id": 37675, "Bleu_1": 0.12727272727041325, "Bleu_2": 0.08408749651670917, "Bleu_3": 0.06437822382780044, "Bleu_4": 0.04759413158233906, "METEOR": 0.1673856217136869, "ROUGE_L": 0.17951736315479697, "CIDEr": 7.135869637556728e-14, "SPICE": {"All": {"pr": 0.041666666666666664, "re": 0.043478260869565216, "f": 0.0425531914893617, "fn": 22.0, "numImages": 1.0, "fp": 23.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.125, "re": 0.16666666666666666, "f": 0.14285714285714288, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}}, "caption": "The image shows a white horse grazing in a field next to a wooden fence. The fence is made of wooden posts and has a gate in the middle. The horse is wearing a brown halter and has a brown mane and tail. The sky is blue and there are some clouds in the distance"}, "232383": {"image_id": 232383, "Bleu_1": 0.32142857142283165, "Bleu_2": 0.24174688920325796, "Bleu_3": 0.1629781336014208, "Bleu_4": 0.09506661657351217, "METEOR": 0.24593137151787783, "ROUGE_L": 0.25206611570247933, "CIDEr": 1.6498341260656092e-13, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.08, "f": 0.0851063829787234, "fn": 23.0, "numImages": 1.0, "fp": 20.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2857142857142857, "f": 0.2105263157894737, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a cat sitting on top of a computer monitor. The cat is looking directly at the camera with its eyes wide open. The monitor is displaying a blue screen with white text on it. The cat's paws are resting on the edge of the monitor. The room is dimly lit with a"}, "137658": {"image_id": 137658, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.11155016321612385, "Bleu_3": 6.290089214849726e-07, "Bleu_4": 1.5012173807660588e-09, "METEOR": 0.09809264305177112, "ROUGE_L": 0.18780788177339902, "CIDEr": 1.766909395523243e-11, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.3125, "f": 0.2325581395348837, "fn": 11.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.2857142857142857, "f": 0.2857142857142857, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.2, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.2727272727272727, "re": 0.5, "f": 0.3529411764705882, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a person holding a small flashlight in their hand. The flashlight is attached to a lanyard and is being held in the person's hand. The person is wearing a blue jacket and has a backpack on their back. The backpack has a small laptop and other electronic devices"}, "209322": {"image_id": 209322, "Bleu_1": 0.29787234041919425, "Bleu_2": 0.21290467263078663, "Bleu_3": 0.14457494718760727, "Bleu_4": 1.618850011273474e-05, "METEOR": 0.23675482780452423, "ROUGE_L": 0.31063017186505404, "CIDEr": 2.2746936940472746e-08, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.22727272727272727, "f": 0.21739130434782608, "fn": 17.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.5, "f": 0.45454545454545453, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a bathroom with a toilet, sink, and shower. The walls are painted with a blue and white pattern. There is a mirror on the wall above the sink. The floor is made of tile.\n\nThe image shows a bathroom with a toilet, sink, and"}, "128644": {"image_id": 128644, "Bleu_1": 0.33333333332679743, "Bleu_2": 0.19999999999603923, "Bleu_3": 0.11775100856360317, "Bleu_4": 1.3580418932543062e-05, "METEOR": 0.22908473254366052, "ROUGE_L": 0.3042123074182919, "CIDEr": 7.597947963754281e-08, "SPICE": {"All": {"pr": 0.09523809523809523, "re": 0.125, "f": 0.1081081081081081, "fn": 14.0, "numImages": 1.0, "fp": 19.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "This is an image of an airplane on the runway\n\nThe airplane is a small, white and red plane with a black nose and tail. It has a large, round window on the side and a small propeller on the front. The plane is parked on the runway, with its whe"}, "342675": {"image_id": 342675, "Bleu_1": 0.18965517241052324, "Bleu_2": 0.12898224153816748, "Bleu_3": 0.06672531065315884, "Bleu_4": 8.572890585584586e-06, "METEOR": 0.25889539077926693, "ROUGE_L": 0.20631341600901915, "CIDEr": 5.491703089685307e-15, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.3125, "f": 0.21739130434782608, "fn": 11.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5714285714285714, "f": 0.4210526315789474, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a man standing on the platform of a train station, looking at a sign on the wall. The train is red and has a number on the side. There are people standing on the platform and in the background, there are buildings and trees. The sky is blue and there are clouds in the sky."}, "200234": {"image_id": 200234, "Bleu_1": 0.1538461538431953, "Bleu_2": 0.05492350363704244, "Bleu_3": 3.922071328139402e-07, "Bleu_4": 1.0533861300924754e-09, "METEOR": 0.13882683496851536, "ROUGE_L": 0.19509594882729217, "CIDEr": 7.465437759279496e-10, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.15789473684210525, "f": 0.16216216216216214, "fn": 16.0, "numImages": 1.0, "fp": 15.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.2857142857142857, "re": 0.3333333333333333, "f": 0.30769230769230765, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The image shows a group of people standing around a picnic table in a forest. There are trees in the background and a small stream running through the area. The people are wearing casual clothing and appear to be enjoying their picnic.\n\nThe image is taken from a bird's eye view"}, "545390": {"image_id": 545390, "Bleu_1": 0.2826086956460303, "Bleu_2": 0.2241462600435509, "Bleu_3": 0.15074532670634294, "Bleu_4": 0.09447475603834063, "METEOR": 0.27275013194132064, "ROUGE_L": 0.2827814569536423, "CIDEr": 1.9153542900745517e-08, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.17391304347826086, "f": 0.15094339622641512, "fn": 19.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.36363636363636365, "f": 0.32, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The woman is holding a pizza in her hands and smiling at the camera. The pizza has various toppings on it, including cheese, pepperoni, and mushrooms. The woman is wearing a red shirt and has a white apron on. The background is a dark brown wooden"}, "43073": {"image_id": 43073, "Bleu_1": 0.3061224489733445, "Bleu_2": 0.23957871187003724, "Bleu_3": 0.16967565256814732, "Bleu_4": 0.10151382408060541, "METEOR": 0.2752327767843513, "ROUGE_L": 0.2945081472540736, "CIDEr": 2.3202680011079353e-09, "SPICE": {"All": {"pr": 0.16129032258064516, "re": 0.17857142857142858, "f": 0.1694915254237288, "fn": 23.0, "numImages": 1.0, "fp": 26.0, "tp": 5.0}, "Relation": {"pr": 0.07692307692307693, "re": 0.1111111111111111, "f": 0.09090909090909093, "fn": 8.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a young boy sitting on a bed, looking at a woman who is holding a hair dryer. The boy has short, blonde hair and is wearing a blue shirt and white pants. The woman is wearing a pink shirt and has long, curly hair. The background"}, "188651": {"image_id": 188651, "Bleu_1": 0.1964285714250638, "Bleu_2": 0.14638501093964223, "Bleu_3": 0.09258553737962033, "Bleu_4": 1.106212078963913e-05, "METEOR": 0.24373365814430878, "ROUGE_L": 0.21682464454976302, "CIDEr": 4.8218274989899296e-14, "SPICE": {"All": {"pr": 0.2692307692307692, "re": 0.2916666666666667, "f": 0.27999999999999997, "fn": 17.0, "numImages": 1.0, "fp": 19.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2727272727272727, "f": 0.3, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4444444444444444, "f": 0.4444444444444444, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image is of a cat lying on the ground in front of a car. The cat is white with black spots and has a pink nose. The car is a silver sedan with a black roof and a black bumper. The license plate is visible on the front of the car. The image is taken"}, "484551": {"image_id": 484551, "Bleu_1": 0.32692307691678996, "Bleu_2": 0.24019223070296639, "Bleu_3": 0.1664955508688948, "Bleu_4": 0.11715489885449303, "METEOR": 0.2546250309379292, "ROUGE_L": 0.28773584905660377, "CIDEr": 2.6232477251000024e-10, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.11428571428571428, "f": 0.12903225806451613, "fn": 31.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.1, "f": 0.09523809523809525, "fn": 9.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.23076923076923078, "f": 0.22222222222222224, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The woman is sitting in the back of a small boat on the water. She is wearing a orange shirt and jeans, and has her hands on the steering wheel. The boat is moving through the water, and there are trees and houses visible in the background.\n\nThe woman is smiling and"}, "396224": {"image_id": 396224, "Bleu_1": 0.27083333332769105, "Bleu_2": 0.13148092653242613, "Bleu_3": 7.216432604089977e-07, "Bleu_4": 1.6999603483372018e-09, "METEOR": 0.10684391368060726, "ROUGE_L": 0.20013123359580048, "CIDEr": 2.6246452379955684e-09, "SPICE": {"All": {"pr": 0.12, "re": 0.13043478260869565, "f": 0.12499999999999997, "fn": 20.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.2727272727272727, "f": 0.28571428571428564, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a table set with a turkey, mashed potatoes, and vegetables. There are two glasses of wine on the table and a bottle of wine on the side. The tablecloth is white and the chairs are wooden. The room is dimly lit and there are candles"}, "255067": {"image_id": 255067, "Bleu_1": 0.22807017543459532, "Bleu_2": 0.1276351315918384, "Bleu_3": 0.06665907105567281, "Bleu_4": 8.605894205852283e-06, "METEOR": 0.22031886793873823, "ROUGE_L": 0.23682750970604544, "CIDEr": 2.841607895249998e-13, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.06451612903225806, "f": 0.07547169811320754, "fn": 29.0, "numImages": 1.0, "fp": 20.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.15384615384615385, "f": 0.17391304347826086, "fn": 11.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a polar bear standing on a rock in the middle of a body of water. The bear is wearing a blue and white striped shirt and has a blue and white striped hat on its head. The bear is looking down at its paws and appears to be trying to catch something in the"}, "479129": {"image_id": 479129, "Bleu_1": 0.2888888888824692, "Bleu_2": 0.2806917861005864, "Bleu_3": 0.24473362695163794, "Bleu_4": 0.21391720958763585, "METEOR": 0.3238579952970674, "ROUGE_L": 0.3588235294117647, "CIDEr": 4.876723548879442e-07, "SPICE": {"All": {"pr": 0.16, "re": 0.14814814814814814, "f": 0.15384615384615383, "fn": 23.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.3, "re": 0.3, "f": 0.3, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a plate of food with bananas and ice cream on it. The bananas are cut in half and topped with chocolate sauce. The ice cream is scooped into a bowl and topped with whipped cream. The plate is on a white tablecloth"}, "363887": {"image_id": 363887, "Bleu_1": 0.18367346938400672, "Bleu_2": 0.12371791482379726, "Bleu_3": 0.08668270200700182, "Bleu_4": 0.06134227411374885, "METEOR": 0.18380394271243164, "ROUGE_L": 0.2139099941554646, "CIDEr": 6.000914936728842e-09, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.23076923076923078, "f": 0.24489795918367346, "fn": 20.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.1111111111111111, "f": 0.13333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.4166666666666667, "f": 0.3846153846153846, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows a red fire truck parked on the side of a dirt road. The truck has a large red ladder on the back and a fire hose coiled around the front bumper. There are several other vehicles parked nearby, including a pickup truck and a small car"}, "441969": {"image_id": 441969, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.23302069120966015, "Bleu_3": 0.18677700456548377, "Bleu_4": 0.1518652930126462, "METEOR": 0.227461743820155, "ROUGE_L": 0.2482558139534883, "CIDEr": 9.097571533191672e-11, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.2608695652173913, "f": 0.24, "fn": 17.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.5, "f": 0.45454545454545453, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a balcony with a table and chairs on it. There is a window on the left side of the balcony with a view of the city. The balcony is surrounded by brick walls and there are plants on the table. The balcony is covered with a white and black"}, "410225": {"image_id": 410225, "Bleu_1": 0.18181818181487608, "Bleu_2": 0.1297498240245396, "Bleu_3": 6.823060598716602e-07, "Bleu_4": 1.5721123718963633e-09, "METEOR": 0.15021862573623943, "ROUGE_L": 0.23788300835654594, "CIDEr": 3.567586004598614e-12, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.11428571428571428, "f": 0.12903225806451613, "fn": 31.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.08333333333333333, "f": 0.09523809523809525, "fn": 11.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.21428571428571427, "f": 0.22222222222222224, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a table with a cup of coffee and a laptop on it. There are also several cups of coffee on the table.\n\nThe laptop has a screen that shows a website with a picture of a person holding a cup of coffee. There are also several cups of coffee on the table."}, "277073": {"image_id": 277073, "Bleu_1": 0.3207547169750801, "Bleu_2": 0.24836183164306555, "Bleu_3": 0.19360515975280349, "Bleu_4": 0.1305277023920121, "METEOR": 0.20696313345857872, "ROUGE_L": 0.28785708266621723, "CIDEr": 5.032583947144854e-12, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.18518518518518517, "f": 0.18181818181818182, "fn": 22.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.3333333333333333, "f": 0.3636363636363636, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a group of people riding on the back of a motorcycle on a busy street. The people are wearing casual clothing and the motorcycle is yellow and black. There are other vehicles on the street, including cars and buses. The sky is clear and there are buildings in the background"}, "41011": {"image_id": 41011, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.1978218215826409, "Bleu_3": 0.1311863860222524, "Bleu_4": 0.08156879226605328, "METEOR": 0.241920266643238, "ROUGE_L": 0.28367380833748546, "CIDEr": 1.23837167808955e-11, "SPICE": {"All": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 12.0, "numImages": 1.0, "fp": 24.0, "tp": 6.0}, "Relation": {"pr": 0.18181818181818182, "re": 0.2857142857142857, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.6666666666666666, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a man in a black suit and top hat riding a horse through a field. The horse is wearing a saddle and bridle. The man is holding the reins with his left hand and has his right hand on the horse's neck. There are several other horses in the background,"}, "343821": {"image_id": 343821, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.15357377920556234, "Bleu_3": 0.11153086590649026, "Bleu_4": 0.0863100371081779, "METEOR": 0.16576495245889689, "ROUGE_L": 0.21721068249258166, "CIDEr": 2.4354166523267385e-12, "SPICE": {"All": {"pr": 0.13636363636363635, "re": 0.13043478260869565, "f": 0.1333333333333333, "fn": 20.0, "numImages": 1.0, "fp": 19.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.16666666666666666, "f": 0.14285714285714288, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 1.0, "f": 0.4, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.2, "re": 0.2222222222222222, "f": 0.2105263157894737, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a white swan swimming in a body of water with its young swimming around it. The swan is surrounded by a group of ducks swimming in the water. The sky is cloudy and there are some trees in the background. The image is taken from a low angle, looking down"}, "530620": {"image_id": 530620, "Bleu_1": 0.19999999999636367, "Bleu_2": 0.12171612388780342, "Bleu_3": 0.06538429607726681, "Bleu_4": 8.562572337389154e-06, "METEOR": 0.1322493788945231, "ROUGE_L": 0.17951736315479697, "CIDEr": 1.3069370454066475e-12, "SPICE": {"All": {"pr": 0.2727272727272727, "re": 0.20689655172413793, "f": 0.23529411764705882, "fn": 23.0, "numImages": 1.0, "fp": 16.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2222222222222222, "f": 0.26666666666666666, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.4444444444444444, "re": 0.3333333333333333, "f": 0.380952380952381, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a group of people standing next to a truck on the side of a road. The truck has a large, yellow and black canopy on top of it, and there are several people standing around it, looking at something on the ground. The sky is clear and blue, with a few clouds"}, "22113": {"image_id": 22113, "Bleu_1": 0.3207547169750801, "Bleu_2": 0.23561672155300986, "Bleu_3": 0.12960567324444897, "Bleu_4": 0.08123169364824535, "METEOR": 0.284299300753972, "ROUGE_L": 0.25707405177603854, "CIDEr": 4.925724954530719e-12, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.16666666666666666, "f": 0.16, "fn": 20.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.3333333333333333, "f": 0.2222222222222222, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.375, "f": 0.33333333333333326, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a red fire hydrant on the sidewalk in front of a building. The hydrant has a green handle and a white cap on top. There is a chain link fence in the background.\n\nThe image is taken in a residential area with a sidewalk and a building in the background"}, "82836": {"image_id": 82836, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.18171094607444632, "Bleu_3": 0.13732533424861082, "Bleu_4": 0.10088532476686506, "METEOR": 0.21584175828738875, "ROUGE_L": 0.25707405177603854, "CIDEr": 1.0874722945906837e-11, "SPICE": {"All": {"pr": 0.3, "re": 0.2727272727272727, "f": 0.28571428571428564, "fn": 16.0, "numImages": 1.0, "fp": 14.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.14285714285714285, "f": 0.16666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.625, "re": 0.5555555555555556, "f": 0.5882352941176471, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 5.0}}, "caption": "The image shows a group of seagulls standing on the beach, looking out to sea. The sky is cloudy and there are waves crashing on the shore. The sand is brown and there are some rocks in the distance. The water is choppy and there are some boats in the distance. The overall"}, "538925": {"image_id": 538925, "Bleu_1": 0.3333333333274854, "Bleu_2": 0.24397501823281484, "Bleu_3": 0.14807546137429284, "Bleu_4": 0.08805699907658898, "METEOR": 0.2480635851991494, "ROUGE_L": 0.2543369816097089, "CIDEr": 1.9699884307416584e-10, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.25, "f": 0.20512820512820512, "fn": 12.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.125, "re": 0.125, "f": 0.125, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a bed with a white sheet and a brown blanket on it. There is a wooden wall behind the bed with a pattern of white and brown stripes. The floor is made of wood and there is a small table with a white vase on it. The room has a white ceiling and white"}, "440189": {"image_id": 440189, "Bleu_1": 0.33333333332679743, "Bleu_2": 0.16329931618231128, "Bleu_3": 0.0816439893537324, "Bleu_4": 1.0318886931044728e-05, "METEOR": 0.18754627895849263, "ROUGE_L": 0.3057644110275689, "CIDEr": 8.340528916445624e-07, "SPICE": {"All": {"pr": 0.12, "re": 0.2, "f": 0.15, "fn": 12.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.375, "f": 0.33333333333333326, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a group of people playing beach volleyball on a sandy beach. The people are wearing beach volleyball uniforms and are standing in a line, ready to serve the ball. The beach is surrounded by tall trees and a rocky cliff in the background. The sky is clear and"}, "32777": {"image_id": 32777, "Bleu_1": 0.32142857142283165, "Bleu_2": 0.20225995873532804, "Bleu_3": 0.1314767947140738, "Bleu_4": 1.439023424524574e-05, "METEOR": 0.2050238681330742, "ROUGE_L": 0.2459677419354839, "CIDEr": 1.0484805998174624e-13, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.22727272727272727, "f": 0.20833333333333331, "fn": 17.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.4, "re": 0.2222222222222222, "f": 0.2857142857142857, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.6666666666666666, "f": 0.8, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.2727272727272727, "re": 0.5, "f": 0.3529411764705882, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a man standing on the platform of a train station. The train is a blue and white train with a yellow stripe on the front. The man is wearing a backpack and has a camera around his neck. There are other people standing on the platform, looking at the train. The train is"}, "50679": {"image_id": 50679, "Bleu_1": 0.24999999999553577, "Bleu_2": 0.19069251784568234, "Bleu_3": 0.13913760149235183, "Bleu_4": 0.10040883216771884, "METEOR": 0.19706737565587476, "ROUGE_L": 0.2476798143851508, "CIDEr": 7.501845158349379e-13, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.15, "f": 0.13953488372093023, "fn": 17.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows an orange lying on the ground in the middle of a parking lot. The orange is a bright orange color and has a rough texture. There are cars parked in the background, but they are not visible in the image. The sky is cloudy and there are no other objects in the image."}, "86250": {"image_id": 86250, "Bleu_1": 0.34615384614718936, "Bleu_2": 0.2018018381949749, "Bleu_3": 0.11766213984418193, "Bleu_4": 0.07593264650632311, "METEOR": 0.1626825304482934, "ROUGE_L": 0.25176886792452824, "CIDEr": 8.255402420502519e-11, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.15789473684210525, "f": 0.14285714285714288, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a young woman sitting on the floor in a living room. She is wearing a black shirt and jeans and has her feet up on the couch. There is a window in the background with blinds open. The room is well lit and has a carpet on the floor."}, "482432": {"image_id": 482432, "Bleu_1": 0.2666666666607408, "Bleu_2": 0.1740776559517855, "Bleu_3": 0.12834519665251867, "Bleu_4": 0.10016820494780898, "METEOR": 0.18550278136789342, "ROUGE_L": 0.28073635765943455, "CIDEr": 9.391989648355029e-07, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.22727272727272727, "f": 0.20833333333333331, "fn": 17.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4, "f": 0.34782608695652173, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a sink with a toothbrush and toothpaste on the counter. The sink is made of white porcelain and has a faucet on the right side. The toothbrush is a black and white one with a long handle and soft bristles. The toothpaste"}, "330880": {"image_id": 330880, "Bleu_1": 0.27272727272107444, "Bleu_2": 0.23891934293536143, "Bleu_3": 0.1894121261728855, "Bleu_4": 0.14932763392911852, "METEOR": 0.2886982208544417, "ROUGE_L": 0.35012755102040816, "CIDEr": 5.01164481484166e-07, "SPICE": {"All": {"pr": 0.07407407407407407, "re": 0.11764705882352941, "f": 0.0909090909090909, "fn": 15.0, "numImages": 1.0, "fp": 25.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.5, "f": 0.26666666666666666, "fn": 2.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a man sitting at a table in a restaurant, looking at a pizza on the table. The pizza has a variety of toppings, including pepperoni, mushrooms, and olives. The man is wearing a black shirt and jeans, and has a beard."}, "201934": {"image_id": 201934, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.21797089763394864, "Bleu_3": 0.1681121136269389, "Bleu_4": 0.14033475286316116, "METEOR": 0.23038002797504958, "ROUGE_L": 0.2995440196422308, "CIDEr": 9.336216762446027e-11, "SPICE": {"All": {"pr": 0.2727272727272727, "re": 0.6428571428571429, "f": 0.3829787234042553, "fn": 5.0, "numImages": 1.0, "fp": 24.0, "tp": 9.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.6, "f": 0.42857142857142855, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.46153846153846156, "re": 1.0, "f": 0.631578947368421, "fn": 0.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a white school bus parked on the side of the road. There are several people standing around the bus, looking at it. The bus has a large bumper sticker on the front that reads, \"School Bus.\" There are also several other cars parked along the side of the road"}, "579462": {"image_id": 579462, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.15070298814247132, "Bleu_3": 0.07587175464144183, "Bleu_4": 9.619833423045096e-06, "METEOR": 0.2060514428173983, "ROUGE_L": 0.22344322344322343, "CIDEr": 3.518502287595791e-12, "SPICE": {"All": {"pr": 0.07692307692307693, "re": 0.08333333333333333, "f": 0.08, "fn": 22.0, "numImages": 1.0, "fp": 24.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The woman is standing in front of a bed with a suitcase on it. She is wearing a dress and has a look of concentration on her face. The room is dimly lit and there is a lamp on the nightstand.\n\nThe woman is standing in front of a bed with a suitcase on"}, "183657": {"image_id": 183657, "Bleu_1": 0.1428571428545919, "Bleu_2": 1.61164592802172e-09, "Bleu_3": 3.636764425856175e-12, "Bleu_4": 1.7356710121002362e-13, "METEOR": 0.1450160189423439, "ROUGE_L": 0.17329545454545456, "CIDEr": 5.705526375838106e-14, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.4, "f": 0.2857142857142857, "fn": 9.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.5, "f": 0.1818181818181818, "fn": 1.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 1.0, "f": 0.33333333333333337, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5714285714285714, "f": 0.4444444444444444, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image is of a cup of coffee floating in the water. The cup is made of ceramic and has a white handle. The cup is sitting on top of a rock in the middle of a frozen lake. The water is frozen and there are no other objects in the image. The sky is cloud"}, "352652": {"image_id": 352652, "Bleu_1": 0.14545454545190087, "Bleu_2": 0.07339758434041055, "Bleu_3": 4.6669078026091577e-07, "Bleu_4": 1.1824181296865702e-09, "METEOR": 0.1297297297297297, "ROUGE_L": 0.17221908526256355, "CIDEr": 1.3877194930447946e-12, "SPICE": {"All": {"pr": 0.10714285714285714, "re": 0.11538461538461539, "f": 0.11111111111111112, "fn": 23.0, "numImages": 1.0, "fp": 25.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.18181818181818182, "f": 0.16666666666666669, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a car parked in the snow. The car has a large pile of snow on top of it. There are several people standing around the car, looking at it. The car has a large pile of snow on top of it. There are several people standing around the car, looking at it"}, "339823": {"image_id": 339823, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.18504536999744126, "Bleu_3": 0.10961273969013748, "Bleu_4": 1.267661295444545e-05, "METEOR": 0.23089892651657923, "ROUGE_L": 0.21585279547062985, "CIDEr": 3.842432283853843e-12, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.25, "f": 0.2553191489361702, "fn": 18.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.125, "re": 0.1, "f": 0.11111111111111112, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.5, "f": 0.36363636363636365, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.375, "re": 0.3, "f": 0.33333333333333326, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The woman in the image is wearing a yellow and orange dress with an orange and yellow pattern on it. She is holding an umbrella in her hand and smiling at the camera. The background is a blue sky with some clouds in it.\n\nThe woman in the image is wearing a yellow and"}, "203690": {"image_id": 203690, "Bleu_1": 0.15094339622356714, "Bleu_2": 0.12047318414543882, "Bleu_3": 0.09486600651870754, "Bleu_4": 0.07644480232985446, "METEOR": 0.16303070755712348, "ROUGE_L": 0.226906385616863, "CIDEr": 1.8525642449158818e-12, "SPICE": {"All": {"pr": 0.07692307692307693, "re": 0.16666666666666666, "f": 0.10526315789473684, "fn": 10.0, "numImages": 1.0, "fp": 24.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.13333333333333333, "re": 0.3333333333333333, "f": 0.19047619047619044, "fn": 4.0, "numImages": 1.0, "fp": 13.0, "tp": 2.0}}, "caption": "The image shows a group of people standing on a beach with a large wave crashing against the shore. The people are wearing wetsuits and surfboards, and the sky is cloudy with a sun setting in the background. The beach is covered in sand and rocks, and there are some buildings in the"}, "344614": {"image_id": 344614, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.21711298242232585, "Bleu_3": 0.1644454671999562, "Bleu_4": 0.1265598373565068, "METEOR": 0.18948879154604648, "ROUGE_L": 0.28126801152737757, "CIDEr": 5.455634671683295e-13, "SPICE": {"All": {"pr": 0.2631578947368421, "re": 0.16666666666666666, "f": 0.2040816326530612, "fn": 25.0, "numImages": 1.0, "fp": 14.0, "tp": 5.0}, "Relation": {"pr": 0.2, "re": 0.1111111111111111, "f": 0.14285714285714285, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 4.0}}, "caption": "The image shows a clock tower in the middle of a city. The clock tower is made of stone and has a large clock face on the front. The clock face has numbers and hands, and the clock is ticking away. There are buildings in the background, including a tall skyscraper with a large window"}, "573549": {"image_id": 573549, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.13736056394607243, "Bleu_3": 0.07178791139239354, "Bleu_4": 9.274617069102212e-06, "METEOR": 0.14395018829866163, "ROUGE_L": 0.22195269860521533, "CIDEr": 9.94251132058201e-13, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.19047619047619047, "f": 0.17777777777777778, "fn": 17.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5, "f": 0.4210526315789474, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a street with a few people walking on it. There are some buildings on either side of the street, and a few cars parked on the sidewalk. The sky is cloudy and there are some trees in the background.\n\nThe image is taken from a bird's eye view, so"}, "522941": {"image_id": 522941, "Bleu_1": 0.2448979591786756, "Bleu_2": 0.17496355305233346, "Bleu_3": 0.10921336092038615, "Bleu_4": 1.2972311576394404e-05, "METEOR": 0.19565349356434455, "ROUGE_L": 0.23091482649842268, "CIDEr": 3.624697238315919e-10, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.2777777777777778, "f": 0.21276595744680854, "fn": 13.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.7142857142857143, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a large elephant standing in a field with people gathered around it. The elephant has a large tusk and is wearing a decorative headpiece. The people in the background are dressed in traditional clothing and appear to be watching the elephant. The sky is clear and"}, "511662": {"image_id": 511662, "Bleu_1": 0.22448979591378598, "Bleu_2": 0.13677530110522793, "Bleu_3": 0.07355956748968669, "Bleu_4": 9.644726515398138e-06, "METEOR": 0.23426275967849916, "ROUGE_L": 0.2622527944969905, "CIDEr": 1.043072679871101e-07, "SPICE": {"All": {"pr": 0.25, "re": 0.3157894736842105, "f": 0.27906976744186046, "fn": 13.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.38461538461538464, "re": 0.8333333333333334, "f": 0.5263157894736842, "fn": 1.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a cruise ship in the foreground, with palm trees and a beach in the background. The ship is surrounded by colorful umbrellas and chairs on the sand. The sky is clear and blue, with a few clouds in the distance.\n\nThe image is taken from a"}, "377371": {"image_id": 377371, "Bleu_1": 0.19148936169805347, "Bleu_2": 0.1442707907155283, "Bleu_3": 0.09743719400736509, "Bleu_4": 0.06771430219804507, "METEOR": 0.2073539014758578, "ROUGE_L": 0.2621883826599533, "CIDEr": 1.9284389291105435e-08, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.16, "f": 0.1818181818181818, "fn": 21.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a wooden cutting board with sliced nuts on it. There is also a banana peel on the cutting board. The background is a green and white striped tablecloth.\n\nThe image shows a wooden cutting board with sliced nuts on it. There is also a"}, "170813": {"image_id": 170813, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.1313579154832357, "Bleu_3": 7.061638347858738e-07, "Bleu_4": 1.6457686856089725e-09, "METEOR": 0.20436609309724313, "ROUGE_L": 0.18252543387193296, "CIDEr": 1.9955585080928655e-11, "SPICE": {"All": {"pr": 0.16129032258064516, "re": 0.22727272727272727, "f": 0.18867924528301885, "fn": 17.0, "numImages": 1.0, "fp": 26.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4444444444444444, "f": 0.39999999999999997, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a person sitting on a bench in a park. The person is wearing a black jacket and black pants. The bench is made of wood and has a backrest. The person is looking down at their phone. The park is surrounded by trees and grass.\n\nThe image is"}, "347210": {"image_id": 347210, "Bleu_1": 0.2727272727223141, "Bleu_2": 0.20100756304815393, "Bleu_3": 0.1150958437258178, "Bleu_4": 1.3085607656499973e-05, "METEOR": 0.2367494843538888, "ROUGE_L": 0.28126801152737757, "CIDEr": 2.9389787771836053e-13, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.15789473684210525, "f": 0.13043478260869565, "fn": 16.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.3333333333333333, "f": 0.23529411764705885, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a park with a bench in the middle of the grass. The bench is made of metal and has a wooden seat. The park is surrounded by trees and there are no people in sight. The sky is clear and there are no clouds.\n\nThe image shows a park with a bench"}, "175494": {"image_id": 175494, "Bleu_1": 0.15686274509496353, "Bleu_2": 0.1252448582145496, "Bleu_3": 0.08618888098293648, "Bleu_4": 0.060433552207849656, "METEOR": 0.1704490484467464, "ROUGE_L": 0.22584228063680117, "CIDEr": 8.030292408653525e-11, "SPICE": {"All": {"pr": 0.16, "re": 0.17391304347826086, "f": 0.16666666666666666, "fn": 19.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5, "f": 0.4210526315789474, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a dog lying on a bed with a cartoon character on the wall. The dog is wearing a collar and tag. The bed has a blue and white striped blanket and a pillow. The wall has a cartoon character on it. The room has a window with curtains"}, "265879": {"image_id": 265879, "Bleu_1": 0.30188679244713423, "Bleu_2": 0.18663625434394554, "Bleu_3": 0.12701286121266542, "Bleu_4": 0.08000981634230918, "METEOR": 0.29065015804006084, "ROUGE_L": 0.27216954824316786, "CIDEr": 6.87341479859742e-11, "SPICE": {"All": {"pr": 0.1875, "re": 0.13333333333333333, "f": 0.15584415584415587, "fn": 39.0, "numImages": 1.0, "fp": 26.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 16.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.09090909090909091, "re": 0.07692307692307693, "f": 0.08333333333333334, "fn": 12.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.3125, "f": 0.35714285714285715, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a group of people sitting at a table outside in the evening. They are wearing casual clothing and smiling. There are several bottles of wine on the table and a man is holding a glass of wine. The table is covered with a white tablecloth and there are candles on"}, "433998": {"image_id": 433998, "Bleu_1": 0.3207547169750801, "Bleu_2": 0.15707781436867324, "Bleu_3": 0.07850304593345392, "Bleu_4": 9.917960624356155e-06, "METEOR": 0.19689864365531096, "ROUGE_L": 0.2473390775468829, "CIDEr": 0.011804195354664137, "SPICE": {"All": {"pr": 0.3103448275862069, "re": 0.36, "f": 0.3333333333333333, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 9.0}, "Relation": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.1111111111111111, "f": 0.13333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.46153846153846156, "re": 0.6, "f": 0.5217391304347826, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a baseball game in progress. There are several players on the field, including a pitcher and a catcher. The crowd is cheering and waving their arms in excitement. The scoreboard shows the score of the game, with the home team leading by a large margin. The field is covered in"}, "286711": {"image_id": 286711, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.12126781251581158, "Bleu_3": 0.0665028658415205, "Bleu_4": 8.801997699590313e-06, "METEOR": 0.17680927206201166, "ROUGE_L": 0.22536945812807885, "CIDEr": 8.083415504943692e-11, "SPICE": {"All": {"pr": 0.08, "re": 0.125, "f": 0.09756097560975609, "fn": 14.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.3333333333333333, "f": 0.26666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a baseball game in progress. The players are wearing uniforms with numbers on the back and the umpire is wearing a white shirt and pants. The crowd is seated in the stands and watching the game. The field is made of grass and there are dirt paths leading to"}, "552744": {"image_id": 552744, "Bleu_1": 0.3454545454482645, "Bleu_2": 0.2529289702084681, "Bleu_3": 0.16901571660708514, "Bleu_4": 0.12918877974107223, "METEOR": 0.20514198459822783, "ROUGE_L": 0.26702102712420855, "CIDEr": 2.0205335693957297e-11, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.17391304347826086, "f": 0.1568627450980392, "fn": 19.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image is of a woman standing in front of a wall with stuffed animals hanging from it. The woman is wearing a black shirt and has long brown hair. She is holding a stuffed animal in her left hand and has a stuffed animal on her right shoulder. The wall behind her is white"}, "447279": {"image_id": 447279, "Bleu_1": 0.3541666666592882, "Bleu_2": 0.27450794146236435, "Bleu_3": 0.20157685554314383, "Bleu_4": 0.15286486774594144, "METEOR": 0.3302488429012665, "ROUGE_L": 0.39102564102564097, "CIDEr": 2.479520122851644e-09, "SPICE": {"All": {"pr": 0.16, "re": 0.17391304347826086, "f": 0.16666666666666666, "fn": 19.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.1111111111111111, "f": 0.16666666666666666, "fn": 8.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.3333333333333333, "f": 0.2608695652173913, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a group of people standing on the sidewalk in front of a streetlight. They are all wearing yellow vests and helmets. There are several horses standing in the street, some of which are wearing saddles and bridles. The street is wet and there are pudd"}, "409217": {"image_id": 409217, "Bleu_1": 0.2888888888824692, "Bleu_2": 0.24308621739673564, "Bleu_3": 0.16034588667675892, "Bleu_4": 0.11836915408169045, "METEOR": 0.23840910047460095, "ROUGE_L": 0.3286195286195286, "CIDEr": 8.694798990526198e-07, "SPICE": {"All": {"pr": 0.21212121212121213, "re": 0.2413793103448276, "f": 0.22580645161290322, "fn": 22.0, "numImages": 1.0, "fp": 26.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2727272727272727, "f": 0.3, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.6, "re": 0.42857142857142855, "f": 0.5, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 3.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5, "f": 0.380952380952381, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a plate of food with steak, broccoli, and beans on it. There is also a fork and knife on the plate. The plate is on a green surface.\n\nThe image shows a plate of food with steak, broccoli, and beans on it."}, "28114": {"image_id": 28114, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.16087993330478273, "Bleu_3": 0.08083557992554675, "Bleu_4": 1.0242161260431777e-05, "METEOR": 0.16136557340474889, "ROUGE_L": 0.18252543387193296, "CIDEr": 3.090171935482994e-11, "SPICE": {"All": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a bus driving down the street. The bus is orange and has the words \"Van\" written on the side. There are people walking on the sidewalk and biking on the sidewalk. There are also buildings on either side of the street.\n\nThe image is taken from a bird"}, "33994": {"image_id": 33994, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.16496470132732644, "Bleu_3": 8.110929465213236e-07, "Bleu_4": 1.8074257588841565e-09, "METEOR": 0.16474964075508414, "ROUGE_L": 0.21356050881082975, "CIDEr": 9.365615792361453e-11, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.23809523809523808, "f": 0.19999999999999998, "fn": 16.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.2, "f": 0.15384615384615385, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4, "f": 0.3636363636363636, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image is of a flower arrangement on a table. The flower is yellow and has a long stem. There are several other flowers on the table as well.\n\nThe table is made of a dark wood and has a green tablecloth. There are several other flowers on the table as well.\n\nThe"}, "278509": {"image_id": 278509, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.21797089763394864, "Bleu_3": 0.1681121136269389, "Bleu_4": 0.13059620291535007, "METEOR": 0.2827157428062086, "ROUGE_L": 0.28773584905660377, "CIDEr": 1.2109256523342051e-10, "SPICE": {"All": {"pr": 0.13793103448275862, "re": 0.14814814814814814, "f": 0.14285714285714285, "fn": 23.0, "numImages": 1.0, "fp": 25.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image is of a black and white photograph of a motorcycle parked on the side of a road. The motorcycle has a black and white striped seat and handlebars, and the tires are black and white. The road is empty and there are no other vehicles or pedestrians in the image"}, "544975": {"image_id": 544975, "Bleu_1": 0.27272727272107444, "Bleu_2": 0.19507682662422146, "Bleu_3": 0.09676553259733332, "Bleu_4": 1.2192550254115238e-05, "METEOR": 0.28786714731146773, "ROUGE_L": 0.23890339425587467, "CIDEr": 2.1215207621077317e-06, "SPICE": {"All": {"pr": 0.07407407407407407, "re": 0.11764705882352941, "f": 0.0909090909090909, "fn": 15.0, "numImages": 1.0, "fp": 25.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2222222222222222, "f": 0.19999999999999998, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a giraffe standing next to a zebra in a zoo. The giraffe is looking down at the zebra, while the zebra is looking up at the giraffe. The giraffe has a long neck and legs, while the zebra has a short"}, "158806": {"image_id": 158806, "Bleu_1": 0.2105263157857803, "Bleu_2": 0.1501878522938693, "Bleu_3": 0.09360785780695649, "Bleu_4": 1.1101605489146436e-05, "METEOR": 0.20827958853601788, "ROUGE_L": 0.26798462383305877, "CIDEr": 3.934212809747285e-14, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.23529411764705882, "f": 0.186046511627907, "fn": 13.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5714285714285714, "f": 0.4210526315789474, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a dog sitting on the ground next to a person who is holding a plate with a piece of toast on it. The dog is looking up at the person with its mouth open. The person is wearing a white shirt and has a plate of toast in their hand. The dog is we"}, "267321": {"image_id": 267321, "Bleu_1": 0.26315789473222534, "Bleu_2": 0.20565318641151553, "Bleu_3": 0.14543068929524094, "Bleu_4": 0.10331208012034214, "METEOR": 0.20725857096401282, "ROUGE_L": 0.3469237018982199, "CIDEr": 2.23914042239498e-08, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.08333333333333333, "f": 0.08333333333333333, "fn": 22.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.25, "f": 0.23529411764705882, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a living room with a couch, a coffee table, and a chair. There is a window on the left side of the room and a door on the right side. The room is well lit and has a white ceiling and white walls. There are two plants on the coffee table and a v"}, "137188": {"image_id": 137188, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.20982172726164103, "Bleu_3": 0.11919074177013061, "Bleu_4": 0.07590831443387894, "METEOR": 0.20115069694372784, "ROUGE_L": 0.30198019801980197, "CIDEr": 1.1873405258842854e-11, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.25, "f": 0.2173913043478261, "fn": 15.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.2, "re": 0.2857142857142857, "f": 0.23529411764705882, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.5, "f": 0.37499999999999994, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a cat sitting on a table in front of a window. The cat is looking at a stuffed animal on the table. There are other stuffed animals on the table as well. The window has a view of the outside.\n\nThe cat is a grey and white cat with a fluffy"}, "132702": {"image_id": 132702, "Bleu_1": 0.20930232557652792, "Bleu_2": 0.15785101684950267, "Bleu_3": 0.12216422810817773, "Bleu_4": 1.461144082685169e-05, "METEOR": 0.2126915007149501, "ROUGE_L": 0.26852531181217903, "CIDEr": 7.08720535604514e-08, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.2631578947368421, "f": 0.20833333333333334, "fn": 14.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.2, "f": 0.12500000000000003, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.16666666666666666, "re": 0.5, "f": 0.25, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Object": {"pr": 0.2727272727272727, "re": 0.375, "f": 0.3157894736842105, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a plate of steamed broccoli on a white tablecloth. The broccoli is cut into florets and arranged on the plate in a neat, symmetrical pattern. The plate is covered with a white napkin. The background is a light brown color."}, "151075": {"image_id": 151075, "Bleu_1": 0.3199999999936, "Bleu_2": 0.19794866371815806, "Bleu_3": 0.0934590374320519, "Bleu_4": 1.1479990094513042e-05, "METEOR": 0.2873380644438548, "ROUGE_L": 0.23252858958068615, "CIDEr": 8.101213127822283e-11, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.16, "f": 0.14545454545454545, "fn": 21.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.2222222222222222, "f": 0.30769230769230765, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.25, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.13333333333333333, "re": 0.2222222222222222, "f": 0.16666666666666669, "fn": 7.0, "numImages": 1.0, "fp": 13.0, "tp": 2.0}}, "caption": "The image shows a person surfing on a wave in the ocean. The person is wearing a black wetsuit and a helmet, and is holding onto the surfboard with one hand while riding the wave with the other. The wave is large and white, and the person is jumping off"}, "516372": {"image_id": 516372, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.18314741859482847, "Bleu_3": 0.13715745469957313, "Bleu_4": 0.08433775160451765, "METEOR": 0.21519959962421165, "ROUGE_L": 0.1821983273596177, "CIDEr": 2.9598929542148527e-12, "SPICE": {"All": {"pr": 0.2, "re": 0.20833333333333334, "f": 0.20408163265306126, "fn": 19.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.125, "f": 0.11764705882352941, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4444444444444444, "f": 0.39999999999999997, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a parked car in a parking lot with a fire hydrant in front of it. The car is a black truck with a white hood and a black roof. The fire hydrant is a red and white metal box with a spout on top. The grass is green and there are"}, "397958": {"image_id": 397958, "Bleu_1": 0.2105263157857803, "Bleu_2": 0.20335531332419785, "Bleu_3": 0.18186373886565455, "Bleu_4": 0.1607862695528365, "METEOR": 0.231953019380073, "ROUGE_L": 0.26798462383305877, "CIDEr": 2.706879269833048e-13, "SPICE": {"All": {"pr": 0.23333333333333334, "re": 0.30434782608695654, "f": 0.26415094339622636, "fn": 16.0, "numImages": 1.0, "fp": 23.0, "tp": 7.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.7142857142857143, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a black and white cow standing in a field with tall grass and a fence in the background. The cow has a white patch on its forehead and is looking directly at the camera. The sky is blue and there are clouds in the background.\n\nThe cow is standing in a field with tall"}, "154004": {"image_id": 154004, "Bleu_1": 0.27083333332769105, "Bleu_2": 0.2400502311449643, "Bleu_3": 0.17112053874527605, "Bleu_4": 0.10272436745896128, "METEOR": 0.23757019240583027, "ROUGE_L": 0.31077147016011636, "CIDEr": 1.724914251852541e-09, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.18181818181818182, "f": 0.18181818181818182, "fn": 18.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of people sitting on the beach, looking out at the ocean. They are wearing swimsuits and sunglasses, and some of them are holding surfboards. The beach is sandy and there are palm trees in the background. The sky is blue and there are"}, "179599": {"image_id": 179599, "Bleu_1": 0.3584905660309719, "Bleu_2": 0.24909123653704068, "Bleu_3": 0.15396523123679995, "Bleu_4": 1.6437073337356213e-05, "METEOR": 0.22550595731288245, "ROUGE_L": 0.28785708266621723, "CIDEr": 4.940482825423316e-11, "SPICE": {"All": {"pr": 0.1875, "re": 0.16666666666666666, "f": 0.17647058823529413, "fn": 15.0, "numImages": 1.0, "fp": 13.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}}, "caption": "The image shows a baseball player in a blue jersey and white pants standing on a mound, ready to throw a pitch. The player has a glove on his right hand and is holding the ball in his left hand. The background is a green field with a dirt infield and a fence"}, "282553": {"image_id": 282553, "Bleu_1": 0.2909090909038017, "Bleu_2": 0.12712834523041283, "Bleu_3": 6.730845772977349e-07, "Bleu_4": 1.5561497729243848e-09, "METEOR": 0.18298445543034644, "ROUGE_L": 0.21095100864553315, "CIDEr": 1.0286945434828662e-12, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.14285714285714285, "f": 0.1379310344827586, "fn": 24.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.18181818181818182, "re": 0.14285714285714285, "f": 0.16, "fn": 12.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows two people walking on a dirt path in the middle of a field at sunset. The sky is a deep blue and there are some clouds in the distance. The people are wearing casual clothing and are walking in the same direction. The path is lined with tall grass and there are"}, "53529": {"image_id": 53529, "Bleu_1": 0.3333333333263889, "Bleu_2": 0.2526455763146363, "Bleu_3": 0.1770551317386454, "Bleu_4": 0.10538486915683062, "METEOR": 0.28875207865015173, "ROUGE_L": 0.3740245261984392, "CIDEr": 5.935691752926389e-06, "SPICE": {"All": {"pr": 0.125, "re": 0.13636363636363635, "f": 0.13043478260869565, "fn": 19.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a dog sitting in the back of a truck with a green shamrock on its head. The truck is decorated with green and white streamers and balloons. The dog is wearing a green collar with a shamrock on it. The driver is wearing a green"}, "13168": {"image_id": 13168, "Bleu_1": 0.2181818181778513, "Bleu_2": 0.12712834523041283, "Bleu_3": 6.730845772977349e-07, "Bleu_4": 1.5561497729243848e-09, "METEOR": 0.22089974764513098, "ROUGE_L": 0.17579250720461098, "CIDEr": 1.6884293744165997e-12, "SPICE": {"All": {"pr": 0.2777777777777778, "re": 0.29411764705882354, "f": 0.28571428571428575, "fn": 12.0, "numImages": 1.0, "fp": 13.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.375, "re": 0.6, "f": 0.4615384615384615, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}}, "caption": "The image shows a train on the tracks at night. The train is made of silver and has windows on the sides. There are people standing on the platform and others inside the train. The train is moving at a slow pace. The sky is dark and there are streetlights on the platform.\n\nThe image"}, "528738": {"image_id": 528738, "Bleu_1": 0.2142857142806123, "Bleu_2": 0.10223972648618562, "Bleu_3": 0.06393320193606379, "Bleu_4": 9.047502044032776e-06, "METEOR": 0.1859158706649367, "ROUGE_L": 0.18236173393124064, "CIDEr": 6.852738490859753e-07, "SPICE": {"All": {"pr": 0.24, "re": 0.20689655172413793, "f": 0.22222222222222224, "fn": 23.0, "numImages": 1.0, "fp": 19.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.1111111111111111, "f": 0.15384615384615383, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.4166666666666667, "f": 0.43478260869565216, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a table with various vegetables and fruits on it. There are carrots, beets, potatoes, onions, garlic, lemons, and other vegetables and fruits on the table. The table is made of wood and has a white tablecloth on it. There"}, "368193": {"image_id": 368193, "Bleu_1": 0.1666666666631945, "Bleu_2": 0.08421519210487878, "Bleu_3": 5.362175432481669e-07, "Bleu_4": 1.3605128106221755e-09, "METEOR": 0.13296040933704215, "ROUGE_L": 0.23461538461538461, "CIDEr": 6.334393182021824e-10, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.09090909090909091, "f": 0.1016949152542373, "fn": 30.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.21428571428571427, "f": 0.22222222222222224, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a group of people riding horses down the street. The horses are brown and white and have saddles on them. The people are wearing cowboy hats and riding boots. The street is lined with houses and trees. The sky is clear and blue.\n\nThe image"}, "538064": {"image_id": 538064, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.19154015512649578, "Bleu_3": 0.1283943344001008, "Bleu_4": 0.08026327033859419, "METEOR": 0.23609531594599206, "ROUGE_L": 0.31443298969072164, "CIDEr": 6.2602135074995806e-12, "SPICE": {"All": {"pr": 0.2413793103448276, "re": 0.35, "f": 0.2857142857142857, "fn": 13.0, "numImages": 1.0, "fp": 22.0, "tp": 7.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.14285714285714285, "f": 0.1111111111111111, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 1.0, "f": 0.4, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5714285714285714, "f": 0.4210526315789474, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The statue is made of bronze and depicts a man in a suit and hat holding a child in his arms. The statue is located on a pedestal in the middle of a city street. There are several buildings and cars in the background.\n\nThe statue is a tribute to the man who founded"}, "265636": {"image_id": 265636, "Bleu_1": 0.20408163264889634, "Bleu_2": 0.15971914124669148, "Bleu_3": 0.1027737074612707, "Bleu_4": 1.23943015938953e-05, "METEOR": 0.21262946036936298, "ROUGE_L": 0.16126900198281557, "CIDEr": 1.931362661066604e-10, "SPICE": {"All": {"pr": 0.35, "re": 0.22580645161290322, "f": 0.2745098039215686, "fn": 24.0, "numImages": 1.0, "fp": 13.0, "tp": 7.0}, "Relation": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.625, "re": 0.4166666666666667, "f": 0.5, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 5.0}}, "caption": "The image shows a stuffed animal sitting on a couch next to a birthday card. The card has the words \"happy birthday\" written on it. The stuffed animal is wearing a bow tie and holding a present in its paws.\n\nThe background of the image is a red and"}, "577796": {"image_id": 577796, "Bleu_1": 0.2727272727223141, "Bleu_2": 0.21320071635169824, "Bleu_3": 0.1197045013172918, "Bleu_4": 1.347665235445903e-05, "METEOR": 0.20646797408812187, "ROUGE_L": 0.2127164942461932, "CIDEr": 2.2464760945951057e-13, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.18181818181818182, "f": 0.17777777777777776, "fn": 18.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.125, "f": 0.15384615384615385, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a bathroom with three urinals on the wall. The urinals are made of white porcelain and have a round shape with a hole in the center. They are mounted on the wall with metal brackets and have a metal handle on the side. The wall is made of white tiles and has"}, "554046": {"image_id": 554046, "Bleu_1": 0.20370370369993146, "Bleu_2": 0.175350303278286, "Bleu_3": 0.13323650444176333, "Bleu_4": 0.08252297546110998, "METEOR": 0.22987496804897145, "ROUGE_L": 0.25507765830346474, "CIDEr": 8.599455305088307e-13, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.12, "f": 0.13953488372093023, "fn": 22.0, "numImages": 1.0, "fp": 15.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.2727272727272727, "f": 0.3157894736842105, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a group of sheep standing in a snowy field. They are wearing coats and appear to be enjoying the snow. The fence in the background is made of wood and has a gate in the center. There is a small shed in the corner of the field. The sky is cloudy"}, "316534": {"image_id": 316534, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.15041420939612582, "Bleu_3": 0.0767719506444456, "Bleu_4": 9.802862511748142e-06, "METEOR": 0.1794632293702579, "ROUGE_L": 0.2167219327333018, "CIDEr": 2.1455198697971766e-11, "SPICE": {"All": {"pr": 0.1875, "re": 0.14285714285714285, "f": 0.16216216216216214, "fn": 36.0, "numImages": 1.0, "fp": 26.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.09090909090909091, "f": 0.1111111111111111, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.29411764705882354, "f": 0.3225806451612903, "fn": 12.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows a group of cows grazing on a grassy hillside next to a body of water. The cows are brown and black with white spots on their faces. They are standing in a line, looking out at the water. In the background, there are palm trees and a blue sky"}, "158950": {"image_id": 158950, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.16205747826515882, "Bleu_3": 0.07913235571850134, "Bleu_4": 9.880177230491347e-06, "METEOR": 0.23354507921100884, "ROUGE_L": 0.21542083578575633, "CIDEr": 5.042995945078241e-13, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.0967741935483871, "f": 0.10344827586206896, "fn": 28.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a train traveling along a track in the middle of a field. The train is blue and yellow and has a number on the side. There are trees and bushes on either side of the track and a cloudy sky above.\n\nThe train is traveling in a straight line and appears to"}, "524822": {"image_id": 524822, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.18216065373578985, "Bleu_3": 0.1491545080862942, "Bleu_4": 0.10787871557193922, "METEOR": 0.24851198385164192, "ROUGE_L": 0.32370283018867924, "CIDEr": 6.302209078726206e-11, "SPICE": {"All": {"pr": 0.23809523809523808, "re": 0.20833333333333334, "f": 0.22222222222222224, "fn": 19.0, "numImages": 1.0, "fp": 16.0, "tp": 5.0}, "Relation": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4444444444444444, "f": 0.4444444444444444, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a living room with white couches, a white coffee table, and a white fireplace. There is a large window on one wall with white curtains. The room is well lit with natural light from the window. There is a flat screen TV on the wall above the fireplace. The"}, "248111": {"image_id": 248111, "Bleu_1": 0.3061224489733445, "Bleu_2": 0.2258769757216551, "Bleu_3": 0.12948675740619273, "Bleu_4": 1.4739391640949857e-05, "METEOR": 0.1913744080509176, "ROUGE_L": 0.26341764342998153, "CIDEr": 8.705638819816208e-10, "SPICE": {"All": {"pr": 0.2727272727272727, "re": 0.2222222222222222, "f": 0.24489795918367346, "fn": 21.0, "numImages": 1.0, "fp": 16.0, "tp": 6.0}, "Relation": {"pr": 0.2857142857142857, "re": 0.18181818181818182, "f": 0.2222222222222222, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4, "f": 0.3636363636363636, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "This is a kitchen with white cabinets and a white refrigerator. There is a stove and sink in the kitchen. The floor is made of hardwood.\n\nThe walls are painted white and there are no windows in the room. The ceiling is made of white plaster. The room is"}, "409964": {"image_id": 409964, "Bleu_1": 0.3499999999912501, "Bleu_2": 0.18946618668147105, "Bleu_3": 0.09812054324169892, "Bleu_4": 1.2640653505172758e-05, "METEOR": 0.3166702460578285, "ROUGE_L": 0.3139705882352941, "CIDEr": 2.384895858121937e-06, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.2222222222222222, "f": 0.19999999999999998, "fn": 21.0, "numImages": 1.0, "fp": 27.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2857142857142857, "f": 0.30769230769230765, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.3076923076923077, "re": 0.3333333333333333, "f": 0.32, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a man playing tennis on a court. He is wearing a white shirt and black shorts, and has a tennis racket in his hand. The court is made of grass and there are trees in the background."}, "337987": {"image_id": 337987, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.22893427324353557, "Bleu_3": 0.18218943070472146, "Bleu_4": 0.1475756952410161, "METEOR": 0.3102146301371168, "ROUGE_L": 0.28367380833748546, "CIDEr": 9.557793747619628e-12, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.12, "f": 0.12499999999999997, "fn": 22.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3, "f": 0.3, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a small bird perched on a branch of a tree. The bird has a brown and white striped body and a black stripe on its head. It has a long, curved beak and its eyes are closed. The bird is perched on the branch with its wings spread out to the"}, "544104": {"image_id": 544104, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.128337789581408, "Bleu_3": 6.952980477333491e-07, "Bleu_4": 1.626739259997693e-09, "METEOR": 0.17392103722917424, "ROUGE_L": 0.18252543387193296, "CIDEr": 1.9023319242545702e-10, "SPICE": {"All": {"pr": 0.47058823529411764, "re": 0.34782608695652173, "f": 0.39999999999999997, "fn": 15.0, "numImages": 1.0, "fp": 9.0, "tp": 8.0}, "Relation": {"pr": 0.75, "re": 0.3, "f": 0.4285714285714285, "fn": 7.0, "numImages": 1.0, "fp": 1.0, "tp": 3.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4444444444444444, "f": 0.4444444444444444, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a harbor with several boats docked at the pier. The boats are small and colorful, with some of them having sails up. The water is calm and clear, reflecting the sun's rays. The buildings on the shore are old and made of stone, with some of them"}, "121210": {"image_id": 121210, "Bleu_1": 0.29787234041919425, "Bleu_2": 0.22760466886504876, "Bleu_3": 0.13204641200979725, "Bleu_4": 1.5124526410231296e-05, "METEOR": 0.2437527062022072, "ROUGE_L": 0.2781758957654723, "CIDEr": 1.350895500679024e-07, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.1, "f": 0.11320754716981132, "fn": 27.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.25, "f": 0.2727272727272727, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a plate of food with a stew made with beef, potatoes, and carrots. The dish is served with a side of green beans and a sprig of parsley on top. The plate is placed on a white tablecloth with a fork and knife on"}, "46551": {"image_id": 46551, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.20982172726164103, "Bleu_3": 0.15017092450877165, "Bleu_4": 0.10735056776927294, "METEOR": 0.14373797239344516, "ROUGE_L": 0.24710648148148148, "CIDEr": 8.069941085570624e-10, "SPICE": {"All": {"pr": 0.16, "re": 0.2, "f": 0.17777777777777778, "fn": 16.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of people standing in a room with a computer and a television in the background. The people are all wearing black shirts and jeans, and one of them is holding a camera. The room is made of brick and has a wooden ceiling. There are some posters on the"}, "535588": {"image_id": 535588, "Bleu_1": 0.340909090901343, "Bleu_2": 0.26711994597152616, "Bleu_3": 1.193221624352811e-06, "Bleu_4": 2.537142570239067e-09, "METEOR": 0.2371157205456722, "ROUGE_L": 0.266209476309227, "CIDEr": 1.3837889873667168e-05, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.13636363636363635, "f": 0.12244897959183673, "fn": 19.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2857142857142857, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "This image shows a bus parked on the side of the road. The bus has a bike rack on the back and a person is standing next to it, looking at the bike. The sky is cloudy and there are trees in the background."}, "173997": {"image_id": 173997, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.16823164622440862, "Bleu_3": 0.10353608435894587, "Bleu_4": 1.220608250473103e-05, "METEOR": 0.2090315102183168, "ROUGE_L": 0.21721068249258166, "CIDEr": 1.3051511842525659e-11, "SPICE": {"All": {"pr": 0.15625, "re": 0.3125, "f": 0.20833333333333334, "fn": 11.0, "numImages": 1.0, "fp": 27.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.3333333333333333, "f": 0.2222222222222222, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows two people sitting on a bench in a park. They are wearing red jackets and have their backs to the camera. The bench is made of wood and has a small table on it. There are trees and bushes in the background. The sky is cloudy and there is a"}, "320396": {"image_id": 320396, "Bleu_1": 0.19565217390879022, "Bleu_2": 0.0932504808219817, "Bleu_3": 5.824828742198937e-07, "Bleu_4": 1.4641826518869363e-09, "METEOR": 0.16649402128063678, "ROUGE_L": 0.27619663648124193, "CIDEr": 1.002993944893116e-08, "SPICE": {"All": {"pr": 0.16, "re": 0.14285714285714285, "f": 0.15094339622641512, "fn": 24.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.1, "f": 0.11764705882352941, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a man playing frisbee on a beach with a group of seagulls in the background. The man is wearing a black shirt and white pants, and he is holding a frisbee in his right hand. The seagulls are flying in the sky above"}, "221282": {"image_id": 221282, "Bleu_1": 0.21999999999560005, "Bleu_2": 0.1772810520820019, "Bleu_3": 0.1094051585098822, "Bleu_4": 1.2919743313490496e-05, "METEOR": 0.2628846424089891, "ROUGE_L": 0.2901307966706302, "CIDEr": 3.284359607960362e-09, "SPICE": {"All": {"pr": 0.25, "re": 0.21212121212121213, "f": 0.22950819672131145, "fn": 26.0, "numImages": 1.0, "fp": 21.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.36363636363636365, "re": 0.4444444444444444, "f": 0.39999999999999997, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.3, "re": 0.21428571428571427, "f": 0.25, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a woman in a white apron and gloves kneeling on a wooden table, cutting a pizza with a knife. The woman is surrounded by other people in white aprons and gloves, all of whom are working on pizzas. The room is filled with the smell of p"}, "25143": {"image_id": 25143, "Bleu_1": 0.2608695652117203, "Bleu_2": 0.22841609628304363, "Bleu_3": 0.1923316074401127, "Bleu_4": 0.16959513235152923, "METEOR": 0.24950049483049688, "ROUGE_L": 0.3084702907711757, "CIDEr": 4.640226236188647e-08, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 12.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5, "f": 0.36363636363636365, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a group of people playing frisbee in a park. They are all wearing green shirts and white pants. One person is throwing the frisbee while the others are running to catch it. The grass is green and there are trees in the background."}, "52835": {"image_id": 52835, "Bleu_1": 0.2448979591786756, "Bleu_2": 0.07142857142709856, "Bleu_3": 4.770332929412208e-07, "Bleu_4": 1.2394301593895302e-09, "METEOR": 0.149008127451453, "ROUGE_L": 0.18815545959284388, "CIDEr": 2.275273705850482e-10, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.09090909090909091, "f": 0.1111111111111111, "fn": 30.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.23076923076923078, "f": 0.2857142857142857, "fn": 10.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a lunch box with various food items inside, including vegetables, fruits, and meat. The box is made of plastic and has a clear plastic lid. The food items are arranged in a row on a tray inside the box. The tray is divided into three compartments,"}, "300962": {"image_id": 300962, "Bleu_1": 0.31481481480898493, "Bleu_2": 0.24371914504394585, "Bleu_3": 0.1999668457252143, "Bleu_4": 0.1751312976579291, "METEOR": 0.3504934805792271, "ROUGE_L": 0.3279569892473118, "CIDEr": 1.0967508704877502e-12, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.13793103448275862, "f": 0.14035087719298248, "fn": 25.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a baseball player swinging a bat at a ball on a baseball field. The player is wearing a white jersey with the number 12 on the back and blue pants. The ball is flying through the air and the player is running towards home plate. The background is a green field"}, "332532": {"image_id": 332532, "Bleu_1": 0.20454545454080583, "Bleu_2": 0.15422177271257892, "Bleu_3": 0.08273336660439992, "Bleu_4": 1.0840880536378982e-05, "METEOR": 0.19662950902186194, "ROUGE_L": 0.25702247191011235, "CIDEr": 7.100659723826325e-08, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.2222222222222222, "f": 0.20512820512820512, "fn": 14.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.6666666666666666, "f": 0.4705882352941177, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a bathroom with a toilet, sink, and trash can. The walls are painted orange and there is a wooden floor.\n\nThe toilet is a white porcelain bowl with a seat and lid. The sink is a white porcelain basin with fauc"}, "528261": {"image_id": 528261, "Bleu_1": 0.19230769230399414, "Bleu_2": 0.086841684284798, "Bleu_3": 5.323069753321686e-07, "Bleu_4": 1.324562934342616e-09, "METEOR": 0.2027263574712166, "ROUGE_L": 0.15024630541871922, "CIDEr": 4.230923873318674e-12, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.2222222222222222, "f": 0.2222222222222222, "fn": 21.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.46153846153846156, "f": 0.4444444444444445, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The giraffe is standing in a fenced in area, looking at something on the ground. The fence is made of wood and has a gate that is open. The giraffe is wearing a halter and has a tag on its neck. The grass is green and there are some trees in the"}, "297046": {"image_id": 297046, "Bleu_1": 0.19999999999636367, "Bleu_2": 0.12171612388780342, "Bleu_3": 0.06538429607726681, "Bleu_4": 8.562572337389154e-06, "METEOR": 0.1812291847811706, "ROUGE_L": 0.14672279013830425, "CIDEr": 4.364604641890017e-12, "SPICE": {"All": {"pr": 0.22580645161290322, "re": 0.25, "f": 0.23728813559322032, "fn": 21.0, "numImages": 1.0, "fp": 24.0, "tp": 7.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.09090909090909091, "f": 0.09999999999999999, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2222222222222222, "re": 0.25, "f": 0.23529411764705882, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.6666666666666666, "f": 0.4444444444444444, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4444444444444444, "f": 0.3636363636363637, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a train traveling down the tracks with cars parked on the side of the road. There are buildings in the background and a few cars parked on the side of the road. The sky is cloudy and there are trees in the background.\n\nThe train is red and has a large engine"}, "130839": {"image_id": 130839, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.17131872291347974, "Bleu_3": 0.1191907417701306, "Bleu_4": 0.07590831443387894, "METEOR": 0.2162966647048623, "ROUGE_L": 0.27949599083619703, "CIDEr": 1.007018191687951e-12, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.10714285714285714, "f": 0.12244897959183672, "fn": 25.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.2727272727272727, "f": 0.2727272727272727, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a man in a black suit and tie carrying two large suitcases on wheels. He is standing on the sidewalk in front of a building with a large window on the side. There are people walking in the background.\n\nThe man is wearing a black suit and tie, and his hair"}, "451120": {"image_id": 451120, "Bleu_1": 0.30357142856600766, "Bleu_2": 0.1965613482731822, "Bleu_3": 0.15294108368862316, "Bleu_4": 0.11929011812129767, "METEOR": 0.2304222830381831, "ROUGE_L": 0.2426136363636364, "CIDEr": 2.0016335115033168e-13, "SPICE": {"All": {"pr": 0.25806451612903225, "re": 0.25806451612903225, "f": 0.25806451612903225, "fn": 23.0, "numImages": 1.0, "fp": 23.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.18181818181818182, "re": 0.15384615384615385, "f": 0.16666666666666669, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.46153846153846156, "re": 0.6, "f": 0.5217391304347826, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a woman standing at a table with several jars of food on it. She is wearing a blue sweater and has a smile on her face. There are several other people in the background, including a man sitting at a desk and a woman standing near the table. The room appears to be"}, "378134": {"image_id": 378134, "Bleu_1": 0.3265306122382341, "Bleu_2": 0.2608202654732723, "Bleu_3": 0.20554741492254136, "Bleu_4": 0.1542676522468039, "METEOR": 0.2118143657053794, "ROUGE_L": 0.26940063091482647, "CIDEr": 6.47123989785964e-10, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.0625, "f": 0.0851063829787234, "fn": 30.0, "numImages": 1.0, "fp": 13.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.15384615384615385, "f": 0.21052631578947367, "fn": 11.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}}, "caption": "The image shows a group of people playing a game of frisbee on a grassy field. The players are wearing orange and black shirts and are running around the field with the frisbee. There are trees in the background and a blue sky above.\n\nThe image is taken at"}, "458953": {"image_id": 458953, "Bleu_1": 0.21999999999560005, "Bleu_2": 0.11605769149245462, "Bleu_3": 6.54689746724509e-07, "Bleu_4": 1.563155525234599e-09, "METEOR": 0.15018800896283105, "ROUGE_L": 0.22333414693678305, "CIDEr": 2.2146011833534126e-09, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.23529411764705882, "f": 0.17777777777777778, "fn": 13.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.1, "re": 0.2, "f": 0.13333333333333333, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.42857142857142855, "f": 0.3157894736842105, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of people standing in a field with kites flying in the sky\n\nThe people in the image are wearing hats and sunglasses, and some of them are holding kites. The kites are in various colors and shapes, and some of them have streamers attached to"}, "159451": {"image_id": 159451, "Bleu_1": 0.2916666666605903, "Bleu_2": 0.23632829671335462, "Bleu_3": 0.1824236779225367, "Bleu_4": 0.1418365277267857, "METEOR": 0.2542329627324837, "ROUGE_L": 0.2675438596491228, "CIDEr": 1.2297270020376442e-09, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.2222222222222222, "f": 0.2142857142857143, "fn": 21.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.625, "f": 0.5555555555555556, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "This is an image of a yellow tractor parked on a dirt road in front of a large building. The tractor has a large bucket on the front and is parked next to a large pile of dirt. There are no other vehicles or people in the image."}, "294258": {"image_id": 294258, "Bleu_1": 0.32692307691678996, "Bleu_2": 0.3100868364669995, "Bleu_3": 0.2586701938782621, "Bleu_4": 0.21455991281324005, "METEOR": 0.32021934390354384, "ROUGE_L": 0.37952488687782804, "CIDEr": 3.2119931157367434e-09, "SPICE": {"All": {"pr": 0.10714285714285714, "re": 0.1111111111111111, "f": 0.10909090909090909, "fn": 24.0, "numImages": 1.0, "fp": 25.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.23076923076923078, "f": 0.22222222222222224, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a man in a suit and tie standing outside a building. He is wearing a red tie and a black suit with a white shirt underneath. He is also wearing a black belt and black shoes. The background is a city street with cars and buildings in the distance."}, "544695": {"image_id": 544695, "Bleu_1": 0.2916666666605903, "Bleu_2": 0.13644420572458105, "Bleu_3": 7.396917719335283e-07, "Bleu_4": 1.731749067280523e-09, "METEOR": 0.23733554513903482, "ROUGE_L": 0.19551282051282048, "CIDEr": 2.5699322695427444e-08, "SPICE": {"All": {"pr": 0.12, "re": 0.13043478260869565, "f": 0.12499999999999997, "fn": 20.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a man and a woman playing tennis on a court. The man is holding a racket and the woman is holding a tennis ball. They are both wearing tennis shoes and white clothing. The background is a green grassy field with trees in the distance."}, "623": {"image_id": 623, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.15041420939612582, "Bleu_3": 0.09672659665842727, "Bleu_4": 0.06555569265748387, "METEOR": 0.22143323077213506, "ROUGE_L": 0.2167219327333018, "CIDEr": 5.456928337578056e-11, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.2727272727272727, "f": 0.24999999999999994, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2857142857142857, "f": 0.30769230769230765, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a woman sitting on the floor next to a large stuffed animal. The woman is wearing a black shirt and black pants, and has her arms around the stuffed animal. The stuffed animal is wearing a pink dress and has its arms around the woman. The woman is sm"}, "236690": {"image_id": 236690, "Bleu_1": 0.24561403508341031, "Bleu_2": 0.16222142112789117, "Bleu_3": 0.11280412717972553, "Bleu_4": 0.07180344843549429, "METEOR": 0.22789907895291275, "ROUGE_L": 0.23682750970604544, "CIDEr": 7.276423011193055e-14, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.3333333333333333, "f": 0.2580645161290323, "fn": 8.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.8, "f": 0.5714285714285714, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image is of a seagull flying over the ocean. The bird is white with black wings and a red beak. The sky is blue and there are waves in the background. The bird is flying high in the air and appears to be in a state of flight. The image is taken from a distance and"}, "382088": {"image_id": 382088, "Bleu_1": 0.26190476189852613, "Bleu_2": 0.19577417125493551, "Bleu_3": 0.15649612776722097, "Bleu_4": 0.13103637161795242, "METEOR": 0.2840023135360377, "ROUGE_L": 0.31077147016011636, "CIDEr": 5.067427566065807e-07, "SPICE": {"All": {"pr": 0.16, "re": 0.13333333333333333, "f": 0.14545454545454545, "fn": 26.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.3076923076923077, "f": 0.33333333333333337, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a white horse standing in a field with a fence in the background. The horse has a white mane and tail and is looking directly at the camera. The sky is blue and there are trees in the background."}, "504711": {"image_id": 504711, "Bleu_1": 0.2888888888824692, "Bleu_2": 0.2562353715895104, "Bleu_3": 0.22027704854856506, "Bleu_4": 0.1662244588219803, "METEOR": 0.32550769142040725, "ROUGE_L": 0.4309569685292229, "CIDEr": 4.0504852809458467e-07, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.11538461538461539, "f": 0.12765957446808512, "fn": 23.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.2857142857142857, "re": 0.2, "f": 0.23529411764705882, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The image shows a piece of chocolate cake with frosting on top. There is a fork on the plate with a piece of cake on it. The fork is held by a person's hand. The background is a white tablecloth with a floral pattern."}, "495348": {"image_id": 495348, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.15534712747310678, "Bleu_3": 0.07844142656278798, "Bleu_4": 9.962310935752864e-06, "METEOR": 0.16971905780853025, "ROUGE_L": 0.18373493975903615, "CIDEr": 6.821942974350032e-11, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.20833333333333334, "f": 0.21739130434782608, "fn": 19.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.14285714285714285, "f": 0.22222222222222224, "fn": 6.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5714285714285714, "f": 0.4444444444444444, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a group of zebras grazing in a grassy field. There are several trees in the background, including a large acacia tree. The sky is clear and blue, with a few clouds scattered across it. The grass is tall and green, and there are no other animals in the field."}, "326217": {"image_id": 326217, "Bleu_1": 0.3454545454482645, "Bleu_2": 0.21161555892585335, "Bleu_3": 0.11911042453441503, "Bleu_4": 1.3426459108767523e-05, "METEOR": 0.21180125507458472, "ROUGE_L": 0.28668407310704963, "CIDEr": 3.966111918423331e-11, "SPICE": {"All": {"pr": 0.16, "re": 0.125, "f": 0.14035087719298245, "fn": 28.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.09090909090909091, "f": 0.13333333333333333, "fn": 10.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.2727272727272727, "f": 0.24999999999999994, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a group of people in a boat on a river. They are wearing traditional Thai clothing and are carrying baskets of fruit and vegetables. The boat is being pulled by a man in a hat and a woman in a long dress. There are other boats in the background with people on"}, "59752": {"image_id": 59752, "Bleu_1": 0.2424242424168963, "Bleu_2": 0.08703882797517024, "Bleu_3": 6.252035830567377e-07, "Bleu_4": 1.6894127988830863e-09, "METEOR": 0.22560703716086164, "ROUGE_L": 0.21922731356693623, "CIDEr": 0.00014538989381907564, "SPICE": {"All": {"pr": 0.16, "re": 0.23529411764705882, "f": 0.19047619047619052, "fn": 13.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.6666666666666666, "f": 0.4444444444444444, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a river with boats on it. There are houses on the other side of the river. The sky is blue and there are trees on the banks of the river."}, "437393": {"image_id": 437393, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.17797249265986642, "Bleu_3": 0.13633224439757732, "Bleu_4": 0.0848007939389795, "METEOR": 0.20255189602916793, "ROUGE_L": 0.22536945812807885, "CIDEr": 7.1242119900585665e-12, "SPICE": {"All": {"pr": 0.32, "re": 0.36363636363636365, "f": 0.3404255319148936, "fn": 14.0, "numImages": 1.0, "fp": 17.0, "tp": 8.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.3333333333333333, "f": 0.30769230769230765, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.5555555555555556, "f": 0.5, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image is a white horse with blue and pink accents on its mane and tail. The horse has a pink bow on its head and a pink rose on its neck. The horse is standing on a white surface.\n\nThe image is a white horse with blue and pink accents on"}, "279209": {"image_id": 279209, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.1447907475849832, "Bleu_3": 0.07387419460030926, "Bleu_4": 9.42924728285967e-06, "METEOR": 0.16494866066070105, "ROUGE_L": 0.23487348734873487, "CIDEr": 1.4860931439404557e-11, "SPICE": {"All": {"pr": 0.23529411764705882, "re": 0.17391304347826086, "f": 0.19999999999999998, "fn": 19.0, "numImages": 1.0, "fp": 13.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.36363636363636365, "f": 0.4210526315789474, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 4.0}}, "caption": "The image shows a person skiing down a snowy mountain trail. The person is wearing a black and white ski suit and carrying a backpack. The trees on either side of the trail are covered in snow and the sky is clear and blue.\n\nThe image is taken in the winter season, and the"}, "202228": {"image_id": 202228, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.21797089763394864, "Bleu_3": 0.1417912480658796, "Bleu_4": 0.10385933746670008, "METEOR": 0.28068177782769754, "ROUGE_L": 0.2784479947831757, "CIDEr": 1.566376607875319e-10, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.12903225806451613, "f": 0.15999999999999998, "fn": 27.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.36363636363636365, "f": 0.380952380952381, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a man standing in front of a mirror, taking a selfie. He is wearing a red jacket and black pants, and has a black hat on his head. The mirror is white and has a frame around it. The room is empty except for the mirror and the man."}, "193661": {"image_id": 193661, "Bleu_1": 0.2075471698074048, "Bleu_2": 0.16714975946414357, "Bleu_3": 0.1180105578990872, "Bleu_4": 0.0900441795848082, "METEOR": 0.2203247643274705, "ROUGE_L": 0.27774615822424586, "CIDEr": 3.580399647030077e-10, "SPICE": {"All": {"pr": 0.24, "re": 0.24, "f": 0.24, "fn": 19.0, "numImages": 1.0, "fp": 19.0, "tp": 6.0}, "Relation": {"pr": 0.125, "re": 0.125, "f": 0.125, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 1.0, "f": 0.4, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4444444444444444, "f": 0.39999999999999997, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a living room with a fireplace, hardwood floors, and white walls. There is a large window on one side of the room and a door on the other. The room is well lit with natural light coming in through the window.\n\nThe room is decorated with a few pieces of"}, "457060": {"image_id": 457060, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.1475489144223407, "Bleu_3": 0.07529519927190922, "Bleu_4": 9.612424997506294e-06, "METEOR": 0.16348408710217754, "ROUGE_L": 0.2334609075997813, "CIDEr": 2.8603889311350446e-10, "SPICE": {"All": {"pr": 0.23529411764705882, "re": 0.21052631578947367, "f": 0.2222222222222222, "fn": 15.0, "numImages": 1.0, "fp": 13.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.5714285714285714, "f": 0.47058823529411764, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows two giraffes standing in a fenced in area. One of the giraffes is looking directly at the camera while the other is looking off to the side. The fence is made of metal and has a gate in the middle. There are trees in the background and a building in"}, "390215": {"image_id": 390215, "Bleu_1": 0.2666666666607408, "Bleu_2": 0.1556997888288053, "Bleu_3": 0.10408285482696128, "Bleu_4": 1.2800356763225978e-05, "METEOR": 0.29746239221788323, "ROUGE_L": 0.336783988957902, "CIDEr": 8.003435818140174e-08, "SPICE": {"All": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 14.0, "numImages": 1.0, "fp": 16.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.6, "re": 0.75, "f": 0.6666666666666665, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 6.0}}, "caption": "The image shows a plate of food with a roasted chicken breast, broccoli, and a side of mashed potatoes. The chicken breast is seasoned with herbs and spices and is served with a side of mashed potatoes. The broccoli is steamed and served on the"}, "579635": {"image_id": 579635, "Bleu_1": 0.29787234041919425, "Bleu_2": 0.26689013146350443, "Bleu_3": 0.2117716998370989, "Bleu_4": 0.12120981065997205, "METEOR": 0.24237603937870883, "ROUGE_L": 0.31302116741500957, "CIDEr": 2.532353589909972e-08, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.14285714285714285, "f": 0.17543859649122806, "fn": 30.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.3125, "f": 0.3703703703703703, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a surfer riding a wave on a surfboard in the ocean. The surfer is wearing a wetsuit and a helmet, and the wave is breaking on the shore. The sky is clear and the sun is setting in the background.\n\nThe surfer is riding"}, "251920": {"image_id": 251920, "Bleu_1": 0.30232558138831805, "Bleu_2": 0.20782052056698208, "Bleu_3": 0.14674773886538112, "Bleu_4": 0.11211689577003678, "METEOR": 0.23857863071174137, "ROUGE_L": 0.28561872909699, "CIDEr": 2.1986222490900885e-05, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.14814814814814814, "f": 0.14035087719298248, "fn": 23.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.36363636363636365, "f": 0.32, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows three pizzas on a black and white checkered plate on top of a stove. The pizzas are topped with various ingredients such as cheese, pepperoni, and vegetables. There is a bottle of wine on the counter next to the pizzas."}, "271117": {"image_id": 271117, "Bleu_1": 0.19230769230399414, "Bleu_2": 0.13730875909260604, "Bleu_3": 7.224517156389943e-07, "Bleu_4": 1.6655497133613283e-09, "METEOR": 0.15441821421870208, "ROUGE_L": 0.21131639722863746, "CIDEr": 2.5754879373706665e-10, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.2727272727272727, "f": 0.24489795918367346, "fn": 16.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4444444444444444, "f": 0.380952380952381, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image is of a small room with a desk, chair, and bookshelf. There are several books and papers on the desk, and a lamp on the bookshelf. The walls are painted a light yellow color and there is a window on one side of the room. The floor is covered in"}, "11051": {"image_id": 11051, "Bleu_1": 0.20370370369993146, "Bleu_2": 0.13862658669016847, "Bleu_3": 0.07176233678698053, "Bleu_4": 9.226349141829447e-06, "METEOR": 0.16491677603730104, "ROUGE_L": 0.2684268426842684, "CIDEr": 1.0141806258576686e-11, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.25, "f": 0.20408163265306123, "fn": 15.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.2857142857142857, "re": 0.4, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a young woman in a black dress and a man in a tuxedo standing next to each other. The woman is holding a bouquet of flowers and the man is holding a boutonniere. They are both smiling and looking at each other. The background is a white wall with a large"}, "170605": {"image_id": 170605, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.1313579154832357, "Bleu_3": 0.07061638347858736, "Bleu_4": 9.254837436221288e-06, "METEOR": 0.09840190502824467, "ROUGE_L": 0.15762273901808785, "CIDEr": 1.0592037437253933e-05, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.5, "f": 0.28571428571428575, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of people on snowmobiles in the middle of a snowy field. The sun is shining down on them, and there are no trees or buildings in the background. The snowmobiles are all parked on the ground, and there are no other vehicles or people in the"}, "84123": {"image_id": 84123, "Bleu_1": 0.3207547169750801, "Bleu_2": 0.22214157542810112, "Bleu_3": 0.1426495013657264, "Bleu_4": 0.08728913550550585, "METEOR": 0.18779151103961592, "ROUGE_L": 0.2381483547127719, "CIDEr": 8.851927220868617e-10, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.15384615384615385, "f": 0.2, "fn": 33.0, "numImages": 1.0, "fp": 15.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.1, "f": 0.15384615384615383, "fn": 9.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5555555555555556, "re": 0.3125, "f": 0.39999999999999997, "fn": 11.0, "numImages": 1.0, "fp": 4.0, "tp": 5.0}}, "caption": "The image shows a street with cars parked on both sides. There are signs on the street indicating the direction of traffic. The sky is cloudy and there is no sunlight. The image is taken from a low angle, looking down the street.\n\nThe image is taken from a low angle, looking down"}, "505899": {"image_id": 505899, "Bleu_1": 0.3399999999932, "Bleu_2": 0.24989793834546398, "Bleu_3": 1.0916783637955145e-06, "Bleu_4": 2.293752541381502e-09, "METEOR": 0.2645692456841877, "ROUGE_L": 0.26521739130434785, "CIDEr": 4.972556634310571e-10, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.05263157894736842, "f": 0.06666666666666667, "fn": 36.0, "numImages": 1.0, "fp": 20.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 15.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 12.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a table with two cups of coffee and a plate of donuts on it. The donuts are covered in glaze and sprinkles. There is a fork and knife on the table. The background is a wooden floor with a green carpet.\n\nThe image is taken in a"}, "256814": {"image_id": 256814, "Bleu_1": 0.29268292682212976, "Bleu_2": 0.22631728213415836, "Bleu_3": 0.18726083047926026, "Bleu_4": 0.16214527868787693, "METEOR": 0.31496608233754836, "ROUGE_L": 0.3617494440326167, "CIDEr": 1.7722742153815988e-06, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.2727272727272727, "f": 0.24999999999999994, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.46153846153846156, "re": 0.5, "f": 0.48000000000000004, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a man holding a donut in his hand while smiling at the camera. The background is a car with a driver and two passengers in the backseat. The sun is shining and there are trees in the distance."}, "419680": {"image_id": 419680, "Bleu_1": 0.22807017543459532, "Bleu_2": 0.15632047282649467, "Bleu_3": 0.07630558803280324, "Bleu_4": 9.52398752047048e-06, "METEOR": 0.1782376975205152, "ROUGE_L": 0.20299500831946757, "CIDEr": 8.136824078606085e-14, "SPICE": {"All": {"pr": 0.28125, "re": 0.3103448275862069, "f": 0.2950819672131148, "fn": 20.0, "numImages": 1.0, "fp": 23.0, "tp": 9.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.09090909090909091, "f": 0.09090909090909091, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2222222222222222, "re": 0.3333333333333333, "f": 0.26666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 6.0}}, "caption": "The image shows a street with a building on the left and a sign on the right. The building has a large window on the top floor and a small window on the bottom floor. The sign has the words \"no parking\" written on it in white letters. The street is lined with trees and there are"}, "519555": {"image_id": 519555, "Bleu_1": 0.2499999999958334, "Bleu_2": 0.17222374095389834, "Bleu_3": 0.1153346404471652, "Bleu_4": 0.07202795932126134, "METEOR": 0.25056325179979216, "ROUGE_L": 0.22956989247311832, "CIDEr": 2.369833345378304e-15, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.16666666666666666, "f": 0.13333333333333333, "fn": 15.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.1111111111111111, "f": 0.15384615384615383, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.14285714285714285, "re": 0.5, "f": 0.22222222222222224, "fn": 2.0, "numImages": 1.0, "fp": 12.0, "tp": 2.0}}, "caption": "The image shows a stop sign in the middle of a field with no cars or people around it. The sign is made of metal and has a red and white background with the word stop written in black letters. The sign is surrounded by tall grass and trees. The sky is cloudy and there is no sun in the"}, "354929": {"image_id": 354929, "Bleu_1": 0.3399999999932, "Bleu_2": 0.24989793834546398, "Bleu_3": 0.15744726510979745, "Bleu_4": 0.09546119715797491, "METEOR": 0.27276371158666907, "ROUGE_L": 0.326397146254459, "CIDEr": 4.783558993688113e-08, "SPICE": {"All": {"pr": 0.16129032258064516, "re": 0.25, "f": 0.19607843137254902, "fn": 15.0, "numImages": 1.0, "fp": 26.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.6666666666666666, "f": 0.8, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.23076923076923078, "re": 0.375, "f": 0.2857142857142857, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a group of people riding bicycles down a street at night. They are wearing helmets and some of them are holding lights on their bikes. The street is lit up by streetlights and there are buildings on either side of the road.\n\nThe image is taken at"}, "17379": {"image_id": 17379, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.15357377920556234, "Bleu_3": 0.07733118331435387, "Bleu_4": 9.806713568284028e-06, "METEOR": 0.23036081281132204, "ROUGE_L": 0.2567069963177275, "CIDEr": 6.782687570464009e-10, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.3333333333333333, "f": 0.23255813953488372, "fn": 10.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.25, "f": 0.125, "fn": 3.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image is of a bathroom with a large mirror on the wall. The mirror is framed in wood and has a television mounted on it. There are two sinks in the bathroom, one on the left and one on the right. The sink on the left has a faucet and a drain"}, "13965": {"image_id": 13965, "Bleu_1": 0.27906976743537054, "Bleu_2": 0.18227065413983304, "Bleu_3": 0.13445913121019032, "Bleu_4": 0.10499901541586201, "METEOR": 0.21853294805668005, "ROUGE_L": 0.31626701231367466, "CIDEr": 1.2391310646242844e-06, "SPICE": {"All": {"pr": 0.12, "re": 0.15, "f": 0.1333333333333333, "fn": 17.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.09090909090909091, "f": 0.11764705882352942, "fn": 10.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.4, "f": 0.25000000000000006, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a green and white trolley car parked on the side of a street. There are several cars parked on the street and a few people standing around. The trolley car has a sign on the side that reads, \"Green Line\"."}, "422836": {"image_id": 422836, "Bleu_1": 0.19999999999636367, "Bleu_2": 0.17213259316161544, "Bleu_3": 0.13076859215453357, "Bleu_4": 0.09630176550491282, "METEOR": 0.3089019277700708, "ROUGE_L": 0.2872277810476751, "CIDEr": 9.279204674952846e-13, "SPICE": {"All": {"pr": 0.125, "re": 0.09090909090909091, "f": 0.10526315789473685, "fn": 30.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.21428571428571427, "f": 0.23999999999999996, "fn": 11.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a man and a woman walking down a street with a building on the left and a cafe on the right. The man is carrying a suitcase and the woman is carrying a purse. There are trees and cars parked on the street.\n\nThe image is in black and white and has"}, "513292": {"image_id": 513292, "Bleu_1": 0.2448979591786756, "Bleu_2": 0.12371791482379726, "Bleu_3": 0.06880010617668034, "Bleu_4": 9.172809483333237e-06, "METEOR": 0.2593937332555629, "ROUGE_L": 0.2663755458515284, "CIDEr": 2.493866630231061e-09, "SPICE": {"All": {"pr": 0.14705882352941177, "re": 0.2777777777777778, "f": 0.1923076923076923, "fn": 13.0, "numImages": 1.0, "fp": 29.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.25, "f": 0.18181818181818182, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5714285714285714, "f": 0.38095238095238093, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a young boy holding a skateboard while walking on the sidewalk. The boy is wearing a red shirt and blue shorts, and has a black and white skateboard in his hand. The background is a residential area with houses and trees.\n\nThe image is in focus"}, "202444": {"image_id": 202444, "Bleu_1": 0.1960784313687044, "Bleu_2": 0.1252448582145496, "Bleu_3": 0.08618888098293648, "Bleu_4": 0.060433552207849656, "METEOR": 0.20996233124974725, "ROUGE_L": 0.19074421513445905, "CIDEr": 3.801064121360998e-10, "SPICE": {"All": {"pr": 0.15789473684210525, "re": 0.125, "f": 0.13953488372093023, "fn": 21.0, "numImages": 1.0, "fp": 16.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a group of people skateboarding on a ramp in a park. The ramp is made of metal and has a railing on the side. There are several people in the image, including a man in a black shirt and jeans, a woman in a white shirt and black"}, "268541": {"image_id": 268541, "Bleu_1": 0.20338983050502735, "Bleu_2": 0.18726241522375653, "Bleu_3": 0.16269522557186727, "Bleu_4": 0.14003177718934404, "METEOR": 0.28335273407082673, "ROUGE_L": 0.29918256130790194, "CIDEr": 1.585843105905521e-14, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.3125, "f": 0.2325581395348837, "fn": 11.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.7142857142857143, "f": 0.5263157894736842, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The man in the image is wearing a suit and tie, and is holding a cup of coffee in his hand. He is standing in front of a white wall with a black and white photo on it. The photo appears to be of a man in a suit and tie, holding a cup of coffee in his hand"}, "377999": {"image_id": 377999, "Bleu_1": 0.3454545454482645, "Bleu_2": 0.2399494896298897, "Bleu_3": 0.14826148601876277, "Bleu_4": 0.08897549762764936, "METEOR": 0.19027459438345862, "ROUGE_L": 0.27555053642010163, "CIDEr": 1.0779903737158573e-11, "SPICE": {"All": {"pr": 0.07142857142857142, "re": 0.07407407407407407, "f": 0.07272727272727272, "fn": 25.0, "numImages": 1.0, "fp": 26.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.18181818181818182, "f": 0.17391304347826086, "fn": 9.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a man standing on the edge of a dock, looking out at a small sailboat in the water. The sky is clear and blue, with a few clouds scattered across it. The water is calm and reflects the sky. The dock is made of wood and has a few boats moored to"}, "272694": {"image_id": 272694, "Bleu_1": 0.32432432431555885, "Bleu_2": 0.25112360116008, "Bleu_3": 0.1754995836599104, "Bleu_4": 0.11228903217058282, "METEOR": 0.21827455363727852, "ROUGE_L": 0.380351946792296, "CIDEr": 0.00040724365850418125, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.1875, "f": 0.20689655172413793, "fn": 26.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.08333333333333333, "f": 0.11764705882352941, "fn": 11.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.38461538461538464, "re": 0.38461538461538464, "f": 0.38461538461538464, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a banana and a cup of coffee on a table. The banana is peeled and the coffee is in a cup with a lid on it. There is also a napkin on the table."}, "137844": {"image_id": 137844, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.20195126571496114, "Bleu_3": 0.11851564692759443, "Bleu_4": 1.3646505647648106e-05, "METEOR": 0.24299268935777085, "ROUGE_L": 0.26116207951070336, "CIDEr": 3.567620718101397e-11, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.1111111111111111, "f": 0.1111111111111111, "fn": 16.0, "numImages": 1.0, "fp": 16.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.4, "f": 0.26666666666666666, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a person performing a trick on a skateboard in a skate park. The person is wearing a black shirt and black pants, and has a helmet on their head. The skate park is surrounded by a fence and there are rocks and trees in the background. The sky"}, "374829": {"image_id": 374829, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.1475489144223407, "Bleu_3": 0.09486600651870754, "Bleu_4": 1.1431164199464497e-05, "METEOR": 0.23284812176421044, "ROUGE_L": 0.26472821959422804, "CIDEr": 4.282175865157174e-09, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.11538461538461539, "f": 0.11320754716981132, "fn": 23.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.1, "re": 0.2, "f": 0.13333333333333333, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows a baseball player in the batter's box, ready to swing at the ball. The crowd is watching from the stands, and there are several umpires standing behind home plate. The field is well manicured and there are no visible obstacles in the way of the game. The sky is"}, "21465": {"image_id": 21465, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.13231937691727144, "Bleu_3": 6.912848008424654e-07, "Bleu_4": 1.5876030342373218e-09, "METEOR": 0.15281778378570646, "ROUGE_L": 0.207506520013607, "CIDEr": 5.489705043086913e-13, "SPICE": {"All": {"pr": 0.16, "re": 0.14285714285714285, "f": 0.15094339622641512, "fn": 24.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.3333333333333333, "f": 0.32, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "This is a picture of a room with a table and chairs in it. There are several items on the table, including a vase, a lamp, and a book. The walls are painted blue and there are several windows in the room. The floor is made of wood and there are several rugs on it"}, "281929": {"image_id": 281929, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.20195126571496114, "Bleu_3": 0.14932035830598486, "Bleu_4": 0.12010449786712427, "METEOR": 0.2661277802085604, "ROUGE_L": 0.3216168717047452, "CIDEr": 2.260081156906969e-10, "SPICE": {"All": {"pr": 0.28, "re": 0.25925925925925924, "f": 0.2692307692307692, "fn": 20.0, "numImages": 1.0, "fp": 18.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2857142857142857, "f": 0.30769230769230765, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.45454545454545453, "f": 0.47619047619047616, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "The man in the image is wearing a suit and tie, and is standing next to a bicycle. The bicycle has a black frame and white tires. The man is holding the handlebars of the bicycle with both hands. The house in the background has a white roof and brown walls"}, "464814": {"image_id": 464814, "Bleu_1": 0.18867924527945892, "Bleu_2": 0.13469311460752553, "Bleu_3": 0.11247608780336717, "Bleu_4": 0.09612424997506293, "METEOR": 0.2317140348090662, "ROUGE_L": 0.28355607205113303, "CIDEr": 8.522979867891698e-12, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.25, "f": 0.26666666666666666, "fn": 24.0, "numImages": 1.0, "fp": 20.0, "tp": 8.0}, "Relation": {"pr": 0.07692307692307693, "re": 0.07142857142857142, "f": 0.07407407407407408, "fn": 13.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.4666666666666667, "f": 0.4827586206896552, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 7.0}}, "caption": "The image shows a living room with a couch, coffee table, and television. The walls are painted white and there are windows on either side of the room. The floor is made of hardwood and there are rugs on the furniture. The room is well lit and there are no clutter or mess."}, "213538": {"image_id": 213538, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.1565560727682472, "Bleu_3": 0.0788478660803684, "Bleu_4": 1.0001000249876893e-05, "METEOR": 0.19945933256316806, "ROUGE_L": 0.21131639722863746, "CIDEr": 6.0467709069704195e-09, "SPICE": {"All": {"pr": 0.16, "re": 0.12121212121212122, "f": 0.1379310344827586, "fn": 29.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4, "f": 0.380952380952381, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a desk with a computer, keyboard, and mouse on it. There is also a red curtain hanging behind the desk. The room appears to be a home office or study area, with a bookshelf and a chair in the background. The walls are painted red and there are windows"}, "461573": {"image_id": 461573, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.09701425001261196, "Bleu_3": 5.76976768606882e-07, "Bleu_4": 1.4143550190666944e-09, "METEOR": 0.15188630544087336, "ROUGE_L": 0.22889305816135083, "CIDEr": 2.7806136307456252e-11, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.1724137931034483, "f": 0.19230769230769232, "fn": 24.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.2857142857142857, "re": 0.18181818181818182, "f": 0.2222222222222222, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a pair of old metal benches sitting on the sidewalk in front of a row of trees. The benches are rusted and have a worn, weathered appearance. The trees are tall and green, with leaves that are a deep shade of green. The sidewalk is cracked and une"}, "360629": {"image_id": 360629, "Bleu_1": 0.2444444444390124, "Bleu_2": 0.1666666666629209, "Bleu_3": 0.12467616434942133, "Bleu_4": 0.0980127897842689, "METEOR": 0.2284849407840435, "ROUGE_L": 0.29468599033816417, "CIDEr": 2.6395512775082636e-07, "SPICE": {"All": {"pr": 0.25, "re": 0.35294117647058826, "f": 0.2926829268292683, "fn": 11.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.18181818181818182, "re": 0.2857142857142857, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.5714285714285714, "f": 0.47058823529411764, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image is of a lunch box with various types of food inside, including sushi, rice, and vegetables. The box is red and has a white lid.\n\nThe image is of a lunch box with various types of food inside, including sushi, rice, and vegetables."}, "114745": {"image_id": 114745, "Bleu_1": 0.2553191489307379, "Bleu_2": 0.16658955971162087, "Bleu_3": 0.08511922922355492, "Bleu_4": 1.088071895098314e-05, "METEOR": 0.31165310791016587, "ROUGE_L": 0.317915309446254, "CIDEr": 2.313962084909079e-09, "SPICE": {"All": {"pr": 0.12, "re": 0.2, "f": 0.15, "fn": 12.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.25, "f": 0.16666666666666666, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 0.5, "f": 0.28571428571428575, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Object": {"pr": 0.2, "re": 0.2857142857142857, "f": 0.23529411764705882, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a man riding a skateboard down a ramp made of concrete. The ramp is surrounded by a fence and there are people watching from the side. The man is wearing a red shirt and has his arms outstretched as he rides the skateboard. The"}, "548878": {"image_id": 548878, "Bleu_1": 0.19148936169805347, "Bleu_2": 0.09124485969890858, "Bleu_3": 5.698161672784481e-07, "Bleu_4": 1.4319831453628615e-09, "METEOR": 0.15576662114683, "ROUGE_L": 0.19303797468354428, "CIDEr": 4.892358667612027e-10, "SPICE": {"All": {"pr": 0.08, "re": 0.06666666666666667, "f": 0.07272727272727272, "fn": 28.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.16666666666666666, "f": 0.16, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a person wearing a snowboarding helmet and goggles jumping off a snowboard ramp. The person is wearing a black and white snowboarding suit and has a red and black scarf around their neck. The background is a city skyline with tall buildings and a"}, "385985": {"image_id": 385985, "Bleu_1": 0.35185185184533607, "Bleu_2": 0.19958027235629977, "Bleu_3": 9.149779456371165e-07, "Bleu_4": 1.9686360129176833e-09, "METEOR": 0.19589595356290385, "ROUGE_L": 0.2582010582010582, "CIDEr": 8.325545365265978e-09, "SPICE": {"All": {"pr": 0.26666666666666666, "re": 0.21052631578947367, "f": 0.23529411764705882, "fn": 15.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5714285714285714, "re": 0.5714285714285714, "f": 0.5714285714285714, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 4.0}}, "caption": "The image shows two people sitting on the ground, one holding a cell phone and the other looking at it. The person on the left is wearing black pants and a white shirt, while the person on the right is wearing black pants and a black shirt. The person on the left is looking"}, "289714": {"image_id": 289714, "Bleu_1": 0.1960784313687044, "Bleu_2": 0.1400280084000279, "Bleu_3": 0.09284415754233383, "Bleu_4": 0.06390070145221528, "METEOR": 0.14518834711924788, "ROUGE_L": 0.15259537210756724, "CIDEr": 5.073640065386614e-11, "SPICE": {"All": {"pr": 0.16, "re": 0.125, "f": 0.14035087719298245, "fn": 28.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.36363636363636365, "f": 0.36363636363636365, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image is of a kitchen with green walls and a wooden floor. There is a stove in the corner of the room and a refrigerator on the wall. There is also a sink and a dishwasher in the room. The walls are covered in floral wallpaper and there are several"}, "230226": {"image_id": 230226, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.06726727939832501, "Bleu_3": 4.4896509096312353e-07, "Bleu_4": 1.1657633846364334e-09, "METEOR": 0.15872291899286828, "ROUGE_L": 0.18373493975903615, "CIDEr": 3.283373316808413e-12, "SPICE": {"All": {"pr": 0.13793103448275862, "re": 0.26666666666666666, "f": 0.18181818181818182, "fn": 11.0, "numImages": 1.0, "fp": 25.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.6666666666666666, "f": 0.42105263157894735, "fn": 2.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a clear plastic container with a handle on top and a small metal basket inside. The container is sitting on top of a white toilet with a toilet paper holder next to it. There is a sink with a faucet and a towel rack next to the sink. The"}, "319534": {"image_id": 319534, "Bleu_1": 0.1964285714250638, "Bleu_2": 0.13363062095380426, "Bleu_3": 0.06915221111259608, "Bleu_4": 8.887622216194529e-06, "METEOR": 0.2076693514642501, "ROUGE_L": 0.2401574803149606, "CIDEr": 4.810099151414783e-12, "SPICE": {"All": {"pr": 0.06451612903225806, "re": 0.06896551724137931, "f": 0.06666666666666667, "fn": 27.0, "numImages": 1.0, "fp": 29.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 10.0, "numImages": 1.0, "fp": 12.0, "tp": 2.0}}, "caption": "The image shows a woman standing in front of a bus with other people on board. The bus is white and has a green stripe on the side. The woman is wearing a white shirt and has her hands on her hips. There are other people on the bus, some of whom are standing and others"}, "427476": {"image_id": 427476, "Bleu_1": 0.3090909090852893, "Bleu_2": 0.27278338367291277, "Bleu_3": 0.21419993346050092, "Bleu_4": 0.16581684998320237, "METEOR": 0.2524007859970546, "ROUGE_L": 0.2978838849701574, "CIDEr": 7.360152606882686e-12, "SPICE": {"All": {"pr": 0.047619047619047616, "re": 0.043478260869565216, "f": 0.04545454545454545, "fn": 22.0, "numImages": 1.0, "fp": 20.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.09090909090909091, "re": 0.14285714285714285, "f": 0.1111111111111111, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}}, "caption": "The image shows a young girl standing in a bathroom with a toilet and sink. The walls are painted white and there is a window in the background. The floor is made of wood and there is a rug on the floor. The girl is wearing a white shirt and jeans and has her hair"}, "101223": {"image_id": 101223, "Bleu_1": 0.3636363636297521, "Bleu_2": 0.2958751215274355, "Bleu_3": 0.21479826040495464, "Bleu_4": 0.13972680638010707, "METEOR": 0.3053035562638337, "ROUGE_L": 0.3640803038524145, "CIDEr": 1.0078390263696568e-11, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.2, "f": 0.16, "fn": 16.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.26666666666666666, "re": 0.4444444444444444, "f": 0.33333333333333337, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "The image shows a view of the mountains from an airplane window. The mountains are covered in snow and there are clouds in the sky. The plane is flying over the mountains and there is a runway in the foreground.\n\nThe image is taken from an airplane window and shows a view of the mountains"}, "123570": {"image_id": 123570, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.18504536999744126, "Bleu_3": 0.12547526428859127, "Bleu_4": 0.07889074281873733, "METEOR": 0.24613184043879158, "ROUGE_L": 0.23961840628507297, "CIDEr": 7.87225082484791e-11, "SPICE": {"All": {"pr": 0.06666666666666667, "re": 0.07407407407407407, "f": 0.07017543859649124, "fn": 25.0, "numImages": 1.0, "fp": 28.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2, "f": 0.17391304347826086, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a woman standing in front of a store at night. She is holding an umbrella over her head and looking down at the ground. The store has a large window with a sign that reads \"open\" in white letters. The woman is wearing a black coat and black boots, and her"}, "368581": {"image_id": 368581, "Bleu_1": 0.20833333332899312, "Bleu_2": 0.1331559103200233, "Bleu_3": 0.09169191011404364, "Bleu_4": 1.1440503453589431e-05, "METEOR": 0.1767990461686543, "ROUGE_L": 0.264069264069264, "CIDEr": 4.984671115116441e-07, "SPICE": {"All": {"pr": 0.1, "re": 0.125, "f": 0.11111111111111112, "fn": 21.0, "numImages": 1.0, "fp": 27.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.125, "f": 0.15384615384615385, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2222222222222222, "f": 0.18181818181818185, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a lunchbox with a sandwich, fruit, and vegetables inside. The sandwich appears to be made of bread and meat, and the fruit and vegetables are arranged on the side of the box. The background is green and there are no other objects in the image."}, "446984": {"image_id": 446984, "Bleu_1": 0.3333333333263889, "Bleu_2": 0.1883108942847125, "Bleu_3": 0.13224261796264494, "Bleu_4": 0.08466919880736143, "METEOR": 0.31218908893383274, "ROUGE_L": 0.35192307692307695, "CIDEr": 9.275566627230804e-10, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.3076923076923077, "f": 0.2222222222222222, "fn": 9.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.75, "f": 0.375, "fn": 1.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of people on bicycles, wearing yellow vests and helmets, standing in front of a large, yellow bike. They are all smiling and looking at something in the distance. There is a large, orange bike in the background with a person sitting on it."}, "514668": {"image_id": 514668, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.16817499303341832, "Bleu_3": 0.1169829962474529, "Bleu_4": 0.0744888414414979, "METEOR": 0.2547344880752691, "ROUGE_L": 0.24110671936758893, "CIDEr": 6.761939065987619e-13, "SPICE": {"All": {"pr": 0.26666666666666666, "re": 0.25, "f": 0.2580645161290323, "fn": 24.0, "numImages": 1.0, "fp": 22.0, "tp": 8.0}, "Relation": {"pr": 0.07692307692307693, "re": 0.08333333333333333, "f": 0.08, "fn": 11.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5384615384615384, "re": 0.5384615384615384, "f": 0.5384615384615384, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 7.0}}, "caption": "The image shows a cat sitting in the back seat of a car looking out the window at the mountains in the distance. The cat is wearing a collar and appears to be enjoying the view. The car is parked on the side of the road and there are trees and rocks in the background."}, "532129": {"image_id": 532129, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.1814970425910117, "Bleu_3": 0.12634404006922517, "Bleu_4": 0.08051110007483202, "METEOR": 0.21912005147865185, "ROUGE_L": 0.32455029136052693, "CIDEr": 2.5346702491017303e-10, "SPICE": {"All": {"pr": 0.13636363636363635, "re": 0.1, "f": 0.11538461538461538, "fn": 27.0, "numImages": 1.0, "fp": 19.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.2727272727272727, "f": 0.24999999999999994, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a pizza on a plate with cheese and pepperoni on top. There is a glass of water on the table next to the pizza.\n\nThe image is taken in a restaurant with a white tablecloth and a white napkin on the table. There is a white plate with"}, "200168": {"image_id": 200168, "Bleu_1": 0.2352941176401385, "Bleu_2": 0.08444006618162872, "Bleu_3": 6.062462295166369e-07, "Bleu_4": 1.6373682487936863e-09, "METEOR": 0.1969230849435258, "ROUGE_L": 0.2680140597539543, "CIDEr": 0.00018798613190532598, "SPICE": {"All": {"pr": 0.25, "re": 0.32, "f": 0.2807017543859649, "fn": 17.0, "numImages": 1.0, "fp": 24.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.4, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.46153846153846156, "re": 0.6, "f": 0.5217391304347826, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a person skiing down a snowy slope. The person is wearing a helmet, goggles, and skis. The trees in the background are covered in snow. The sky is clear and blue."}, "470801": {"image_id": 470801, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.16803361008003348, "Bleu_3": 0.08321444966006321, "Bleu_4": 1.046739829058619e-05, "METEOR": 0.1917870663075836, "ROUGE_L": 0.29204069419509276, "CIDEr": 7.356864494865622e-11, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.23809523809523808, "f": 0.20833333333333334, "fn": 16.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.3333333333333333, "f": 0.2, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a woman holding a kite in her hand. The kite has a red and white striped tail and a blue and white striped body. The woman is standing in front of a blue sky with fluffy white clouds. She is wearing a black shirt and jeans. The image"}, "138713": {"image_id": 138713, "Bleu_1": 0.2765957446749661, "Bleu_2": 0.17339191115885713, "Bleu_3": 0.08742085527780723, "Bleu_4": 1.1100642574269578e-05, "METEOR": 0.20231008851097004, "ROUGE_L": 0.23843648208469054, "CIDEr": 1.6364700554471561e-07, "SPICE": {"All": {"pr": 0.30434782608695654, "re": 0.23333333333333334, "f": 0.26415094339622636, "fn": 23.0, "numImages": 1.0, "fp": 16.0, "tp": 7.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.06666666666666667, "f": 0.07692307692307691, "fn": 14.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5454545454545454, "f": 0.5217391304347826, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 6.0}}, "caption": "The image shows a group of people playing frisbee in a park. They are all wearing purple shirts and white shorts. One person is throwing the frisbee while the others are running to catch it. The sky is blue and there are trees in the background.\n\nThe"}, "195917": {"image_id": 195917, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.20550493309104514, "Bleu_3": 0.13633224439757732, "Bleu_4": 0.0848007939389795, "METEOR": 0.2931565235089285, "ROUGE_L": 0.37952488687782804, "CIDEr": 2.813718723852076e-10, "SPICE": {"All": {"pr": 0.08, "re": 0.10526315789473684, "f": 0.0909090909090909, "fn": 17.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.2857142857142857, "f": 0.23529411764705882, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a man with long hair and a beard, wearing a black shirt and black pants, standing in front of a window with a view of the city outside. He is holding a toothbrush in his hand and appears to be brushing his teeth. The image is in black and"}, "145391": {"image_id": 145391, "Bleu_1": 0.3265306122382341, "Bleu_2": 0.23328473740311126, "Bleu_3": 0.15144863020774949, "Bleu_4": 0.09322009782129659, "METEOR": 0.2367255918889058, "ROUGE_L": 0.3546511627906977, "CIDEr": 9.782860110021352e-05, "SPICE": {"All": {"pr": 0.15789473684210525, "re": 0.0967741935483871, "f": 0.12000000000000001, "fn": 28.0, "numImages": 1.0, "fp": 16.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.18181818181818182, "f": 0.2222222222222222, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The image is a picture of a roll of tape with a scissors and a pair of scissors on top of it. The tape is wrapped around a roll of tape and has a label on it that says \"Christmas\". The scissors are open and have a sharp blade."}, "459303": {"image_id": 459303, "Bleu_1": 0.1964285714250638, "Bleu_2": 0.11952286093128565, "Bleu_3": 6.419522618397191e-07, "Bleu_4": 1.4947139323454504e-09, "METEOR": 0.14814505574908274, "ROUGE_L": 0.20795454545454545, "CIDEr": 9.27604988894399e-13, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.16, "f": 0.16666666666666666, "fn": 21.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.15384615384615385, "f": 0.21052631578947367, "fn": 11.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.4, "f": 0.5714285714285715, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.2222222222222222, "re": 0.3333333333333333, "f": 0.26666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a train traveling down the tracks on a platform in front of a large building. The train is black and yellow and has a number on the side. There are people standing on the platform and watching the train go by. The sky is cloudy and there are trees in the background.\n\nThe"}, "497334": {"image_id": 497334, "Bleu_1": 0.26315789473222534, "Bleu_2": 0.19389168357893838, "Bleu_3": 8.808836678271836e-07, "Bleu_4": 1.886211891489747e-09, "METEOR": 0.22157826825594848, "ROUGE_L": 0.2697622996130459, "CIDEr": 1.81327695170753e-13, "SPICE": {"All": {"pr": 0.2, "re": 0.10810810810810811, "f": 0.14035087719298245, "fn": 33.0, "numImages": 1.0, "fp": 16.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.2857142857142857, "f": 0.32, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a bed with a book on it. The book is open and has a red cover with white and black patterns on it. The bed is made of a white sheet and has a pillow on it. There is a lamp on the nightstand next to the bed. The room is dimly lit and"}, "173138": {"image_id": 173138, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.17797249265986642, "Bleu_3": 0.08588393224796005, "Bleu_4": 1.0663135316252687e-05, "METEOR": 0.1731359748610297, "ROUGE_L": 0.26293103448275856, "CIDEr": 1.7874338595401266e-11, "SPICE": {"All": {"pr": 0.3333333333333333, "re": 0.28125, "f": 0.3050847457627119, "fn": 23.0, "numImages": 1.0, "fp": 18.0, "tp": 9.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.25, "f": 0.26666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5384615384615384, "f": 0.5185185185185186, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 7.0}}, "caption": "The image shows a person standing on the beach, looking out at the ocean. The person is wearing a black wetsuit and has a surfboard under their arm. The waves are crashing against the shore and the sky is cloudy. The beach is sandy and there are some rocks in the water"}, "404984": {"image_id": 404984, "Bleu_1": 0.28070175438104034, "Bleu_2": 0.1873171623130232, "Bleu_3": 0.1366524683031566, "Bleu_4": 0.09859892260444003, "METEOR": 0.2928270654865765, "ROUGE_L": 0.26798462383305877, "CIDEr": 8.959240740908099e-14, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.22727272727272727, "f": 0.22222222222222224, "fn": 17.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.5, "f": 0.36363636363636365, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a living room with a couch, coffee table, and television. The walls are painted a light gray color and there are two windows on the left side of the room. The floor is made of hardwood and there is a rug in the middle of the room. The room is well lit and there"}, "427965": {"image_id": 427965, "Bleu_1": 0.22448979591378598, "Bleu_2": 0.19342948582067712, "Bleu_3": 0.15847928398352254, "Bleu_4": 0.12693173928949333, "METEOR": 0.27451825763382076, "ROUGE_L": 0.3386798272671191, "CIDEr": 9.24703165159227e-10, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.3333333333333333, "f": 0.2926829268292683, "fn": 12.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.2857142857142857, "f": 0.36363636363636365, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5714285714285714, "f": 0.4444444444444444, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a living room with a couch, coffee table, and chairs. There is a television on the wall and a window with curtains. The room is decorated with various items such as a lamp, vase, and bookshelf. The floor is made of wood and there is a"}, "445834": {"image_id": 445834, "Bleu_1": 0.2181818181778513, "Bleu_2": 0.14213381090113214, "Bleu_3": 0.09135163170376533, "Bleu_4": 0.06187801819921283, "METEOR": 0.18504382189543925, "ROUGE_L": 0.24358243011979464, "CIDEr": 1.249831307789995e-12, "SPICE": {"All": {"pr": 0.13793103448275862, "re": 0.16, "f": 0.14814814814814817, "fn": 21.0, "numImages": 1.0, "fp": 25.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4444444444444444, "f": 0.3636363636363637, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a group of people standing in front of a bus. They are all wearing blue uniforms and have their hands on their hips. The bus is parked in front of a building with a large window on the side. There is a sign on the side of the building that reads \"Welcome"}, "386958": {"image_id": 386958, "Bleu_1": 0.14545454545190087, "Bleu_2": 0.07339758434041055, "Bleu_3": 4.6669078026091577e-07, "Bleu_4": 1.1824181296865702e-09, "METEOR": 0.18322512673480326, "ROUGE_L": 0.17951736315479697, "CIDEr": 1.3354911618768288e-11, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.15384615384615385, "f": 0.1509433962264151, "fn": 22.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.08333333333333333, "f": 0.125, "fn": 11.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.25, "f": 0.28571428571428575, "fn": 3.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a fire hydrant on the side of a road. The hydrant is made of metal and has a green handle on top. The hydrant is surrounded by concrete and has a yellow line on the side. The image is taken from a low angle, looking down at the hydrant from the sidewalk"}, "306135": {"image_id": 306135, "Bleu_1": 0.3207547169750801, "Bleu_2": 0.1923802475574326, "Bleu_3": 0.11322098426534662, "Bleu_4": 1.3052770239201211e-05, "METEOR": 0.2194346430775356, "ROUGE_L": 0.27036011080332406, "CIDEr": 1.118343588375721e-10, "SPICE": {"All": {"pr": 0.12, "re": 0.09375, "f": 0.10526315789473684, "fn": 29.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 17.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.3333333333333333, "f": 0.18181818181818182, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.16666666666666666, "f": 0.1904761904761905, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a man walking down a set of stairs in front of a building with columns and arches. The man is wearing a black shirt and pants, and has a backpack on his back. There are people walking on the sidewalk in front of the building, and a statue of a"}, "335839": {"image_id": 335839, "Bleu_1": 0.36538461537758876, "Bleu_2": 0.293211273104214, "Bleu_3": 0.19017369208087398, "Bleu_4": 1.9355919558434562e-05, "METEOR": 0.32562059108396246, "ROUGE_L": 0.28175519630484985, "CIDEr": 2.0657706148963455e-09, "SPICE": {"All": {"pr": 0.08, "re": 0.07692307692307693, "f": 0.0784313725490196, "fn": 24.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.09090909090909091, "f": 0.13333333333333333, "fn": 10.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.09090909090909091, "re": 0.1111111111111111, "f": 0.09999999999999999, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}}, "caption": "The image shows a man standing on the sidewalk in front of a brick building with graffiti on the walls. The man is wearing a black jacket and black pants, and has a red hat on his head. There is a fire hydrant on the sidewalk next to him. The building has"}, "190313": {"image_id": 190313, "Bleu_1": 0.36734693876801333, "Bleu_2": 0.27664166758053954, "Bleu_3": 0.2137779263252579, "Bleu_4": 0.17072522153615455, "METEOR": 0.3440246406284333, "ROUGE_L": 0.35442220787604906, "CIDEr": 1.3619412123749141e-08, "SPICE": {"All": {"pr": 0.15, "re": 0.15, "f": 0.15, "fn": 17.0, "numImages": 1.0, "fp": 17.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.42857142857142855, "f": 0.375, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a woman sitting on a bench outside a store. She is wearing a black jacket and black pants, and has a white shirt on underneath. She is holding a bag of groceries in her lap. The store has a large window with a sign that reads, `"}, "85328": {"image_id": 85328, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.11155016321612385, "Bleu_3": 6.290089214849726e-07, "Bleu_4": 1.5012173807660588e-09, "METEOR": 0.16635912423619667, "ROUGE_L": 0.17983490566037735, "CIDEr": 1.448436120415568e-11, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.22727272727272727, "f": 0.20408163265306123, "fn": 17.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4, "f": 0.3636363636363636, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a train on the tracks, with people standing on the platform. The train is green and yellow, with a white roof and black wheels. There are buildings in the background, with people walking on the sidewalk.\n\nThe train is moving slowly, with its doors open. There are no other"}, "104002": {"image_id": 104002, "Bleu_1": 0.24390243901844147, "Bleu_2": 0.17460757393808263, "Bleu_3": 0.09211998312650962, "Bleu_4": 1.1976212357708837e-05, "METEOR": 0.23895184868588235, "ROUGE_L": 0.26703633445206476, "CIDEr": 5.384512067502949e-07, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.11538461538461539, "f": 0.11320754716981132, "fn": 23.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of cows standing in a field with a fence in the background. The cows are grazing on the grass in the field. There is a small pond in the background. The sky is clear and blue."}, "37389": {"image_id": 37389, "Bleu_1": 0.19298245613696527, "Bleu_2": 0.11740724488886634, "Bleu_3": 0.07943630475846972, "Bleu_4": 0.05519704586938769, "METEOR": 0.21752341532093597, "ROUGE_L": 0.2136602451838879, "CIDEr": 1.2946929440637685e-14, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.12, "f": 0.13043478260869565, "fn": 22.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.3333333333333333, "f": 0.35294117647058826, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image is of a clock tower in the middle of a city. The clock tower is made of stone and has a large clock face on the front. The clock face has numbers and hands, and the clock is ticking away. There are buildings around the clock tower, and there are people walking on the sidewalk"}, "383594": {"image_id": 383594, "Bleu_1": 0.31914893616342244, "Bleu_2": 0.2355932146939368, "Bleu_3": 0.13511835401219335, "Bleu_4": 1.538766030885031e-05, "METEOR": 0.2274860488657606, "ROUGE_L": 0.31302116741500957, "CIDEr": 1.0555536121800284e-07, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.18181818181818182, "f": 0.16666666666666669, "fn": 18.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.1, "re": 0.14285714285714285, "f": 0.11764705882352941, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a plate of fried chicken and fries on a table with a red and white checkered tablecloth. There are two glasses of soda on the table and a jar of pickles on the side.\n\nThe fried chicken is served on a bun with lettuce"}, "319696": {"image_id": 319696, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.20866567588950136, "Bleu_3": 0.13682045713838573, "Bleu_4": 1.5044258140708106e-05, "METEOR": 0.2736667773420156, "ROUGE_L": 0.26472411655300687, "CIDEr": 1.6439326225241456e-12, "SPICE": {"All": {"pr": 0.28, "re": 0.28, "f": 0.28, "fn": 18.0, "numImages": 1.0, "fp": 18.0, "tp": 7.0}, "Relation": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.2857142857142857, "f": 0.36363636363636365, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4444444444444444, "f": 0.3636363636363637, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a red microwave oven sitting on a countertop in a kitchen. The countertop is made of tile and there is a sink to the left of the microwave. There are two wine glasses on the counter next to the sink. The walls are made of brick and there is a"}, "318911": {"image_id": 318911, "Bleu_1": 0.27999999999440006, "Bleu_2": 0.19999999999595922, "Bleu_3": 0.11856311014724848, "Bleu_4": 0.0771680849931398, "METEOR": 0.21936241394138345, "ROUGE_L": 0.27128335451080055, "CIDEr": 3.0779249086356235e-10, "SPICE": {"All": {"pr": 0.12, "re": 0.16666666666666666, "f": 0.13953488372093023, "fn": 15.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.3333333333333333, "f": 0.2222222222222222, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a flock of sheep grazing in a green field. The sheep are all different colors, including white, black, and brown. They are standing in a line, with their heads down and their tails hanging down. The sky is blue and there are trees in the background.\n\nThe"}, "455506": {"image_id": 455506, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.2303502213753046, "Bleu_3": 0.1977932000419102, "Bleu_4": 0.17728494858288124, "METEOR": 0.34599930722043837, "ROUGE_L": 0.34099378881987574, "CIDEr": 5.782476803251301e-10, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.25, "f": 0.24000000000000002, "fn": 18.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4444444444444444, "f": 0.3636363636363637, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The dog is running with a frisbee in its mouth. The dog is wearing a collar and tag. The dog is standing in the grass with its front paws on the ground and its back paws in the air. The dog is looking up at the person holding the frisbee."}, "444631": {"image_id": 444631, "Bleu_1": 0.19999999999600004, "Bleu_2": 0.12777531299740644, "Bleu_3": 0.06980462899277687, "Bleu_4": 9.223350291134877e-06, "METEOR": 0.25551117995516914, "ROUGE_L": 0.23252858958068615, "CIDEr": 3.699848283184483e-11, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.20689655172413793, "f": 0.21818181818181817, "fn": 23.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.1, "re": 0.1111111111111111, "f": 0.10526315789473685, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.1111111111111111, "f": 0.13333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Object": {"pr": 0.4, "re": 0.36363636363636365, "f": 0.380952380952381, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a person standing on the beach holding a surfboard. The person is wearing a black and red wetsuit and has a surfboard under their arm. The sky is cloudy and there are waves in the distance. The beach is sandy and there are rocks and driftwood on"}, "497014": {"image_id": 497014, "Bleu_1": 0.19230769230399414, "Bleu_2": 0.12281268769488378, "Bleu_3": 0.06706647632268699, "Bleu_4": 8.857886206727456e-06, "METEOR": 0.16686780321667646, "ROUGE_L": 0.18373493975903615, "CIDEr": 3.9419752160369477e-10, "SPICE": {"All": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The dog is standing on the grass in front of a fence, with its mouth open and its tongue hanging out. The dog is wearing a collar and tag on its neck. The fence is made of wood and has a gate in the middle. There are trees in the background.\n\nThe"}, "502749": {"image_id": 502749, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.1842190315423256, "Bleu_3": 0.1107243128286132, "Bleu_4": 1.2901292604257922e-05, "METEOR": 0.22071508745901175, "ROUGE_L": 0.22536945812807885, "CIDEr": 7.014100597415127e-12, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.18181818181818182, "f": 0.17391304347826086, "fn": 18.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.1, "re": 0.2, "f": 0.13333333333333333, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "This is a kitchen with white cabinets and countertops. There is a large island in the center of the room with a sink and stove on it. The walls are painted white and there are red wreaths hanging from the ceiling. The floor is made of dark wood and there are two"}, "230593": {"image_id": 230593, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.1814970425910117, "Bleu_3": 0.1103716852403353, "Bleu_4": 1.293698116812338e-05, "METEOR": 0.21629379905669666, "ROUGE_L": 0.2238532110091743, "CIDEr": 8.463929619537661e-11, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.23076923076923078, "f": 0.23076923076923078, "fn": 20.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.1, "re": 0.1111111111111111, "f": 0.10526315789473685, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.36363636363636365, "re": 0.36363636363636365, "f": 0.36363636363636365, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a group of geese walking along a path in a park. The geese are white and have black heads and necks. They are walking in a line, with their beaks facing forward. The path is lined with trees and there are benches and a fountain in the distance."}, "364636": {"image_id": 364636, "Bleu_1": 0.2264150943353507, "Bleu_2": 0.16163173752903065, "Bleu_3": 0.100810174751238, "Bleu_4": 1.1964258135320571e-05, "METEOR": 0.22882379019033555, "ROUGE_L": 0.2896142433234421, "CIDEr": 3.390878949278012e-11, "SPICE": {"All": {"pr": 0.30434782608695654, "re": 0.28, "f": 0.2916666666666667, "fn": 18.0, "numImages": 1.0, "fp": 16.0, "tp": 7.0}, "Relation": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4, "f": 0.4210526315789474, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The dog is a brown and white spotted dog with a collar and tag on its neck. It is standing on the ground and looking up at the camera. The dog's ears are perked up and its tail is wagging. The background is a dirt path with some trees and a fence"}, "288313": {"image_id": 288313, "Bleu_1": 0.2941176470530566, "Bleu_2": 0.15339299776643633, "Bleu_3": 7.830779525362617e-07, "Bleu_4": 1.778457282414307e-09, "METEOR": 0.15004848196093426, "ROUGE_L": 0.24883449883449882, "CIDEr": 2.078964562013436e-09, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.23809523809523808, "f": 0.22727272727272724, "fn": 16.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.45454545454545453, "f": 0.43478260869565216, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a table set with a variety of foods, including cheese, ham, and bread. There are also two glasses of orange juice on the table. The table is set with a white tablecloth and a white napkin. The background is a light blue wall with a window in the"}, "384503": {"image_id": 384503, "Bleu_1": 0.20370370369993146, "Bleu_2": 0.06199569426559689, "Bleu_3": 4.1966869140147383e-07, "Bleu_4": 1.0972040044962828e-09, "METEOR": 0.11428571428571428, "ROUGE_L": 0.14896214896214896, "CIDEr": 3.3360786629969535e-13, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.21052631578947367, "f": 0.1951219512195122, "fn": 15.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4444444444444444, "f": 0.380952380952381, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a train traveling down a track in the rain\n\nThe train is green and has a yellow stripe on the side. There are trees on either side of the track and a building in the distance. There is a yellow bow on the front of the train.\n\nThe sky is cloudy"}, "190156": {"image_id": 190156, "Bleu_1": 0.28571428570918367, "Bleu_2": 0.2038588765713768, "Bleu_3": 0.132168790590052, "Bleu_4": 0.09661292227451955, "METEOR": 0.2632800683770508, "ROUGE_L": 0.2330786026200873, "CIDEr": 9.055583017839455e-12, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.14285714285714285, "f": 0.12765957446808512, "fn": 18.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.09090909090909091, "re": 0.16666666666666666, "f": 0.11764705882352942, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}}, "caption": "The image shows a black and white cat sitting on a table with a cup of coffee in its paws. The cat is looking up at the camera with its eyes closed. There is a laptop on the table next to the cat. The background is a messy office with papers and books scattered around.\n\nThe"}, "174123": {"image_id": 174123, "Bleu_1": 0.43749999999088546, "Bleu_2": 0.31999002643358254, "Bleu_3": 0.20726454375199196, "Bleu_4": 0.14104189406737797, "METEOR": 0.2200064441198377, "ROUGE_L": 0.33639705882352944, "CIDEr": 5.949442731792945e-07, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.20689655172413793, "f": 0.23076923076923075, "fn": 23.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.1, "f": 0.14285714285714288, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.45454545454545453, "f": 0.41666666666666663, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a person holding a slice of pizza in their hand. The pizza has cheese, tomato sauce, and various toppings on it. The person is sitting at a table with a white tablecloth and a red and white checkered napkin. There are other plates of food"}, "557239": {"image_id": 557239, "Bleu_1": 0.2448979591786756, "Bleu_2": 0.1889822365007167, "Bleu_3": 0.13160986848342301, "Bleu_4": 0.09977790531466467, "METEOR": 0.3188020295042116, "ROUGE_L": 0.2663755458515284, "CIDEr": 1.524068707234404e-09, "SPICE": {"All": {"pr": 0.125, "re": 0.2222222222222222, "f": 0.16, "fn": 14.0, "numImages": 1.0, "fp": 28.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.09090909090909091, "re": 0.16666666666666666, "f": 0.11764705882352942, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.14285714285714285, "re": 0.5, "f": 0.22222222222222224, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image is a cat sitting in a toilet bowl. The cat is white with orange eyes and has a pink nose. The toilet bowl is white and has a seat in the middle. There is a toilet paper roll on the wall next to the toilet. The cat"}, "184474": {"image_id": 184474, "Bleu_1": 0.1296296296272291, "Bleu_2": 0.08565936145673987, "Bleu_3": 0.05206135179390746, "Bleu_4": 7.252605189423233e-06, "METEOR": 0.20031746549725443, "ROUGE_L": 0.14896214896214896, "CIDEr": 1.849156275126918e-13, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.20689655172413793, "f": 0.21052631578947367, "fn": 23.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.18181818181818182, "re": 0.2222222222222222, "f": 0.19999999999999998, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.3333333333333333, "f": 0.3636363636363636, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a bench sitting on top of a hill with green grass and trees in the background. The sky is blue and there are clouds in the distance.\n\nThe bench is made of wood and has a backrest. It is in a grassy area with no other objects nearby. The hill is"}, "335099": {"image_id": 335099, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.0951302988290515, "Bleu_3": 5.656605687724057e-07, "Bleu_4": 1.3863341114193003e-09, "METEOR": 0.12663446966940323, "ROUGE_L": 0.14386792452830188, "CIDEr": 7.606683621950698e-09, "SPICE": {"All": {"pr": 0.041666666666666664, "re": 0.05263157894736842, "f": 0.04651162790697675, "fn": 18.0, "numImages": 1.0, "fp": 23.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.07692307692307693, "re": 0.14285714285714285, "f": 0.1, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}}, "caption": "The image shows a dog sitting on a fence looking out at the street. The dog is wearing a collar and tag. The fence is made of metal and has a gate that is open. There are buildings in the background. The sky is cloudy and there are trees in the foreground."}, "431306": {"image_id": 431306, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.18857036045053577, "Bleu_3": 0.1278888396607359, "Bleu_4": 0.08042331753470428, "METEOR": 0.24878670649554283, "ROUGE_L": 0.26472411655300687, "CIDEr": 3.404183441177242e-12, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.29411764705882354, "f": 0.24390243902439027, "fn": 12.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.5714285714285714, "f": 0.47058823529411764, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image is of a bathroom with two sinks and a mirror on the wall. The mirror is made of glass and has a white frame. The sinks are made of white porcelain and have faucets on them. The floor is made of white tiles and there are no other objects in the"}, "125815": {"image_id": 125815, "Bleu_1": 0.23214285713871177, "Bleu_2": 0.1591372845179993, "Bleu_3": 0.11205307876458186, "Bleu_4": 1.2764355107341873e-05, "METEOR": 0.20461681614849253, "ROUGE_L": 0.2459677419354839, "CIDEr": 1.6529013270375918e-11, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.20833333333333334, "f": 0.1923076923076923, "fn": 19.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.1, "f": 0.0909090909090909, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4, "f": 0.3636363636363636, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a train on the tracks at a train station\n\nThe train is a red and white train with a number on the side. It is pulling into the station and there are people standing on the platform. There are also cars parked on the side of the road.\n\nThe sky is blue and"}, "521106": {"image_id": 521106, "Bleu_1": 0.18181818181487608, "Bleu_2": 0.14213381090113214, "Bleu_3": 0.09135163170376533, "Bleu_4": 1.1003640569767388e-05, "METEOR": 0.1906644022946788, "ROUGE_L": 0.21095100864553315, "CIDEr": 1.1600567376621567e-10, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.21052631578947367, "f": 0.186046511627907, "fn": 15.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.375, "f": 0.2857142857142857, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a tennis player hitting a ball with a racket on a tennis court. The player is wearing a yellow shirt and black shorts, and has a yellow racket in his hand. The ball is flying through the air and the player is running towards it. The background is a blue sky with"}, "508672": {"image_id": 508672, "Bleu_1": 0.1960784313687044, "Bleu_2": 0.15339299776643633, "Bleu_3": 0.13390444632098428, "Bleu_4": 0.11893260654292824, "METEOR": 0.30123525480188396, "ROUGE_L": 0.27319257837492, "CIDEr": 7.553276373324907e-11, "SPICE": {"All": {"pr": 0.2727272727272727, "re": 0.3157894736842105, "f": 0.2926829268292683, "fn": 13.0, "numImages": 1.0, "fp": 16.0, "tp": 6.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.09090909090909091, "f": 0.1111111111111111, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.8, "f": 0.5000000000000001, "fn": 1.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a bicycle parked next to a flooded road. The bicycle is a red and black one with a basket on the front. The road is covered in water and there are trees on either side of it. The sky is cloudy and there are no buildings in sight."}, "221737": {"image_id": 221737, "Bleu_1": 0.1964285714250638, "Bleu_2": 0.13363062095380426, "Bleu_3": 0.09973474676294886, "Bleu_4": 0.07822100624658515, "METEOR": 0.21840604318810966, "ROUGE_L": 0.23461538461538461, "CIDEr": 5.482918234564447e-12, "SPICE": {"All": {"pr": 0.25, "re": 0.2608695652173913, "f": 0.2553191489361702, "fn": 17.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.09090909090909091, "f": 0.1111111111111111, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.625, "f": 0.5, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a road with a sign on it that says \"stop\". There is a bridge over the road with a sign on it that says \"do not cross\". The road is lined with trees and there are no cars or people in sight.\n\nThe image is taken from the perspective of someone standing on"}, "345580": {"image_id": 345580, "Bleu_1": 0.19999999999600004, "Bleu_2": 0.16903085094228823, "Bleu_3": 0.10598398329266914, "Bleu_4": 0.07094237342460057, "METEOR": 0.21909070163070557, "ROUGE_L": 0.23252858958068615, "CIDEr": 2.8128666133940153e-10, "SPICE": {"All": {"pr": 0.08, "re": 0.11764705882352941, "f": 0.09523809523809526, "fn": 15.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.3333333333333333, "f": 0.2222222222222222, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a zebra standing in a field with a fence in the background. The zebra is black and white with a long mane and tail. It is standing on its hind legs and looking to the left. The image is taken from a bird's eye view.\n\nThe"}, "46440": {"image_id": 46440, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.21797089763394864, "Bleu_3": 0.1786457781290674, "Bleu_4": 0.1553059336992734, "METEOR": 0.33012449541526884, "ROUGE_L": 0.30049261083743845, "CIDEr": 1.4382250797729756e-10, "SPICE": {"All": {"pr": 0.09375, "re": 0.2, "f": 0.12765957446808512, "fn": 12.0, "numImages": 1.0, "fp": 29.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.5, "f": 0.3157894736842105, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a group of young men playing basketball in a gym. They are wearing orange and blue jerseys and black shorts. One player is dribbling the ball while the others are trying to block him. The ball is flying through the air and the players are jumping up to try"}, "270066": {"image_id": 270066, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.18890811128268284, "Bleu_3": 0.14281978481313226, "Bleu_4": 0.10496354025791335, "METEOR": 0.25974270871924343, "ROUGE_L": 0.26704190118824267, "CIDEr": 3.8049867171780515e-10, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.35294117647058826, "f": 0.3, "fn": 11.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.2857142857142857, "re": 0.3333333333333333, "f": 0.30769230769230765, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.8, "f": 0.4210526315789473, "fn": 1.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a white motorcycle parked in front of a building with a green roof. The motorcycle has a black seat and a black handlebar. The building has a green roof and a white wall. There is a sign on the wall that reads, \"Welcome to the Motorcycle Shop\". The"}, "419867": {"image_id": 419867, "Bleu_1": 0.27906976743537054, "Bleu_2": 0.16302782918400804, "Bleu_3": 0.1090410128226247, "Bleu_4": 1.3417679370522617e-05, "METEOR": 0.2215309370483842, "ROUGE_L": 0.2616154395997141, "CIDEr": 1.2575047358208283e-07, "SPICE": {"All": {"pr": 0.2413793103448276, "re": 0.30434782608695654, "f": 0.2692307692307692, "fn": 16.0, "numImages": 1.0, "fp": 22.0, "tp": 7.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.125, "f": 0.11764705882352941, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.16666666666666666, "f": 0.14285714285714288, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.5555555555555556, "f": 0.4761904761904762, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a grocery store with aisles of shelves stocked with various products, including bananas, bread, and other food items. There are also people shopping in the store.\n\nThe image is taken from a store with aisles of shelves stocked with various"}, "194724": {"image_id": 194724, "Bleu_1": 0.2826086956460303, "Bleu_2": 0.13726099121497223, "Bleu_3": 0.07537266335317148, "Bleu_4": 9.989489215549929e-06, "METEOR": 0.11340318862660885, "ROUGE_L": 0.22620519159456118, "CIDEr": 1.5907435321144628e-06, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.3, "f": 0.22641509433962265, "fn": 14.0, "numImages": 1.0, "fp": 27.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.625, "f": 0.43478260869565216, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 5.0}}, "caption": "The image shows a table with two pizzas on it. One of the pizzas has pepperoni and mushrooms on it, while the other has olives and onions. There are also two glasses of soda on the table.\n\nThe background of the image is a red and"}, "236426": {"image_id": 236426, "Bleu_1": 0.3090909090852893, "Bleu_2": 0.22696949467552002, "Bleu_3": 0.1428653103202315, "Bleu_4": 1.5388427959471035e-05, "METEOR": 0.2599562786516057, "ROUGE_L": 0.24013046901360924, "CIDEr": 5.583565388147309e-12, "SPICE": {"All": {"pr": 0.10344827586206896, "re": 0.125, "f": 0.11320754716981132, "fn": 21.0, "numImages": 1.0, "fp": 26.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2, "f": 0.1904761904761905, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a tennis player in action on a court. The player is wearing a white shirt and shorts, and is holding a tennis racket in his right hand. The player is jumping up to hit the ball with his left hand. The ball is flying through the air and is about to hit"}, "499826": {"image_id": 499826, "Bleu_1": 0.387755102032903, "Bleu_2": 0.28422229490636114, "Bleu_3": 0.19014845702222558, "Bleu_4": 0.11056817626488456, "METEOR": 0.2909662915872034, "ROUGE_L": 0.31399944847872047, "CIDEr": 4.959693378565932e-07, "SPICE": {"All": {"pr": 0.12, "re": 0.16666666666666666, "f": 0.13953488372093023, "fn": 15.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.42857142857142855, "f": 0.3157894736842105, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a woman standing on the sidewalk next to a group of elephants. The woman is wearing a white shirt and jeans, and has a camera around her neck. The elephants are standing in a line, with their trunks curled up and their ears flapping in the"}, "514904": {"image_id": 514904, "Bleu_1": 0.3061224489733445, "Bleu_2": 0.21128856367777224, "Bleu_3": 0.15604075333783501, "Bleu_4": 0.11336958836408244, "METEOR": 0.257723655018851, "ROUGE_L": 0.2663755458515284, "CIDEr": 9.572445967570875e-10, "SPICE": {"All": {"pr": 0.09375, "re": 0.15789473684210525, "f": 0.11764705882352941, "fn": 16.0, "numImages": 1.0, "fp": 29.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a woman holding a hot dog in her hand. She is wearing a black and white striped shirt and a pair of sunglasses. She is standing on the sidewalk in front of a building with a red awning. There are several bicycles parked on the sidewalk"}, "359864": {"image_id": 359864, "Bleu_1": 0.2075471698074048, "Bleu_2": 0.1412673303879691, "Bleu_3": 0.09215405117124459, "Bleu_4": 1.1185188895571611e-05, "METEOR": 0.2505672388949899, "ROUGE_L": 0.28355607205113303, "CIDEr": 1.4246659089239811e-11, "SPICE": {"All": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 8.0, "numImages": 1.0, "fp": 16.0, "tp": 6.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.25, "f": 0.18181818181818182, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.3333333333333333, "f": 0.30769230769230765, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.375, "re": 0.75, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a man wearing a red shirt and sunglasses standing on the deck of a boat. The man is looking out at the water and appears to be steering the boat. The boat is white and has a blue stripe running along the side. There are other boats in the background,"}, "247333": {"image_id": 247333, "Bleu_1": 0.3265306122382341, "Bleu_2": 0.27355060221045585, "Bleu_3": 0.2428883403110997, "Bleu_4": 0.20792339374451632, "METEOR": 0.3525006142351732, "ROUGE_L": 0.45864661654135336, "CIDEr": 1.887560693434884e-08, "SPICE": {"All": {"pr": 0.3, "re": 0.2608695652173913, "f": 0.27906976744186046, "fn": 17.0, "numImages": 1.0, "fp": 14.0, "tp": 6.0}, "Relation": {"pr": 0.25, "re": 0.2222222222222222, "f": 0.23529411764705882, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.4444444444444444, "f": 0.4210526315789474, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a plate of food on a table with a glass of soda and a bottle of ketchup on the side\n\nThere are several items on the plate, including a slice of ham, a slice of cheese, and a slice of tomato. The cheese is melted and"}, "54277": {"image_id": 54277, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.1596564940032875, "Bleu_3": 0.0804252399656011, "Bleu_4": 1.0203142794069551e-05, "METEOR": 0.19734266957014562, "ROUGE_L": 0.21997836278398844, "CIDEr": 2.0197552192623777e-10, "SPICE": {"All": {"pr": 0.125, "re": 0.17647058823529413, "f": 0.14634146341463414, "fn": 14.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a person on a snowboard in a large indoor skiing area. The person is wearing a green jacket and black pants, and has a helmet on their head. There are several other people in the background, also on snowboards. The walls of the building are made of concrete"}, "80172": {"image_id": 80172, "Bleu_1": 0.3636363636280992, "Bleu_2": 0.27588029391667884, "Bleu_3": 0.1935310651946666, "Bleu_4": 0.15175652200765855, "METEOR": 0.3408031437793003, "ROUGE_L": 0.35835509138381205, "CIDEr": 5.291404915885328e-07, "SPICE": {"All": {"pr": 0.12, "re": 0.10344827586206896, "f": 0.11111111111111112, "fn": 26.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.25, "f": 0.23076923076923075, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a young boy sitting on the toilet, brushing his teeth with a toothbrush. The boy is wearing a blue shirt and blue pants, and has a toothbrush in his hand. The toilet is white and has a toilet seat cover on"}, "376959": {"image_id": 376959, "Bleu_1": 0.31481481480898493, "Bleu_2": 0.24371914504394585, "Bleu_3": 0.1999668457252143, "Bleu_4": 0.14726728039928383, "METEOR": 0.26375129146712845, "ROUGE_L": 0.30198019801980197, "CIDEr": 1.40933749583137e-10, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.1724137931034483, "f": 0.17857142857142858, "fn": 24.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2222222222222222, "re": 0.2, "f": 0.2105263157894737, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a young girl sitting at a table with a pencil and paper in front of her. She is wearing a pink dress and has a pink bow in her hair. The background is a wooden floor with a white tablecloth on it. There is a window in the background with cur"}, "47055": {"image_id": 47055, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.1593710424504603, "Bleu_3": 0.07926525909642891, "Bleu_4": 9.990095999341766e-06, "METEOR": 0.1999919447066522, "ROUGE_L": 0.24707788450410828, "CIDEr": 8.158945361927774e-10, "SPICE": {"All": {"pr": 0.06666666666666667, "re": 0.06666666666666667, "f": 0.06666666666666667, "fn": 28.0, "numImages": 1.0, "fp": 28.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a bedroom with a bed, nightstand, and lamp. The walls are painted white and the floor is made of wood. There is a window on one side of the room with blinds. The bed has a headboard and a nightstand with a lamp on it. The room is well lit"}, "154816": {"image_id": 154816, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.1596564940032875, "Bleu_3": 0.0804252399656011, "Bleu_4": 1.0203142794069551e-05, "METEOR": 0.21009826260169073, "ROUGE_L": 0.2799770510613884, "CIDEr": 4.3617173112966077e-10, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.23809523809523808, "f": 0.23255813953488372, "fn": 16.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.625, "f": 0.5555555555555556, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "The image shows a baseball player in the process of hitting a ball with a bat. The player is wearing a red and white uniform with a black helmet and gloves. The umpire is standing behind the player, watching him hit the ball. The crowd is cheering in the background.\n\nThe"}, "155179": {"image_id": 155179, "Bleu_1": 0.21999999999560005, "Bleu_2": 0.06700593942469525, "Bleu_3": 4.5393651709319995e-07, "Bleu_4": 1.187741350296707e-09, "METEOR": 0.10264462650798749, "ROUGE_L": 0.21759809750297268, "CIDEr": 1.9151725262609915e-11, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.10256410256410256, "f": 0.12698412698412698, "fn": 35.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.08333333333333333, "f": 0.10526315789473685, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.1875, "f": 0.21428571428571427, "fn": 13.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a round loaf of bread with a hole in the middle. The bread appears to be made of white flour and has a golden brown crust. There are no visible toppings or fillings on the bread. The image is taken from a white background with good lighting."}, "328374": {"image_id": 328374, "Bleu_1": 0.29787234041919425, "Bleu_2": 0.16094080478419465, "Bleu_3": 0.08318402702711739, "Bleu_4": 1.0694655186909581e-05, "METEOR": 0.1700768730117881, "ROUGE_L": 0.29832497860374124, "CIDEr": 6.615459033012092e-08, "SPICE": {"All": {"pr": 0.2692307692307692, "re": 0.3181818181818182, "f": 0.2916666666666667, "fn": 15.0, "numImages": 1.0, "fp": 19.0, "tp": 7.0}, "Relation": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of people standing on the side of a snowy mountain. They are all wearing ski gear and some of them are holding skis. The people are standing in a line, looking up at something. There is a large snowman in the background."}, "264919": {"image_id": 264919, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.13026252642719696, "Bleu_3": 6.97519462339574e-07, "Bleu_4": 1.6222516606715129e-09, "METEOR": 0.20104089386554141, "ROUGE_L": 0.21580188679245285, "CIDEr": 1.2083823471029537e-08, "SPICE": {"All": {"pr": 0.3181818181818182, "re": 0.3181818181818182, "f": 0.3181818181818182, "fn": 15.0, "numImages": 1.0, "fp": 15.0, "tp": 7.0}, "Relation": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.14285714285714285, "f": 0.22222222222222224, "fn": 6.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.625, "f": 0.5, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a man wearing a balloon costume, standing on the sidewalk. He is holding a phone to his ear and looking down at the ground. The balloons are in the shape of a heart and are tied to a pole. There are people in the background, some of whom are"}, "48185": {"image_id": 48185, "Bleu_1": 0.2608695652117203, "Bleu_2": 0.18650096164396338, "Bleu_3": 0.14677648688658226, "Bleu_4": 0.1218724568456559, "METEOR": 0.3047075278319705, "ROUGE_L": 0.3855878634639696, "CIDEr": 4.339687696292062e-08, "SPICE": {"All": {"pr": 0.2, "re": 0.17857142857142858, "f": 0.1886792452830189, "fn": 23.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.5555555555555556, "f": 0.4545454545454546, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "This is a bathroom with a large mirror, sink, and shower. The walls are painted white and the floor is made of tile. There is a large window with curtains and a view of the outdoors.\n\nThe bathroom has a large mirror, sink, and shower. The"}, "43376": {"image_id": 43376, "Bleu_1": 0.2916666666605903, "Bleu_2": 0.22281245492303936, "Bleu_3": 0.14793835438670558, "Bleu_4": 1.6377871492796307e-05, "METEOR": 0.24613023424331765, "ROUGE_L": 0.31282051282051276, "CIDEr": 5.885828215537769e-09, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.20689655172413793, "f": 0.20689655172413793, "fn": 23.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.09090909090909091, "f": 0.08695652173913043, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.5555555555555556, "f": 0.4761904761904762, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a giraffe standing on a fence in a zoo. The giraffe is wearing a harness and is looking down at the ground. There are trees and bushes in the background.\n\nThe image is taken from a bird's eye view, with the camera positioned above"}, "204994": {"image_id": 204994, "Bleu_1": 0.13999999999720003, "Bleu_2": 0.09258200997538464, "Bleu_3": 5.631239402103081e-07, "Bleu_4": 1.3961385815466904e-09, "METEOR": 0.1571036046492912, "ROUGE_L": 0.18944099378881987, "CIDEr": 1.901223929437661e-10, "SPICE": {"All": {"pr": 0.2631578947368421, "re": 0.2, "f": 0.22727272727272727, "fn": 20.0, "numImages": 1.0, "fp": 14.0, "tp": 5.0}, "Relation": {"pr": 0.2, "re": 0.125, "f": 0.15384615384615385, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.1, "f": 0.14285714285714288, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.42857142857142855, "f": 0.3529411764705882, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a man standing on a wooden bench next to a giraffe. The man is wearing a black shirt and pants, and has a beard. The giraffe is standing on the ground and has a long neck and spots on its back. The sky is blue and there"}, "309264": {"image_id": 309264, "Bleu_1": 0.2181818181778513, "Bleu_2": 0.08989331499344942, "Bleu_3": 5.342275830329977e-07, "Bleu_4": 1.3085607656499974e-09, "METEOR": 0.16010168207691258, "ROUGE_L": 0.2127164942461932, "CIDEr": 2.1450636728944116e-12, "SPICE": {"All": {"pr": 0.11428571428571428, "re": 0.18181818181818182, "f": 0.14035087719298245, "fn": 18.0, "numImages": 1.0, "fp": 31.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.26666666666666666, "re": 0.6666666666666666, "f": 0.3809523809523809, "fn": 2.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "The image shows a large, open space with many cages containing various types of birds. The cages are made of metal and have perches and food dishes inside. There are also several people standing in the space, looking at the birds. The walls are painted red and there are several windows on the sides of"}, "356028": {"image_id": 356028, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.16205747826515882, "Bleu_3": 0.11412860603242152, "Bleu_4": 0.07312149273622857, "METEOR": 0.1842967468394623, "ROUGE_L": 0.21095100864553315, "CIDEr": 4.179722705060367e-12, "SPICE": {"All": {"pr": 0.12903225806451613, "re": 0.17391304347826086, "f": 0.14814814814814814, "fn": 19.0, "numImages": 1.0, "fp": 27.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.36363636363636365, "f": 0.33333333333333337, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a room with several beds in it. There are no people in the room, but there are some blankets on the beds. The walls are made of wood and there are windows on either side of the room. The floor is made of wood and there are some chairs in the room."}, "544794": {"image_id": 544794, "Bleu_1": 0.30232558138831805, "Bleu_2": 0.2399704669849505, "Bleu_3": 0.17777222549589156, "Bleu_4": 1.9359000638018422e-05, "METEOR": 0.29428737146785894, "ROUGE_L": 0.3238221632382216, "CIDEr": 1.7143157548178832e-06, "SPICE": {"All": {"pr": 0.1, "re": 0.12, "f": 0.1090909090909091, "fn": 22.0, "numImages": 1.0, "fp": 27.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1, "re": 0.125, "f": 0.11111111111111112, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2222222222222222, "f": 0.19999999999999998, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a plate of pizza with mushrooms, onions, and pepperoni on top. There are two slices of pizza on the plate, one with mushrooms and onions, and the other with pepperoni. The plate is on a table with a white tablecloth"}, "264619": {"image_id": 264619, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.23782574707262866, "Bleu_3": 0.16540015368980918, "Bleu_4": 0.09802862511748141, "METEOR": 0.18523703636415342, "ROUGE_L": 0.3114262560777957, "CIDEr": 7.988147461425535e-09, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.08333333333333333, "f": 0.08333333333333333, "fn": 22.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.3333333333333333, "f": 0.21052631578947367, "fn": 4.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a group of people on the beach, with kitesurfers in the background. The sky is clear and blue, with a few clouds in the distance. The waves are crashing against the shore, and there are some surfers in the water. The beach is sandy and has some rocks and"}, "322222": {"image_id": 322222, "Bleu_1": 0.40740740739986286, "Bleu_2": 0.24798277706238742, "Bleu_3": 0.16786747656058937, "Bleu_4": 0.09813690956954695, "METEOR": 0.2852247989733415, "ROUGE_L": 0.29151732377538825, "CIDEr": 8.009154105788346e-11, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.17857142857142858, "f": 0.17543859649122806, "fn": 23.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.07692307692307693, "f": 0.09090909090909093, "fn": 12.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.36363636363636365, "f": 0.32, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a man playing tennis on a court. He is wearing a white shirt and white shorts and is holding a tennis racket in his right hand. The woman in the background is also playing tennis and is wearing a green shirt and white shorts. The two players are on opposite sides"}, "359791": {"image_id": 359791, "Bleu_1": 0.20689655172057078, "Bleu_2": 0.10435177140627983, "Bleu_3": 0.05793449266323809, "Bleu_4": 7.711027176657378e-06, "METEOR": 0.14120703317426975, "ROUGE_L": 0.16850828729281767, "CIDEr": 2.100604090272966e-14, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.2727272727272727, "f": 0.24999999999999994, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.2, "re": 0.2222222222222222, "f": 0.2105263157894737, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.5714285714285714, "f": 0.5, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a group of people standing on a boat in the middle of a body of water. The boat is white and has a large open area on the top for people to stand on. There are several orange chairs lined up along the sides of the boat. In the background, there is a large body"}, "404635": {"image_id": 404635, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.14401064146758893, "Bleu_3": 0.10755901259843535, "Bleu_4": 0.08441965712987705, "METEOR": 0.2701167758218105, "ROUGE_L": 0.32370283018867924, "CIDEr": 1.0194399428150032e-08, "SPICE": {"All": {"pr": 0.36363636363636365, "re": 0.27586206896551724, "f": 0.3137254901960784, "fn": 21.0, "numImages": 1.0, "fp": 14.0, "tp": 8.0}, "Relation": {"pr": 0.25, "re": 0.2222222222222222, "f": 0.23529411764705882, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.6, "re": 0.4, "f": 0.48, "fn": 9.0, "numImages": 1.0, "fp": 4.0, "tp": 6.0}}, "caption": "The image shows a group of elephants standing in a grassy field. They are all facing the same direction and appear to be communicating with each other. The elephants are of different sizes and colors, with some having tusks and others not. The background is a clear blue sky with some clouds"}, "364343": {"image_id": 364343, "Bleu_1": 0.15999999999680004, "Bleu_2": 0.09897433185907904, "Bleu_3": 0.05887550428177755, "Bleu_4": 8.117578843784567e-06, "METEOR": 0.11491582073593913, "ROUGE_L": 0.22732919254658387, "CIDEr": 2.5968758926647532e-11, "SPICE": {"All": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 20.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.1, "f": 0.11111111111111112, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.5, "re": 0.5714285714285714, "f": 0.5333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 4.0}}, "caption": "The image is a plate of food with various fruits and vegetables on it. The plate is surrounded by a white tablecloth and there is a glass of orange juice on the table. The background is a light brown color with a few chairs and a table in the foreground."}, "1573": {"image_id": 1573, "Bleu_1": 0.27083333332769105, "Bleu_2": 0.16974114626800244, "Bleu_3": 8.556026937263809e-07, "Bleu_4": 1.931527870007565e-09, "METEOR": 0.1598337220432785, "ROUGE_L": 0.23461538461538461, "CIDEr": 1.7893938560146244e-09, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.17391304347826086, "f": 0.17777777777777776, "fn": 19.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4, "f": 0.380952380952381, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image is of a kitchen with a stove, sink, and refrigerator. There are several items on the counter, including a kettle, a toaster, and a blender. The walls are painted white and there is a window on the left side of the room. The floor is made"}, "174898": {"image_id": 174898, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.22117152024581002, "Bleu_3": 0.19596643100126745, "Bleu_4": 0.17249699387135528, "METEOR": 0.31446967325331354, "ROUGE_L": 0.3279569892473118, "CIDEr": 1.7359998904895053e-11, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.11764705882352941, "f": 0.12903225806451615, "fn": 30.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.1, "re": 0.09090909090909091, "f": 0.09523809523809525, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.23076923076923078, "f": 0.21428571428571427, "fn": 10.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}}, "caption": "The image shows a black motorcycle parked on the side of a road in front of a row of trees. The motorcycle has a black seat and a black engine. The trees are bare and the sky is clear. The sun is shining down on the motorcycle. The road is paved and there are"}, "527580": {"image_id": 527580, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.1842190315423256, "Bleu_3": 0.1107243128286132, "Bleu_4": 1.2901292604257922e-05, "METEOR": 0.13929411764705885, "ROUGE_L": 0.25523012552301255, "CIDEr": 3.606653290379805e-09, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.10714285714285714, "f": 0.11764705882352941, "fn": 25.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2727272727272727, "f": 0.2608695652173913, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a train on the beach with people sitting on the sand and umbrellas set up. There are buildings in the background with palm trees and mountains in the distance. The sky is cloudy with a few white clouds.\n\nThe train is a vintage style train with wooden seats and"}, "522020": {"image_id": 522020, "Bleu_1": 0.23214285713871177, "Bleu_2": 0.20544535089962757, "Bleu_3": 0.1575153781294635, "Bleu_4": 0.11019978611152552, "METEOR": 0.26605174811221965, "ROUGE_L": 0.21229698375870068, "CIDEr": 5.612992973469426e-13, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.18518518518518517, "f": 0.20408163265306123, "fn": 22.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.09090909090909091, "f": 0.14285714285714288, "fn": 10.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.25, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4444444444444444, "f": 0.3636363636363637, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image is of a white cat sitting in the grass. The cat has a fluffy coat and is looking directly at the camera. The background is a house with a window on the left side and a door on the right side. The sky is blue and there are some clouds in the background.\n\nThe"}, "142890": {"image_id": 142890, "Bleu_1": 0.22413793103061835, "Bleu_2": 0.10861276726025744, "Bleu_3": 0.05950104277147965, "Bleu_4": 7.86688425224129e-06, "METEOR": 0.22324478354792437, "ROUGE_L": 0.23041225987481118, "CIDEr": 1.6583795252480313e-13, "SPICE": {"All": {"pr": 0.17647058823529413, "re": 0.3157894736842105, "f": 0.22641509433962262, "fn": 13.0, "numImages": 1.0, "fp": 28.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.18181818181818182, "re": 0.3333333333333333, "f": 0.23529411764705885, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}, "Size": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4444444444444444, "f": 0.3636363636363637, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The cat is sitting on the keyboard of a computer. The cat is looking at the screen of the computer. The cat is black and has a white collar around its neck. The cat is sitting on the keyboard of the computer with its front paws on the keys. The cat is looking at the screen of the"}, "503238": {"image_id": 503238, "Bleu_1": 0.30188679244713423, "Bleu_2": 0.2285817953234835, "Bleu_3": 0.17238297400489144, "Bleu_4": 0.1324062816098676, "METEOR": 0.3104730845757504, "ROUGE_L": 0.3190005810575247, "CIDEr": 5.951514588124436e-11, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.15384615384615385, "f": 0.1509433962264151, "fn": 22.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.36363636363636365, "f": 0.36363636363636365, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a red bus driving down a busy street in a city. The bus is stopped at a red light, and there are people walking on the sidewalk. There are tall buildings in the background.\n\nThe bus is a double decker with a large number 10 on the front. It has"}, "522430": {"image_id": 522430, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.18857036045053577, "Bleu_3": 0.1278888396607359, "Bleu_4": 0.08042331753470428, "METEOR": 0.18139354254012358, "ROUGE_L": 0.18496058217101274, "CIDEr": 1.208191478732134e-11, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.17391304347826086, "f": 0.17777777777777776, "fn": 19.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of cows standing in a field. They are all brown and white in color and have long, curly hair. They are standing in a line, looking at the camera. The sun is setting in the background, casting a warm glow over the scene. The cows are standing in"}, "155897": {"image_id": 155897, "Bleu_1": 0.21999999999560005, "Bleu_2": 0.13401187884939042, "Bleu_3": 0.09078730341863991, "Bleu_4": 0.06316759949914878, "METEOR": 0.20435368459500278, "ROUGE_L": 0.19830949284785435, "CIDEr": 5.5148830520946874e-11, "SPICE": {"All": {"pr": 0.16, "re": 0.18181818181818182, "f": 0.1702127659574468, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.125, "f": 0.16666666666666666, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.375, "f": 0.35294117647058826, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a woman sitting on a bench, holding a sandwich in her hand. The sandwich appears to be made of bread, meat, and cheese. The woman is wearing a blue shirt and shorts, and has a red hat on her head. There are several people in the background"}, "214494": {"image_id": 214494, "Bleu_1": 0.28947368420290864, "Bleu_2": 0.25017774619423927, "Bleu_3": 0.17342247040502523, "Bleu_4": 0.11048721210035556, "METEOR": 0.28841042659823857, "ROUGE_L": 0.34491114701130854, "CIDEr": 1.2646138816791375e-05, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.19230769230769232, "f": 0.20833333333333331, "fn": 21.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a chocolate cake with a knife cutting into it. The cake is on a black plate and there is a hand holding the knife. The background is dark and there is no other visible detail."}, "223093": {"image_id": 223093, "Bleu_1": 0.3333333333274854, "Bleu_2": 0.24397501823281484, "Bleu_3": 0.16297813360147376, "Bleu_4": 1.68266850141747e-05, "METEOR": 0.2811199407259054, "ROUGE_L": 0.3284868066774367, "CIDEr": 1.1647264063341685e-13, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.16, "f": 0.1568627450980392, "fn": 21.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.14285714285714285, "f": 0.16666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.25, "f": 0.2608695652173913, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a brick building with a clock tower on top. The clock tower has a large clock face on it and is surrounded by trees. The sky is blue and there are clouds in the background.\n\nThe building appears to be old and has a lot of character. The clock tower is tall and imposing"}, "422706": {"image_id": 422706, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.1650868521781302, "Bleu_3": 0.10158210507506187, "Bleu_4": 0.06733182132555685, "METEOR": 0.18287327908336967, "ROUGE_L": 0.2582010582010582, "CIDEr": 1.701660694323381e-10, "SPICE": {"All": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 22.0, "numImages": 1.0, "fp": 26.0, "tp": 0.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}}, "caption": "The image shows a group of people standing on the deck of a cruise ship looking out at the ocean. The people are wearing life jackets and the ship is equipped with safety equipment. The image is taken from a distance and shows the ship's size and shape.\n\nThe image is taken from"}, "4011": {"image_id": 4011, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.12778110819056318, "Bleu_3": 0.06841022856919289, "Bleu_4": 8.945369410433846e-06, "METEOR": 0.25334045507409697, "ROUGE_L": 0.29781924704350654, "CIDEr": 1.0656122550703776e-09, "SPICE": {"All": {"pr": 0.13793103448275862, "re": 0.19047619047619047, "f": 0.16, "fn": 17.0, "numImages": 1.0, "fp": 25.0, "tp": 4.0}, "Relation": {"pr": 0.1, "re": 0.16666666666666666, "f": 0.125, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3333333333333333, "f": 0.27272727272727276, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The woman is wearing a black apron and holding a cake that appears to be a dog. The cake is made of white fondant and has a realistic dog face on it. The woman is smiling and appears to be proud of her creation. The background is a kitchen with white cabinets and"}, "188824": {"image_id": 188824, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.25556221638112636, "Bleu_3": 0.21718893759224348, "Bleu_4": 0.18725075919606607, "METEOR": 0.2841446824659466, "ROUGE_L": 0.3070715141829804, "CIDEr": 2.1508890464060283e-10, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.1111111111111111, "f": 0.09523809523809525, "fn": 16.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a cat sitting on a couch next to a television. The cat is looking at the television with its head tilted to the side. The couch is covered in a patterned fabric and has a pillow on it. The television is on and shows a cartoon playing on it. The"}, "247206": {"image_id": 247206, "Bleu_1": 0.28571428570918367, "Bleu_2": 0.2279211529151689, "Bleu_3": 0.15670342877076837, "Bleu_4": 1.6414962956838477e-05, "METEOR": 0.2553441511271417, "ROUGE_L": 0.2772727272727273, "CIDEr": 4.2594436124603437e-13, "SPICE": {"All": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 21.0, "tp": 7.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.25, "f": 0.18181818181818182, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.07692307692307693, "re": 0.3333333333333333, "f": 0.125, "fn": 2.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Size": {"pr": 0.25, "re": 1.0, "f": 0.4, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.625, "re": 0.7142857142857143, "f": 0.6666666666666666, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 5.0}}, "caption": "The image is of a man lying on the floor with a cat on his chest. The cat is looking up at the man with its eyes. The man is holding a toy in his hand. The background is a wall with a window in the background.\n\nThe image is of a man lying on the"}, "430047": {"image_id": 430047, "Bleu_1": 0.3818181818112397, "Bleu_2": 0.27888667550624097, "Bleu_3": 0.2064953630318252, "Bleu_4": 0.13565611984581905, "METEOR": 0.27589985917859344, "ROUGE_L": 0.26114211451723873, "CIDEr": 6.338395874272961e-11, "SPICE": {"All": {"pr": 0.15, "re": 0.125, "f": 0.13636363636363635, "fn": 21.0, "numImages": 1.0, "fp": 17.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image is a living room with a television on the wall, a coffee table in the center of the room, and a couch against the wall. The room is well lit and has a large window with curtains. The floor is made of hardwood and the walls are painted white. There are no other"}, "244240": {"image_id": 244240, "Bleu_1": 0.24999999999479172, "Bleu_2": 0.1630820182599269, "Bleu_3": 0.10496103543446565, "Bleu_4": 1.2660998324356511e-05, "METEOR": 0.16130589410023513, "ROUGE_L": 0.29383429672447015, "CIDEr": 1.7986593055791413e-08, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.0967741935483871, "f": 0.1111111111111111, "fn": 28.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.2727272727272727, "f": 0.3, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a toilet in a bathroom with a sink and a shower head. The toilet is white and has a seat and lid. The sink is made of stainless steel and has a faucet. The shower head is made of plastic and has a hose attached"}, "49810": {"image_id": 49810, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.16817499303341832, "Bleu_3": 0.08111147935548949, "Bleu_4": 1.0064933408714859e-05, "METEOR": 0.2540715457376437, "ROUGE_L": 0.2513243084167157, "CIDEr": 4.093230085243932e-12, "SPICE": {"All": {"pr": 0.2916666666666667, "re": 0.4375, "f": 0.35000000000000003, "fn": 9.0, "numImages": 1.0, "fp": 17.0, "tp": 7.0}, "Relation": {"pr": 0.2857142857142857, "re": 0.3333333333333333, "f": 0.30769230769230765, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.625, "f": 0.5263157894736842, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a cat sitting on a wooden deck looking at its reflection in a mirror. The cat has a white coat with orange patches on its face and ears. The cat's eyes are looking directly at the camera. The background is a blurred image of trees and buildings. The lighting is natural"}, "85914": {"image_id": 85914, "Bleu_1": 0.3829787233961069, "Bleu_2": 0.2235033479141667, "Bleu_3": 0.1304553364641582, "Bleu_4": 0.08428168658384227, "METEOR": 0.24531704650602038, "ROUGE_L": 0.38547394337299956, "CIDEr": 1.2389413731872758e-05, "SPICE": {"All": {"pr": 0.2916666666666667, "re": 0.30434782608695654, "f": 0.2978723404255319, "fn": 16.0, "numImages": 1.0, "fp": 17.0, "tp": 7.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.125, "f": 0.11764705882352941, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4444444444444444, "f": 0.4444444444444444, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image is a plate of food with a variety of vegetables, including broccoli, potatoes, and onions. There is also a piece of meat on the plate. The plate is on a white tablecloth.\n\nThe image is a plate of food with a variety of vegetables, including"}, "442942": {"image_id": 442942, "Bleu_1": 0.15094339622356714, "Bleu_2": 0.0933181271719728, "Bleu_3": 5.547797716282368e-07, "Bleu_4": 1.3594021798767564e-09, "METEOR": 0.18035286262756295, "ROUGE_L": 0.18496058217101274, "CIDEr": 1.4339304912819617e-12, "SPICE": {"All": {"pr": 0.24, "re": 0.17142857142857143, "f": 0.19999999999999998, "fn": 29.0, "numImages": 1.0, "fp": 19.0, "tp": 6.0}, "Relation": {"pr": 0.1, "re": 0.08333333333333333, "f": 0.0909090909090909, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.3333333333333333, "f": 0.3846153846153846, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a train on a track in the middle of a field. There are people standing on the platform and in the train. The train has a red and white striped awning on top and a green and white striped awning on the sides. The people are wearing hats and sunglass"}, "162543": {"image_id": 162543, "Bleu_1": 0.1538461538431953, "Bleu_2": 0.12281268769488377, "Bleu_3": 0.06706647632268699, "Bleu_4": 8.857886206727456e-06, "METEOR": 0.13874581036450803, "ROUGE_L": 0.17983490566037735, "CIDEr": 3.07420484048587e-11, "SPICE": {"All": {"pr": 0.13636363636363635, "re": 0.15789473684210525, "f": 0.14634146341463414, "fn": 16.0, "numImages": 1.0, "fp": 19.0, "tp": 3.0}, "Relation": {"pr": 0.1, "re": 0.1, "f": 0.10000000000000002, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.2857142857142857, "f": 0.25, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a group of elephants standing in a field with a fence in the background\n\nThe elephants are standing in a field with a fence in the background. They are wearing collars and are standing in a line. There are trees in the background and a building in the distance"}, "157352": {"image_id": 157352, "Bleu_1": 0.20454545454080583, "Bleu_2": 0.09753841331211074, "Bleu_3": 0.0609584657118344, "Bleu_4": 8.621434964642648e-06, "METEOR": 0.1745647847910774, "ROUGE_L": 0.20890410958904113, "CIDEr": 7.292277228908375e-08, "SPICE": {"All": {"pr": 0.10714285714285714, "re": 0.14285714285714285, "f": 0.12244897959183672, "fn": 18.0, "numImages": 1.0, "fp": 25.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a group of people skateboarding on a ramp. They are all wearing skateboarding gear, including helmets, pads, and shoes. The ramp is made of concrete and has a smooth surface. The sky is blue and there are trees in the background"}, "262810": {"image_id": 262810, "Bleu_1": 0.1874999999960938, "Bleu_2": 0.0631613940786591, "Bleu_3": 4.426378293466139e-07, "Bleu_4": 1.178238656172971e-09, "METEOR": 0.16926699485570507, "ROUGE_L": 0.14523809523809522, "CIDEr": 2.339896305657893e-08, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.09523809523809523, "f": 0.09302325581395349, "fn": 19.0, "numImages": 1.0, "fp": 20.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.2, "f": 0.2105263157894737, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a wedding reception in a large ballroom. There are several tables set up with white tablecloths and candles on them. The bride and groom are standing at the head table, smiling and holding hands. The guests are seated at the tables, chatting and taking pictures"}, "498807": {"image_id": 498807, "Bleu_1": 0.21276595744228163, "Bleu_2": 0.16658955971162087, "Bleu_3": 0.13511835401219335, "Bleu_4": 0.10290348647814142, "METEOR": 0.18371910077781747, "ROUGE_L": 0.24416277518345564, "CIDEr": 2.1414886761721213e-09, "SPICE": {"All": {"pr": 0.07407407407407407, "re": 0.09523809523809523, "f": 0.08333333333333333, "fn": 19.0, "numImages": 1.0, "fp": 25.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2222222222222222, "f": 0.1904761904761905, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a woman surfing on a pink surfboard in the ocean. She is wearing a black wetsuit and a helmet, and her hair is blowing in the wind. The waves are crashing against the shore, and there are other surfers in the background. The sky"}, "563605": {"image_id": 563605, "Bleu_1": 0.1960784313687044, "Bleu_2": 0.1252448582145496, "Bleu_3": 0.08618888098293648, "Bleu_4": 0.060433552207849656, "METEOR": 0.14608080600564144, "ROUGE_L": 0.18654434250764526, "CIDEr": 6.111886596580441e-10, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.12, "f": 0.13043478260869565, "fn": 22.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2727272727272727, "f": 0.2608695652173913, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of people standing in front of a building with umbrellas. They are all wearing black and white outfits and have their heads down. The building is a large, white structure with columns and arches. There are people walking on the sidewalk in front of the building."}, "162503": {"image_id": 162503, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.2690691175932999, "Bleu_3": 0.22626422750896208, "Bleu_4": 0.19406656348953982, "METEOR": 0.2765796969763782, "ROUGE_L": 0.30049261083743845, "CIDEr": 6.19372024919349e-11, "SPICE": {"All": {"pr": 0.17647058823529413, "re": 0.21428571428571427, "f": 0.1935483870967742, "fn": 11.0, "numImages": 1.0, "fp": 14.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.25, "f": 0.26666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The image shows a bird perched on a branch in a forest. The bird is a small, brown and white owl with a round face and large eyes. It is perched on a thin branch in a tall pine tree, surrounded by other trees and underbrush. The forest is dense and green,"}, "62089": {"image_id": 62089, "Bleu_1": 0.34782608694896033, "Bleu_2": 0.24866794885861784, "Bleu_3": 0.14112555009560282, "Bleu_4": 1.5989582418932847e-05, "METEOR": 0.2596649361768378, "ROUGE_L": 0.35305466237942124, "CIDEr": 8.532458890353638e-07, "SPICE": {"All": {"pr": 0.047619047619047616, "re": 0.038461538461538464, "f": 0.0425531914893617, "fn": 25.0, "numImages": 1.0, "fp": 20.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.08333333333333333, "re": 0.1111111111111111, "f": 0.09523809523809525, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}}, "caption": "The image shows a young boy wearing a skateboard helmet and knee pads, standing on a skateboard ramp in an empty parking lot. The boy is wearing a black shirt with white stripes and black pants. The ramp is made of concrete and has a metal"}, "340737": {"image_id": 340737, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.1650868521781302, "Bleu_3": 0.10158210507506187, "Bleu_4": 0.06733182132555685, "METEOR": 0.21987161128143942, "ROUGE_L": 0.21863799283154117, "CIDEr": 3.680073907703811e-12, "SPICE": {"All": {"pr": 0.2916666666666667, "re": 0.4375, "f": 0.35000000000000003, "fn": 9.0, "numImages": 1.0, "fp": 17.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 3.0}, "Size": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Color": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.6666666666666666, "f": 0.4444444444444444, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image is of a bathroom with a bathtub, sink, and toilet. The walls are yellow and the floor is made of tile. There is a window on the left side of the room and a door on the right side. The room is well lit and has a large mirror on the wall"}, "423744": {"image_id": 423744, "Bleu_1": 0.33333333332716053, "Bleu_2": 0.20982172726164103, "Bleu_3": 0.11919074177013061, "Bleu_4": 1.3498619260852726e-05, "METEOR": 0.23768549346889736, "ROUGE_L": 0.25507765830346474, "CIDEr": 2.636816692304078e-10, "SPICE": {"All": {"pr": 0.3, "re": 0.4090909090909091, "f": 0.34615384615384615, "fn": 13.0, "numImages": 1.0, "fp": 21.0, "tp": 9.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}, "Size": {"pr": 0.25, "re": 1.0, "f": 0.4, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5454545454545454, "re": 0.75, "f": 0.631578947368421, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 6.0}}, "caption": "This is a computer workstation with a laptop and a monitor on top of it. The laptop is open and the monitor is turned on. There are several tools and wires on the workbench next to the computer. The workbench is made of wood and has a small table on top of it. There"}, "343903": {"image_id": 343903, "Bleu_1": 0.34782608694896033, "Bleu_2": 0.24866794885861784, "Bleu_3": 0.17780705124344343, "Bleu_4": 0.1069287822328279, "METEOR": 0.2700670896776397, "ROUGE_L": 0.42318365287588294, "CIDEr": 2.311681343015473e-05, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.14285714285714285, "f": 0.1694915254237288, "fn": 30.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.26666666666666666, "f": 0.32, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a plate with a blueberry pancake on it, sitting on top of a pink and white checkered tablecloth. There is a glass of milk next to the plate. The background is a colorful, patterned tablecloth.\n\nThe image is taken in a kitchen or"}, "117786": {"image_id": 117786, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.17131872291347974, "Bleu_3": 0.1191907417701306, "Bleu_4": 0.09027070761263256, "METEOR": 0.18870431488422804, "ROUGE_L": 0.24355464293862653, "CIDEr": 8.168975744173439e-11, "SPICE": {"All": {"pr": 0.15, "re": 0.11538461538461539, "f": 0.13043478260869565, "fn": 23.0, "numImages": 1.0, "fp": 17.0, "tp": 3.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.1111111111111111, "f": 0.125, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.125, "f": 0.18181818181818182, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.1, "re": 0.1111111111111111, "f": 0.10526315789473685, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}}, "caption": "The image shows a man standing on a sidewalk in front of a large building with a large dome on top. The building appears to be made of stone and has a large, ornate facade. The man is wearing a white shirt and black pants and is holding a small bird in his hand"}, "3693": {"image_id": 3693, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.1258455564244646, "Bleu_3": 0.06816612219878869, "Bleu_4": 8.966592262802168e-06, "METEOR": 0.2012440912902509, "ROUGE_L": 0.25722891566265055, "CIDEr": 6.430386631746808e-12, "SPICE": {"All": {"pr": 0.16, "re": 0.25, "f": 0.19512195121951217, "fn": 12.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.6666666666666666, "f": 0.4705882352941177, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a group of people standing in front of a graffiti wall. They are all wearing black and white clothing and have their hands in their pockets. The wall has various colors and designs on it, including blue, green, and red. There are also some words written on the wall,"}, "187852": {"image_id": 187852, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.22562131409418867, "Bleu_3": 0.12674769789184412, "Bleu_4": 0.08028899837277662, "METEOR": 0.18682896779187994, "ROUGE_L": 0.24349908759124086, "CIDEr": 1.8429767931668945e-08, "SPICE": {"All": {"pr": 0.21875, "re": 0.35, "f": 0.2692307692307692, "fn": 13.0, "numImages": 1.0, "fp": 25.0, "tp": 7.0}, "Relation": {"pr": 0.1, "re": 0.3333333333333333, "f": 0.15384615384615383, "fn": 2.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.2222222222222222, "f": 0.2105263157894737, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.4, "re": 0.5, "f": 0.4444444444444445, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of people standing in front of a building. They are all wearing sunglasses and some are holding cameras. There are several people in the background, including a man in a white shirt and a woman in a pink shirt. The building in the background appears to be"}, "414078": {"image_id": 414078, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.19154015512649578, "Bleu_3": 0.1283943344001008, "Bleu_4": 1.427305210255098e-05, "METEOR": 0.2289148392592537, "ROUGE_L": 0.2367487247726769, "CIDEr": 2.3984724076792537e-11, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.09090909090909091, "f": 0.09090909090909091, "fn": 20.0, "numImages": 1.0, "fp": 20.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a black and white cat lying on a blue blanket on a bed. The cat is looking up at the camera with its eyes. The bed has a blue sheet and a blue blanket on it. The cat is wearing a collar with a tag on it.\n\nThe image is taken"}, "121716": {"image_id": 121716, "Bleu_1": 0.27999999999440006, "Bleu_2": 0.15118578920063636, "Bleu_3": 7.808966656031396e-07, "Bleu_4": 1.78410623850286e-09, "METEOR": 0.1747107948028988, "ROUGE_L": 0.18944099378881987, "CIDEr": 3.8488482263822053e-10, "SPICE": {"All": {"pr": 0.2962962962962963, "re": 0.23529411764705882, "f": 0.2622950819672131, "fn": 26.0, "numImages": 1.0, "fp": 19.0, "tp": 8.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.08333333333333333, "f": 0.09523809523809525, "fn": 11.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.4375, "f": 0.4666666666666667, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 7.0}}, "caption": "The image shows a person skiing down a snowy slope. The person is wearing a black jacket, black pants, and black ski boots. They are holding onto two skis and are jumping off a snowy slope. The sky is blue and there are no other people or objects in the"}, "327794": {"image_id": 327794, "Bleu_1": 0.3111111111041976, "Bleu_2": 0.11891767799944002, "Bleu_3": 6.902527228627572e-07, "Bleu_4": 1.6727990522975117e-09, "METEOR": 0.19988257075200305, "ROUGE_L": 0.19934640522875818, "CIDEr": 9.06937696765732e-08, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.1, "f": 0.11764705882352941, "fn": 27.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "This is a pot of vegetables and meat cooking in a pan on the stove. The vegetables are carrots, onions, and mushrooms, and the meat is chicken. The pot is made of stainless steel and has a lid on top. The stove is electric and"}, "143370": {"image_id": 143370, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.1814970425910117, "Bleu_3": 0.12634404006922517, "Bleu_4": 0.09574437304568632, "METEOR": 0.2664694247880666, "ROUGE_L": 0.361711139347734, "CIDEr": 1.991382646850797e-07, "SPICE": {"All": {"pr": 0.2, "re": 0.15789473684210525, "f": 0.17647058823529413, "fn": 32.0, "numImages": 1.0, "fp": 24.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.11764705882352941, "f": 0.15999999999999998, "fn": 15.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4, "f": 0.34782608695652173, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image is of a bathroom with a toilet, sink, and mirror. The walls are painted white and there is a window on one side of the room. The floor is made of hardwood and there is a rug in the corner of the room. The toilet is a white porcelain"}, "354202": {"image_id": 354202, "Bleu_1": 0.20370370369993146, "Bleu_2": 0.16402518938356242, "Bleu_3": 0.11578340987777527, "Bleu_4": 0.07427492256013218, "METEOR": 0.3064214737573749, "ROUGE_L": 0.25507765830346474, "CIDEr": 1.1969764698574995e-12, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.19230769230769232, "f": 0.2, "fn": 21.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.5, "f": 0.41666666666666663, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows two giraffes standing on a fence in a zoo. They are both brown and have long necks and legs. One of them is looking directly at the camera while the other is looking away. The fence is made of metal and has a gate in the middle. There are trees and"}, "189193": {"image_id": 189193, "Bleu_1": 0.2653061224435652, "Bleu_2": 0.18210783976741574, "Bleu_3": 8.902660440381883e-07, "Bleu_4": 1.9790209828892805e-09, "METEOR": 0.22239791195941241, "ROUGE_L": 0.26940063091482647, "CIDEr": 3.528870624361402e-10, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.10526315789473684, "f": 0.0930232558139535, "fn": 17.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.2857142857142857, "f": 0.25, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a woman standing on a skateboard in the middle of a parking lot. She is wearing a black hat and a green sweater, and has a backpack on her back. There is a white car parked in the background.\n\nThe woman is standing on the skateboard"}, "561967": {"image_id": 561967, "Bleu_1": 0.2909090909038017, "Bleu_2": 0.164121987969432, "Bleu_3": 7.980300087819457e-07, "Bleu_4": 1.7681275091206876e-09, "METEOR": 0.22519867906260388, "ROUGE_L": 0.2363032650802435, "CIDEr": 1.274083232416331e-12, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.10526315789473684, "f": 0.12903225806451615, "fn": 17.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.16666666666666666, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.14285714285714285, "f": 0.16666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}}, "caption": "The image shows a train traveling along a track with other trains in the background. The train has a yellow and green color scheme and is pulling a cargo car. There are buildings in the background and trees in the foreground. The sky is cloudy and there is snow on the ground.\n\nThe train is"}, "404071": {"image_id": 404071, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.1978218215826409, "Bleu_3": 0.14438930089696148, "Bleu_4": 0.10423563468018283, "METEOR": 0.22298949810065394, "ROUGE_L": 0.24970760233918127, "CIDEr": 5.567214972544156e-12, "SPICE": {"All": {"pr": 0.10714285714285714, "re": 0.16666666666666666, "f": 0.13043478260869565, "fn": 15.0, "numImages": 1.0, "fp": 25.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2857142857142857, "f": 0.2105263157894737, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a train traveling along a track in the countryside. The train is blue and white with a red stripe on the front. There are trees and bushes on either side of the track and a bridge in the distance. The sky is clear and there are no clouds. The train is"}, "251572": {"image_id": 251572, "Bleu_1": 0.32692307691678996, "Bleu_2": 0.21182963642996727, "Bleu_3": 0.15311620018402886, "Bleu_4": 0.11002068284143864, "METEOR": 0.2949834651302108, "ROUGE_L": 0.35219399538106233, "CIDEr": 3.425962121166846e-10, "SPICE": {"All": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 16.0, "numImages": 1.0, "fp": 16.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a woman sitting on a couch with a small puppy in her lap. The woman is wearing a gray sweater and has her hair tied back in a ponytail. The puppy is a small white and brown dog with a red collar. The background is a beige colored wall"}, "436791": {"image_id": 436791, "Bleu_1": 0.29629629629080934, "Bleu_2": 0.24798277706238742, "Bleu_3": 0.21149976731142073, "Bleu_4": 0.1826533698053674, "METEOR": 0.3287080240385856, "ROUGE_L": 0.3210526315789473, "CIDEr": 3.192621699974654e-11, "SPICE": {"All": {"pr": 0.125, "re": 0.15, "f": 0.13636363636363635, "fn": 17.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.2222222222222222, "re": 0.25, "f": 0.23529411764705882, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The man in the image is wearing a black jacket and is holding a cell phone to his ear. He is walking down the street, looking at the ground. The sky is blue and there are trees in the background. The man's hair is short and he has a beard. He is wearing"}, "319257": {"image_id": 319257, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.15357377920556234, "Bleu_3": 0.07733118331435387, "Bleu_4": 9.806713568284028e-06, "METEOR": 0.15586483732453263, "ROUGE_L": 0.24811156304474144, "CIDEr": 3.957608330526502e-12, "SPICE": {"All": {"pr": 0.3, "re": 0.24, "f": 0.2666666666666666, "fn": 19.0, "numImages": 1.0, "fp": 14.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2857142857142857, "f": 0.30769230769230765, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 4.0}}, "caption": "The image shows a cat sitting on a windowsill, looking out the window. The cat is wearing a collar and has a tag on its neck. The cat is sitting on top of a potted plant with long, thin leaves. The plant is sitting on a windowsill, and the cat is looking out"}, "279437": {"image_id": 279437, "Bleu_1": 0.2653061224435652, "Bleu_2": 0.2102800206225174, "Bleu_3": 0.14132092548377867, "Bleu_4": 1.573857459307643e-05, "METEOR": 0.2503301256206706, "ROUGE_L": 0.3175245806824754, "CIDEr": 1.067718664329851e-08, "SPICE": {"All": {"pr": 0.125, "re": 0.15, "f": 0.13636363636363635, "fn": 17.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.375, "f": 0.33333333333333326, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The room appears to be a bedroom with a bed, a dresser, and a window with curtains. The walls are painted with a floral pattern and there is a rug on the floor. The bed has a white comforter and pillows. There is a vase on the nightstand with"}, "175718": {"image_id": 175718, "Bleu_1": 0.34782608694896033, "Bleu_2": 0.24866794885861784, "Bleu_3": 0.14112555009560282, "Bleu_4": 0.08991602966703008, "METEOR": 0.22778191019701563, "ROUGE_L": 0.2827814569536423, "CIDEr": 3.767060451812703e-08, "SPICE": {"All": {"pr": 0.28125, "re": 0.34615384615384615, "f": 0.3103448275862069, "fn": 17.0, "numImages": 1.0, "fp": 23.0, "tp": 9.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.5454545454545454, "re": 0.6, "f": 0.5714285714285713, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 6.0}}, "caption": "The image shows a pizza on a wooden cutting board with a beer bottle next to it. The pizza has cheese, pepperoni, and onions on it. The beer bottle has a label that reads \"Bud Light.\" There is a coffee mug on the cutting board as"}, "126671": {"image_id": 126671, "Bleu_1": 0.28571428570845486, "Bleu_2": 0.20412414522772235, "Bleu_3": 0.1385492093235938, "Bleu_4": 0.10369816700419776, "METEOR": 0.27168396070944933, "ROUGE_L": 0.3753846153846154, "CIDEr": 4.253743391740142e-09, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.12, "f": 0.11764705882352941, "fn": 22.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.2727272727272727, "f": 0.24999999999999994, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image is of a bathroom with a toilet, sink, and shower. The walls are painted white and the floor is made of tile. There is a window on the left side of the room with curtains. The shower has a curtain and a shower head. The sink has"}, "281424": {"image_id": 281424, "Bleu_1": 0.16981132075151303, "Bleu_2": 0.11429089766174176, "Bleu_3": 0.0915921222537406, "Bleu_4": 0.07445752386401949, "METEOR": 0.17885733801257842, "ROUGE_L": 0.18908865468071917, "CIDEr": 3.952897765115136e-12, "SPICE": {"All": {"pr": 0.13636363636363635, "re": 0.125, "f": 0.13043478260869565, "fn": 21.0, "numImages": 1.0, "fp": 19.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a cat sleeping in a suitcase. The cat is lying on its side with its paws tucked under its body. The suitcase is open and the cat is lying inside. There is a small table next to the suitcase with a lamp on it. The room appears to be a"}, "136": {"image_id": 136, "Bleu_1": 0.23999999999520005, "Bleu_2": 0.09897433185907906, "Bleu_3": 5.887550428177757e-07, "Bleu_4": 1.4435323317269659e-09, "METEOR": 0.15195076636427327, "ROUGE_L": 0.22732919254658387, "CIDEr": 7.397016239977097e-11, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.12, "f": 0.11764705882352941, "fn": 22.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a group of giraffes standing in a large enclosure. They are looking out of the window at something in the distance. The enclosure is made of metal and has a large door that is open. There are several people standing in the enclosure, looking at the giraffes."}, "71929": {"image_id": 71929, "Bleu_1": 0.2941176470530566, "Bleu_2": 0.17149858513911254, "Bleu_3": 0.10628002947657487, "Bleu_4": 0.07071775095333474, "METEOR": 0.21827455363727852, "ROUGE_L": 0.28247746630281984, "CIDEr": 9.008955356987004e-08, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.125, "f": 0.1509433962264151, "fn": 28.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.07692307692307693, "f": 0.10526315789473684, "fn": 12.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.2727272727272727, "f": 0.3, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a plate of food with shrimp, rice, and vegetables on it. There is also a glass of water on the table. The background is a blue sky with white clouds.\n\nThe image is taken from a boat in the ocean. The sun is shining brightly in the sky"}, "69293": {"image_id": 69293, "Bleu_1": 0.2653061224435652, "Bleu_2": 0.18210783976741574, "Bleu_3": 0.11216649288903491, "Bleu_4": 0.07442312429536306, "METEOR": 0.2412389310788975, "ROUGE_L": 0.3386798272671191, "CIDEr": 2.0051745714212554e-09, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.08695652173913043, "f": 0.1111111111111111, "fn": 21.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.2222222222222222, "f": 0.26666666666666666, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}}, "caption": "The image shows a bathroom with a toilet, sink, and mirror. The floor is made of tile and the walls are made of wood. There is a shower in the corner of the room. The toilet is a white porcelain bowl with a seat and lid. The sink is"}, "90040": {"image_id": 90040, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.14793756076371775, "Bleu_3": 0.09382178207455442, "Bleu_4": 1.1226048713415489e-05, "METEOR": 0.20865781107686254, "ROUGE_L": 0.23788300835654594, "CIDEr": 1.271030657971761e-12, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.20833333333333334, "f": 0.21739130434782608, "fn": 19.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.09090909090909091, "f": 0.1111111111111111, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.375, "f": 0.3157894736842105, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a river with boats docked on the shore. There are buildings on the other side of the river, and a person is standing on the shore. The sky is cloudy and there are trees in the background.\n\nThe image is taken from a low angle, looking down at the river and the"}, "409646": {"image_id": 409646, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.15639188177115004, "Bleu_3": 0.07776932968046642, "Bleu_4": 9.799721254702973e-06, "METEOR": 0.19865621367817765, "ROUGE_L": 0.21585279547062985, "CIDEr": 3.5795977896851924e-12, "SPICE": {"All": {"pr": 0.25, "re": 0.22727272727272727, "f": 0.23809523809523808, "fn": 17.0, "numImages": 1.0, "fp": 15.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 1.0, "re": 0.14285714285714285, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5714285714285714, "f": 0.4444444444444444, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a black bear standing in the middle of a road. The bear is looking at the camera with its head tilted to the side. The background is a green field with trees in the distance. The sky is cloudy with some sun peeking through the clouds. The car is parked on"}, "296383": {"image_id": 296383, "Bleu_1": 0.3333333333253969, "Bleu_2": 0.25503068521918915, "Bleu_3": 0.14815567561510906, "Bleu_4": 1.6993098242184558e-05, "METEOR": 0.19771516601589958, "ROUGE_L": 0.36288951841359773, "CIDEr": 1.701475996864362e-05, "SPICE": {"All": {"pr": 0.06451612903225806, "re": 0.08, "f": 0.07142857142857142, "fn": 23.0, "numImages": 1.0, "fp": 29.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2222222222222222, "f": 0.1904761904761905, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a cell phone with a green light emitting from the screen. The phone is on a wooden floor.\n\nThe image shows a cell phone with a green light emitting from the screen. The phone is on a wooden floor."}, "566672": {"image_id": 566672, "Bleu_1": 0.3061224489733445, "Bleu_2": 0.2258769757216551, "Bleu_3": 0.16314309133869306, "Bleu_4": 1.7528189410229203e-05, "METEOR": 0.23329409622579997, "ROUGE_L": 0.2982103884766477, "CIDEr": 2.528728923504094e-09, "SPICE": {"All": {"pr": 0.06896551724137931, "re": 0.06896551724137931, "f": 0.06896551724137931, "fn": 27.0, "numImages": 1.0, "fp": 27.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2, "f": 0.17391304347826086, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a baseball player hitting a ball on a baseball field. The player is wearing a white jersey and black pants, and is holding a bat. The umpire is standing behind the plate, watching the player hit the ball. The crowd is cheering in the background.\n\nThe"}, "202865": {"image_id": 202865, "Bleu_1": 0.3541666666592882, "Bleu_2": 0.2126329371370835, "Bleu_3": 0.12526923159832437, "Bleu_4": 1.4457066598210432e-05, "METEOR": 0.1984735960030748, "ROUGE_L": 0.23036253776435048, "CIDEr": 9.04188556816719e-10, "SPICE": {"All": {"pr": 0.15151515151515152, "re": 0.1724137931034483, "f": 0.16129032258064518, "fn": 24.0, "numImages": 1.0, "fp": 28.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.36363636363636365, "f": 0.32, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a hand holding a doughnut with a bite taken out of it. The doughnut appears to be covered in powdered sugar and has a glaze on top. The hand holding the doughnut is wearing a white glove. The background is a green grassy area with"}, "226805": {"image_id": 226805, "Bleu_1": 0.339622641503026, "Bleu_2": 0.2285817953234835, "Bleu_3": 0.12701286121266542, "Bleu_4": 0.08000981634230918, "METEOR": 0.25993561597170883, "ROUGE_L": 0.2959369314736203, "CIDEr": 3.073811560545086e-10, "SPICE": {"All": {"pr": 0.125, "re": 0.10714285714285714, "f": 0.11538461538461538, "fn": 25.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a large, open living room with high ceilings and large windows. There are several couches and chairs arranged around the room, and a large wooden table in the center. The walls are painted a light gray color and there are several large paintings on the walls. The room is well"}, "235221": {"image_id": 235221, "Bleu_1": 0.33333333332679743, "Bleu_2": 0.21602468994265056, "Bleu_3": 0.1418983411941997, "Bleu_4": 0.08783602619536428, "METEOR": 0.21577789317669688, "ROUGE_L": 0.3232644409114997, "CIDEr": 2.1624654863552782e-08, "SPICE": {"All": {"pr": 0.16, "re": 0.36363636363636365, "f": 0.2222222222222222, "fn": 7.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.6666666666666666, "f": 0.42105263157894735, "fn": 2.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a lightning storm in the background, with a clock tower in the foreground. The clock tower has a red light on top of it, and there are buildings in the background. The sky is dark and stormy, with lightning bolts striking the ground.\n\nThe image is taken from"}, "499402": {"image_id": 499402, "Bleu_1": 0.32142857142283165, "Bleu_2": 0.26482044884665296, "Bleu_3": 0.15735365791500988, "Bleu_4": 1.6466021000899253e-05, "METEOR": 0.2502306195349712, "ROUGE_L": 0.3396436525612472, "CIDEr": 1.4804693875385274e-13, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.15625, "f": 0.1639344262295082, "fn": 27.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.2222222222222222, "re": 0.14285714285714285, "f": 0.17391304347826086, "fn": 12.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.2727272727272727, "f": 0.24999999999999994, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a view of the wing of an airplane from the perspective of a passenger on board. The wing is painted in the colors of the airline, with the logo on the tail. The plane is flying over a mountain range, with snow covered peaks in the background. The sky is clear and blue"}, "539557": {"image_id": 539557, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.20159019488753482, "Bleu_3": 0.11680628450365418, "Bleu_4": 1.3361559690687091e-05, "METEOR": 0.21661188116039673, "ROUGE_L": 0.22195269860521533, "CIDEr": 1.3290839770285463e-12, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.29411764705882354, "f": 0.21276595744680848, "fn": 12.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.26666666666666666, "re": 0.5714285714285714, "f": 0.36363636363636365, "fn": 3.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "The image is of a large ship with a purple and white hull and a white mast. There are several birds flying overhead and a lighthouse in the background. The sky is a light blue color with some clouds. The image is taken from a distance, with the ship and lighthouse in the"}, "109976": {"image_id": 109976, "Bleu_1": 0.23999999999520005, "Bleu_2": 0.19794866371815809, "Bleu_3": 0.13479125657772084, "Bleu_4": 0.10103674020452826, "METEOR": 0.24356304795271164, "ROUGE_L": 0.22938079719227877, "CIDEr": 4.5814949584176023e-10, "SPICE": {"All": {"pr": 0.16, "re": 0.15384615384615385, "f": 0.1568627450980392, "fn": 22.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.1, "f": 0.14285714285714288, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.375, "f": 0.35294117647058826, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a kitchen with white cabinets and a white refrigerator. There is a stove and oven in the kitchen. The floor is made of tile and there is a microwave on the counter. The walls are painted white and there are no windows in the room.\n\nThe image"}, "210448": {"image_id": 210448, "Bleu_1": 0.29999999999400007, "Bleu_2": 0.20701966779852365, "Bleu_3": 0.12132137515765275, "Bleu_4": 0.07851064200963404, "METEOR": 0.24557751107985043, "ROUGE_L": 0.29647630619684084, "CIDEr": 2.850314144915882e-10, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.2222222222222222, "f": 0.19999999999999998, "fn": 14.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a group of zebras standing in a fenced enclosure. They are all looking at each other and appear to be in good health. The fence is made of wood and has a gate that is open. There are trees and bushes in the background.\n\nThe zebras are"}, "32724": {"image_id": 32724, "Bleu_1": 0.22916666666189242, "Bleu_2": 0.20948264539468747, "Bleu_3": 0.1562665723986159, "Bleu_4": 0.09596136927101263, "METEOR": 0.2686323539603848, "ROUGE_L": 0.2459677419354839, "CIDEr": 1.0704578948454252e-09, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.2222222222222222, "f": 0.24, "fn": 21.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.6, "re": 0.46153846153846156, "f": 0.5217391304347826, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 6.0}}, "caption": "The image shows a group of giraffes running across a grassy plain. They are running in a line, with their long necks and legs stretched out in front of them. The sky is blue and there are mountains in the distance.\n\nThe giraffes are brown with white spots"}, "277689": {"image_id": 277689, "Bleu_1": 0.3749999999921875, "Bleu_2": 0.17864740024886272, "Bleu_3": 0.1115377437347146, "Bleu_4": 0.07451835561525198, "METEOR": 0.24024820452032783, "ROUGE_L": 0.27477477477477474, "CIDEr": 5.419053790077257e-07, "SPICE": {"All": {"pr": 0.2916666666666667, "re": 0.28, "f": 0.2857142857142857, "fn": 18.0, "numImages": 1.0, "fp": 17.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.4, "re": 0.2222222222222222, "f": 0.2857142857142857, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.7142857142857143, "f": 0.5555555555555556, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a wedding cake on a table with white tablecloths and red strawberries on top. There are also glasses of champagne on the table. The background is a blue sky with palm trees in the distance.\n\nThe image is well lit and the colors are v"}, "167818": {"image_id": 167818, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.14002800840008178, "Bleu_3": 0.07319587495056674, "Bleu_4": 9.458362067959735e-06, "METEOR": 0.21545074255973604, "ROUGE_L": 0.24151583710407235, "CIDEr": 9.438408162881444e-11, "SPICE": {"All": {"pr": 0.34615384615384615, "re": 0.45, "f": 0.3913043478260869, "fn": 11.0, "numImages": 1.0, "fp": 17.0, "tp": 9.0}, "Relation": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.4, "re": 0.4, "f": 0.4000000000000001, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.36363636363636365, "re": 0.6666666666666666, "f": 0.4705882352941177, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "This is a desk with a computer, keyboard, mouse, and other computer accessories on it. There is a window behind the desk with a view of the city. The room is well lit and has a white wall with a window on it.\n\nThe desk is made of wood and has a"}, "445135": {"image_id": 445135, "Bleu_1": 0.3773584905589178, "Bleu_2": 0.29509782884468133, "Bleu_3": 0.21718893759224345, "Bleu_4": 0.15745849216150845, "METEOR": 0.3271225972880136, "ROUGE_L": 0.3779039752194115, "CIDEr": 8.17425382096457e-09, "SPICE": {"All": {"pr": 0.07407407407407407, "re": 0.14285714285714285, "f": 0.0975609756097561, "fn": 12.0, "numImages": 1.0, "fp": 25.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.4, "f": 0.23529411764705882, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a man playing tennis on a court with a net in the background. The man is wearing a black shirt and white shorts, and is holding a tennis racket in his right hand. The court is made of green grass and there are several other tennis players in the background."}, "3145": {"image_id": 3145, "Bleu_1": 0.30188679244713423, "Bleu_2": 0.18663625434394554, "Bleu_3": 0.1397957424660571, "Bleu_4": 0.08597614289216929, "METEOR": 0.2320122032729868, "ROUGE_L": 0.28355607205113303, "CIDEr": 1.565238151572012e-10, "SPICE": {"All": {"pr": 0.3125, "re": 0.30303030303030304, "f": 0.3076923076923077, "fn": 23.0, "numImages": 1.0, "fp": 22.0, "tp": 10.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3, "re": 0.3, "f": 0.3, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.6666666666666666, "re": 0.6666666666666666, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 2.0}, "Object": {"pr": 0.5833333333333334, "re": 0.5833333333333334, "f": 0.5833333333333334, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 7.0}}, "caption": "The image shows a living room with a couch, coffee table, and chairs. There is a window on the left side of the room with curtains open. The walls are painted a light green color and there is a ceiling fan in the center of the room. The floor is made of hardwood"}, "319127": {"image_id": 319127, "Bleu_1": 0.2448979591786756, "Bleu_2": 0.1889822365007167, "Bleu_3": 0.13160986848342301, "Bleu_4": 1.4920276910509015e-05, "METEOR": 0.2296433571005205, "ROUGE_L": 0.24190350297422336, "CIDEr": 1.2113419844685766e-10, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.16, "f": 0.17391304347826086, "fn": 21.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.36363636363636365, "f": 0.39999999999999997, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a snowy scene with trees and a park bench in the foreground\n\nThe trees are bare and there is a bench in the foreground. The sky is cloudy and there is snow on the ground.\n\nThe image is in black and white.\n\nThe image is taken"}, "380906": {"image_id": 380906, "Bleu_1": 0.26315789473222534, "Bleu_2": 0.1371021242743437, "Bleu_3": 6.991578304852386e-07, "Bleu_4": 1.586108817962657e-09, "METEOR": 0.1748157262111888, "ROUGE_L": 0.2314363143631436, "CIDEr": 9.023517250086152e-13, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.16666666666666666, "f": 0.17777777777777778, "fn": 20.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.125, "f": 0.18181818181818182, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.3333333333333333, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a bench on the beach with a view of the ocean in the background. The bench is made of wood and has a purple ribbon tied around it. The beach is covered in sand and there are some rocks in the distance. The sky is clear and there are some clouds in the distance"}, "524850": {"image_id": 524850, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.19897929585627183, "Bleu_3": 0.1581995558142908, "Bleu_4": 0.12477720076473013, "METEOR": 0.18596687354249103, "ROUGE_L": 0.24151583710407235, "CIDEr": 2.1624272794758549e-10, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.2631578947368421, "f": 0.20833333333333334, "fn": 14.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.625, "f": 0.45454545454545453, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows a large airplane with a group of people standing around it. The plane has a large engine on the front and a wing on the side. There are people standing on the tarmac and in the background, there are other airplanes and buildings. The sky is clear and blue."}, "85926": {"image_id": 85926, "Bleu_1": 0.2727272727223141, "Bleu_2": 0.14213381090113214, "Bleu_3": 0.07250583813278431, "Bleu_4": 9.252921909857738e-06, "METEOR": 0.209192146060414, "ROUGE_L": 0.24013046901360924, "CIDEr": 3.972388761954507e-12, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.2727272727272727, "f": 0.24999999999999994, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4, "f": 0.3636363636363636, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a brown bear and her cubs standing in a field with tall grass and wildflowers. The bear is looking down at her cubs while they are eating grass. The cubs are also looking at their mother. The image is taken in a beautiful landscape with a clear blue sky and green grass"}, "102355": {"image_id": 102355, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.1447907475849832, "Bleu_3": 7.387419460030929e-07, "Bleu_4": 1.6767836295274806e-09, "METEOR": 0.16971476026624566, "ROUGE_L": 0.20962199312714777, "CIDEr": 2.1471855293607683e-11, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.13043478260869565, "f": 0.13043478260869565, "fn": 20.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.375, "f": 0.3157894736842105, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a horse and cart on a dirt road in front of a house with a red roof. The horse is pulling the cart with a rope. There are people standing on the sidewalk and in the background, there are trees and a fence. The sky is blue and there are clouds."}, "47112": {"image_id": 47112, "Bleu_1": 0.29787234041919425, "Bleu_2": 0.19711142525707728, "Bleu_3": 0.09522194049088256, "Bleu_4": 1.1835581532782098e-05, "METEOR": 0.22056470243462453, "ROUGE_L": 0.23843648208469054, "CIDEr": 5.4361242542806536e-08, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.14285714285714285, "f": 0.14545454545454545, "fn": 24.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.07142857142857142, "f": 0.1, "fn": 13.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.3333333333333333, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a table with a pizza on it. The pizza has various toppings such as mushrooms, pepperoni, and olives. There are two glasses of wine on the table, one on the left and one on the right. The table has a white tablecloth and the"}, "215709": {"image_id": 215709, "Bleu_1": 0.1874999999960938, "Bleu_2": 0.12632278815731815, "Bleu_3": 0.10133876550919194, "Bleu_4": 0.08246811684519828, "METEOR": 0.1691128617097674, "ROUGE_L": 0.20497311827956988, "CIDEr": 5.650273021400897e-10, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.16666666666666666, "f": 0.15686274509803924, "fn": 20.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.1, "f": 0.10526315789473685, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a bathroom with a sink, toilet, and shower. The walls are made of wood and there are tiles on the floor. There is a wooden door and a window with curtains. The room is well lit and there are plants on the windowsill.\n\nThe bath"}, "512982": {"image_id": 512982, "Bleu_1": 0.21739130434310025, "Bleu_2": 0.12038585308312308, "Bleu_3": 0.06906098117642889, "Bleu_4": 9.355294087801786e-06, "METEOR": 0.16090938026745602, "ROUGE_L": 0.24238410596026488, "CIDEr": 7.346231842265579e-09, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.17857142857142858, "f": 0.18181818181818182, "fn": 23.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.1, "re": 0.08333333333333333, "f": 0.0909090909090909, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a bathroom with a sink, toilet, and shower. The sink is made of white porcelain and has a faucet on the right side. The toilet is covered in white tile and has a toilet seat on top. The shower is made of glass"}, "344633": {"image_id": 344633, "Bleu_1": 0.1702127659538253, "Bleu_2": 1.9236105428737107e-09, "Bleu_3": 4.348511215471945e-12, "Bleu_4": 2.0791808991436597e-13, "METEOR": 0.2249711104130006, "ROUGE_L": 0.1667805878332194, "CIDEr": 9.398669238284606e-10, "SPICE": {"All": {"pr": 0.12121212121212122, "re": 0.15384615384615385, "f": 0.13559322033898305, "fn": 22.0, "numImages": 1.0, "fp": 29.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.3333333333333333, "f": 0.30769230769230765, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows two horses in a corral, one of which is being ridden by a person wearing a helmet and riding boots. The other horse is standing nearby, looking at the person riding it. The person is wearing a riding helmet and riding boots, and the"}, "555942": {"image_id": 555942, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.16803361008003348, "Bleu_3": 0.13209470492933909, "Bleu_4": 0.10955546868940362, "METEOR": 0.23722638839035562, "ROUGE_L": 0.29847094801223245, "CIDEr": 2.5631021590390502e-11, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.16666666666666666, "f": 0.16, "fn": 20.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 0.25, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a green motorcycle parked on the sidewalk next to a row of parked cars. The motorcycle has a black and white striped seat and a black and white striped gas tank. The motorcycle has a black and white striped front fender and a black and white striped rear"}, "471567": {"image_id": 471567, "Bleu_1": 0.27083333332769105, "Bleu_2": 0.16974114626800244, "Bleu_3": 8.556026937263809e-07, "Bleu_4": 1.931527870007565e-09, "METEOR": 0.1676919728270867, "ROUGE_L": 0.27371794871794874, "CIDEr": 3.461941035654583e-09, "SPICE": {"All": {"pr": 0.25, "re": 0.4117647058823529, "f": 0.3111111111111111, "fn": 10.0, "numImages": 1.0, "fp": 21.0, "tp": 7.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.125, "f": 0.1, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 6.0, "tp": 6.0}}, "caption": "The giraffe is standing on its hind legs, looking up at the camera. It has a long neck and spotted fur. The background is a wooden fence with trees in the distance.\n\nThe giraffe is standing on its hind legs, looking up at the camera. It has a"}, "24260": {"image_id": 24260, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.1027524665455226, "Bleu_3": 0.059548592709689166, "Bleu_4": 8.102230734220573e-06, "METEOR": 0.14072938150197783, "ROUGE_L": 0.18780788177339902, "CIDEr": 6.069649909630834e-07, "SPICE": {"All": {"pr": 0.07142857142857142, "re": 0.10526315789473684, "f": 0.0851063829787234, "fn": 17.0, "numImages": 1.0, "fp": 26.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.07142857142857142, "re": 0.16666666666666666, "f": 0.1, "fn": 5.0, "numImages": 1.0, "fp": 13.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.14285714285714285, "re": 1.0, "f": 0.25, "fn": 0.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Object": {"pr": 0.125, "re": 0.125, "f": 0.125, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}}, "caption": "The image shows two horses in a field, one of which is jumping over a fence\n\nThe horses are brown and white with long manes and tails. The one on the left is jumping over a fence made of wood and has a white stripe on its back. The one on the"}, "106508": {"image_id": 106508, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.11215443081631235, "Bleu_3": 0.06230790884875894, "Bleu_4": 8.29879120277201e-06, "METEOR": 0.19554291567800058, "ROUGE_L": 0.1783625730994152, "CIDEr": 1.3432093339358897e-11, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.20833333333333334, "f": 0.19607843137254902, "fn": 19.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.625, "f": 0.5263157894736842, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a bus stop with a bus and a police officer standing on the sidewalk. The bus stop has a sign that says \"Bus Stop\" and the officer is wearing a yellow vest and holding a baton. There are trees and buildings in the background.\n\nThe image is taken from a bird"}, "311082": {"image_id": 311082, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.1927248223279925, "Bleu_3": 0.14573527583425525, "Bleu_4": 0.1071288763939954, "METEOR": 0.235755558672015, "ROUGE_L": 0.29647630619684084, "CIDEr": 3.152718889523594e-10, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.14285714285714285, "f": 0.14814814814814817, "fn": 24.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.1, "re": 0.1, "f": 0.10000000000000002, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2727272727272727, "f": 0.2608695652173913, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows two elephants standing in a field with trees in the background. The elephants are standing next to each other and appear to be interacting with each other. The elephants are brown in color and have large ears. The image is taken from a bird's eye view."}, "312167": {"image_id": 312167, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.1628822035823003, "Bleu_3": 0.08206702275709715, "Bleu_4": 1.041362763232212e-05, "METEOR": 0.2125359297406661, "ROUGE_L": 0.22938079719227877, "CIDEr": 2.8064552942642366e-10, "SPICE": {"All": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 14.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.125, "f": 0.16666666666666666, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.5555555555555556, "f": 0.5, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image is a still life of a vase with pink flowers in it. The vase is sitting on a table with a red background. The flowers are pink and have long, thin stems with small, delicate petals. The vase is made of clear glass and has a round shape."}, "324937": {"image_id": 324937, "Bleu_1": 0.2181818181778513, "Bleu_2": 0.12712834523041283, "Bleu_3": 0.06730845772977347, "Bleu_4": 8.750873255009596e-06, "METEOR": 0.18917282278037953, "ROUGE_L": 0.24013046901360924, "CIDEr": 9.284219095681243e-12, "SPICE": {"All": {"pr": 0.25, "re": 0.2692307692307692, "f": 0.2592592592592593, "fn": 19.0, "numImages": 1.0, "fp": 21.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.25, "f": 0.18181818181818182, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5454545454545454, "f": 0.5217391304347826, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 6.0}}, "caption": "The image shows a white and brown dog lying on a couch. The dog is looking up at the camera with its tongue hanging out of its mouth. The couch is covered in a patterned fabric and has a pillow on it. The room is dimly lit and there are some books and a lamp"}, "65415": {"image_id": 65415, "Bleu_1": 0.2826086956460303, "Bleu_2": 0.17720317768650332, "Bleu_3": 0.08936417387415972, "Bleu_4": 1.1350251107826057e-05, "METEOR": 0.23404395583817075, "ROUGE_L": 0.2121001390820584, "CIDEr": 4.365823337658807e-09, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.19047619047619047, "f": 0.1702127659574468, "fn": 17.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.16666666666666666, "f": 0.14285714285714288, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a woman in a red jacket and black pants standing on skis in the snow. She is holding a pair of skis and wearing ski goggles. There is a sign in the background that reads, \"Skiing Lessons\".\n\nThe woman is standing on a"}, "201925": {"image_id": 201925, "Bleu_1": 0.17999999999640004, "Bleu_2": 0.12121830534381621, "Bleu_3": 0.08491317075202563, "Bleu_4": 0.06007680516395326, "METEOR": 0.16401215084679377, "ROUGE_L": 0.25386444708680145, "CIDEr": 1.1627372084746213e-09, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.25, "f": 0.22641509433962265, "fn": 18.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.2857142857142857, "f": 0.2105263157894737, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.4, "re": 0.36363636363636365, "f": 0.380952380952381, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a cake in a black pan on a stove. The cake appears to be baked and has a golden brown color. There are no visible ingredients or decorations on the cake. The stove appears to be a gas stove with a burner on the right side and"}, "273132": {"image_id": 273132, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.20159019488753482, "Bleu_3": 0.15853051819285777, "Bleu_4": 0.13361559690687091, "METEOR": 0.27054704318238454, "ROUGE_L": 0.26472411655300687, "CIDEr": 2.2060637830510356e-11, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.14814814814814814, "f": 0.14035087719298248, "fn": 23.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "This is a black and white photograph of an elderly woman sitting on a bench in a park. She is wearing a hat and a jacket, and has a cigarette in her hand. The bench is made of wood and has a metal frame. The park is surrounded by trees and there are"}, "475238": {"image_id": 475238, "Bleu_1": 0.2749999999931251, "Bleu_2": 0.20568833779665252, "Bleu_3": 1.0364425802784488e-06, "Bleu_4": 2.3421165563772675e-09, "METEOR": 0.23391905598532511, "ROUGE_L": 0.23680124223602486, "CIDEr": 3.1639818969689533e-06, "SPICE": {"All": {"pr": 0.2631578947368421, "re": 0.2, "f": 0.22727272727272727, "fn": 20.0, "numImages": 1.0, "fp": 14.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.1, "f": 0.14285714285714288, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.4444444444444444, "f": 0.47058823529411764, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 4.0}}, "caption": "The image shows a train traveling down the tracks on a sunny day. The train is pulling into the station and the people on the platform are waving at it. There are also some trees and buildings in the background."}, "130527": {"image_id": 130527, "Bleu_1": 0.1964285714250638, "Bleu_2": 0.11952286093128565, "Bleu_3": 0.0641952261839719, "Bleu_4": 8.405394134956185e-06, "METEOR": 0.15702363636019706, "ROUGE_L": 0.1769141531322506, "CIDEr": 4.0501351597477807e-13, "SPICE": {"All": {"pr": 0.28, "re": 0.4117647058823529, "f": 0.3333333333333333, "fn": 10.0, "numImages": 1.0, "fp": 18.0, "tp": 7.0}, "Relation": {"pr": 0.1, "re": 0.125, "f": 0.11111111111111112, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.5, "f": 0.28571428571428575, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.7142857142857143, "f": 0.588235294117647, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "The image shows a green field with cows grazing in it. The sky is blue and there are mountains in the background.\n\nThe image is taken from the perspective of a person looking out at the field from a car window. The car is parked on the side of the road and there is a f"}, "337563": {"image_id": 337563, "Bleu_1": 0.20833333332899312, "Bleu_2": 0.14887283354071895, "Bleu_3": 0.09877211597249048, "Bleu_4": 0.0680256405311088, "METEOR": 0.2170088968032359, "ROUGE_L": 0.20497311827956988, "CIDEr": 1.6336727128404928e-09, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.23809523809523808, "f": 0.22222222222222224, "fn": 16.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.5, "f": 0.45454545454545453, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a woman sitting on a bed with her daughter sitting next to her. The woman is holding a banana in her hand and the daughter is holding a toy banana in her hand. The room is dimly lit and there are curtains on the windows."}, "135356": {"image_id": 135356, "Bleu_1": 0.1904761904716554, "Bleu_2": 0.13631963531491414, "Bleu_3": 0.07744955853581141, "Bleu_4": 1.0447155481232024e-05, "METEOR": 0.21663221757062306, "ROUGE_L": 0.29383429672447015, "CIDEr": 3.2917339613684824e-07, "SPICE": {"All": {"pr": 0.26666666666666666, "re": 0.47058823529411764, "f": 0.3404255319148936, "fn": 9.0, "numImages": 1.0, "fp": 22.0, "tp": 8.0}, "Relation": {"pr": 0.2, "re": 0.6666666666666666, "f": 0.30769230769230765, "fn": 1.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.4, "re": 0.25, "f": 0.3076923076923077, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.26666666666666666, "re": 0.6666666666666666, "f": 0.3809523809523809, "fn": 2.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "The image shows a man standing in a kitchen, holding a sink faucet in his hand. He is wearing a plaid shirt and jeans. The kitchen has a stainless steel refrigerator, a stove, and a dishwasher. There is a window on the"}, "290078": {"image_id": 290078, "Bleu_1": 0.24999999999553577, "Bleu_2": 0.19069251784568234, "Bleu_3": 0.1264149004545122, "Bleu_4": 1.3972680638015505e-05, "METEOR": 0.21569125580631157, "ROUGE_L": 0.27062999112688557, "CIDEr": 2.47929712821166e-13, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.12, "f": 0.11764705882352941, "fn": 22.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a toilet sitting on the sidewalk in front of a building. The toilet is white and has a small hole in the bottom for waste to go through. There is no one in the image. The building is a two story brick structure with a red door and windows on the top floor"}, "578314": {"image_id": 578314, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.1927248223279925, "Bleu_3": 0.11567016508408626, "Bleu_4": 0.07575155495908963, "METEOR": 0.24014424545062246, "ROUGE_L": 0.22938079719227877, "CIDEr": 1.9015736416468855e-10, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.14285714285714285, "f": 0.14545454545454545, "fn": 24.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a toilet in a bathroom with a sink and a mirror on the wall\n\nThe toilet is white and has a seat and lid. The sink is made of stainless steel and has a faucet and a drain. The mirror is made of glass and has a"}, "174893": {"image_id": 174893, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.22893427324353557, "Bleu_3": 0.1917956598705532, "Bleu_4": 0.14273052102550982, "METEOR": 0.28703092744266495, "ROUGE_L": 0.3210526315789473, "CIDEr": 4.363776667848141e-12, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.3157894736842105, "f": 0.26666666666666666, "fn": 13.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.75, "f": 0.5454545454545454, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The image shows a young girl sitting at a table with a pair of scissors in her hand. She is cutting up pieces of yellow paper into small squares. The table is made of wood and has a white surface. The girl is wearing a pink shirt and has long blonde hair. The background"}, "539310": {"image_id": 539310, "Bleu_1": 0.38888888888168727, "Bleu_2": 0.2569780843702196, "Bleu_3": 0.13643933968476482, "Bleu_4": 1.493867787690423e-05, "METEOR": 0.29766247826768216, "ROUGE_L": 0.2738496071829405, "CIDEr": 6.73309466652849e-10, "SPICE": {"All": {"pr": 0.16, "re": 0.18181818181818182, "f": 0.1702127659574468, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.42857142857142855, "f": 0.3, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a view of a street with cars parked on both sides. There are buildings in the background and a traffic light at the intersection. The sky is cloudy and there are trees in the foreground.\n\nThe image is taken from the perspective of someone driving a car. The car is parked"}, "49740": {"image_id": 49740, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.11155016321612385, "Bleu_3": 0.07925015807505881, "Bleu_4": 0.05645482725503897, "METEOR": 0.19992153268497928, "ROUGE_L": 0.22536945812807885, "CIDEr": 2.257736523842403e-11, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.24, "f": 0.20689655172413793, "fn": 19.0, "numImages": 1.0, "fp": 27.0, "tp": 6.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.09090909090909091, "f": 0.09090909090909091, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1, "re": 0.25, "f": 0.14285714285714288, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4, "f": 0.3636363636363636, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a baseball player swinging a bat at a ball that is being thrown by a pitcher. The player is wearing a red and white uniform with the number 13 on the back. The pitcher is wearing a black and white uniform with the number 22 on the back. The"}, "274549": {"image_id": 274549, "Bleu_1": 0.38461538460798816, "Bleu_2": 0.19418390934138346, "Bleu_3": 0.11468206134015693, "Bleu_4": 0.07448564757962442, "METEOR": 0.2463625764011994, "ROUGE_L": 0.2167219327333018, "CIDEr": 2.0969399527623271e-10, "SPICE": {"All": {"pr": 0.2, "re": 0.14814814814814814, "f": 0.1702127659574468, "fn": 23.0, "numImages": 1.0, "fp": 16.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 0.3333333333333333, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.125, "re": 0.09090909090909091, "f": 0.10526315789473685, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.3, "f": 0.33333333333333326, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a person skiing down a snowy slope. The person is wearing a black and orange jacket, black pants, and black ski boots. They are holding a black ski pole in their right hand and have a backpack on their back. The snow is covered in trees and there is"}, "537211": {"image_id": 537211, "Bleu_1": 0.18867924527945892, "Bleu_2": 0.1703748109207509, "Bleu_3": 0.1505903985438184, "Bleu_4": 0.1285643204879068, "METEOR": 0.354719213679156, "ROUGE_L": 0.3620178041543027, "CIDEr": 1.3106514569016409e-11, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.21052631578947367, "f": 0.186046511627907, "fn": 15.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4, "f": 0.380952380952381, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a man sitting on a wooden dock eating a hot dog. The man is wearing a black shirt and black pants. The dock is made of wooden planks and has a railing around it. In the background, there are tall buildings and a body of water. The sky is clear"}, "533743": {"image_id": 533743, "Bleu_1": 0.19999999999636367, "Bleu_2": 0.12171612388780342, "Bleu_3": 0.06538429607726681, "Bleu_4": 8.562572337389154e-06, "METEOR": 0.20217757452499455, "ROUGE_L": 0.24110671936758893, "CIDEr": 2.3065150095038194e-13, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.3333333333333333, "f": 0.27272727272727276, "fn": 12.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.25, "re": 0.2857142857142857, "f": 0.26666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.5, "f": 0.4444444444444445, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a man playing a video game on a television in the background. The man is wearing a black shirt and jeans, and has a white shirt on his head. There is a couch in the background with a table in front of it. The room appears to be a living room with"}, "81812": {"image_id": 81812, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.20957473279408317, "Bleu_3": 0.16487339804051732, "Bleu_4": 0.13901690971832786, "METEOR": 0.28143743486290607, "ROUGE_L": 0.3051907442151345, "CIDEr": 2.02621675058997e-11, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.23809523809523808, "f": 0.21276595744680848, "fn": 16.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.5, "f": 0.47619047619047616, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a group of people sitting around a table in a restaurant. They are all smiling and laughing as they enjoy their meals. The table is set with white tablecloths and red and white checkered napkins. There are several bottles of wine on the table, as well as a"}, "59743": {"image_id": 59743, "Bleu_1": 0.20408163264889634, "Bleu_2": 0.13041013273663615, "Bleu_3": 0.071259308775086, "Bleu_4": 9.417632649770063e-06, "METEOR": 0.19011727841684598, "ROUGE_L": 0.24190350297422336, "CIDEr": 9.266841924645172e-11, "SPICE": {"All": {"pr": 0.06896551724137931, "re": 0.08, "f": 0.07407407407407408, "fn": 23.0, "numImages": 1.0, "fp": 27.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.18181818181818182, "f": 0.17391304347826086, "fn": 9.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a group of people standing on the beach, holding surfboards and wetsuits. They are standing in the water, with the waves crashing against the shore. The sky is overcast, with clouds covering the sun. The beach is sandy, with some rocks and shells scattered along the"}, "356131": {"image_id": 356131, "Bleu_1": 0.3090909090852893, "Bleu_2": 0.20016827936980822, "Bleu_3": 0.09109716901600831, "Bleu_4": 1.0980644330046064e-05, "METEOR": 0.1844882949885357, "ROUGE_L": 0.24358243011979464, "CIDEr": 1.3506798435009486e-07, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.14814814814814814, "f": 0.14545454545454545, "fn": 23.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a small boat with two people in it, sailing on a calm body of water. The sky is a pale blue color with some clouds in it. The water is a light blue color with some ripples on it. The boat is a light brown color with some white sails on it."}, "311310": {"image_id": 311310, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.1593710424504603, "Bleu_3": 0.07926525909642891, "Bleu_4": 9.990095999341766e-06, "METEOR": 0.1327984916125616, "ROUGE_L": 0.24007646463510626, "CIDEr": 1.3943985352368655e-11, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.19047619047619047, "f": 0.19047619047619047, "fn": 17.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.375, "f": 0.375, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a group of people playing with kites in a park. The kites are made of colorful fabric and have tails that are streaming behind them. The people are standing on the grass and looking up at the kites. There are trees in the background and a sky with clouds.\n\nThe"}, "165257": {"image_id": 165257, "Bleu_1": 0.2444444444390124, "Bleu_2": 0.14907119849663567, "Bleu_3": 0.08024900879514213, "Bleu_4": 1.0532159683631698e-05, "METEOR": 0.22172496395438257, "ROUGE_L": 0.32084155161078237, "CIDEr": 4.7808187326412885e-08, "SPICE": {"All": {"pr": 0.1, "re": 0.1111111111111111, "f": 0.10526315789473685, "fn": 24.0, "numImages": 1.0, "fp": 27.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "This is a kitchen with wooden floors and cabinets. There is a sink and stove in the corner, and a refrigerator on the wall. The countertops are made of black granite. The walls are painted a light color. There are no windows in the room."}, "202658": {"image_id": 202658, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.15861031714066393, "Bleu_3": 0.11322098426538772, "Bleu_4": 0.07303863538757409, "METEOR": 0.19025249696479535, "ROUGE_L": 0.27949599083619703, "CIDEr": 2.219731361256109e-12, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.3333333333333333, "f": 0.25641025641025644, "fn": 10.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.25, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 1.0, "f": 0.5714285714285715, "fn": 0.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a toilet with a pink seat and a white tank. The toilet is in a small room with a wooden floor and a metal door. There is a small table with a white bucket on it in the corner of the room. The bucket has a pink ribbon tied around it"}, "50926": {"image_id": 50926, "Bleu_1": 0.26315789473222534, "Bleu_2": 0.20565318641151553, "Bleu_3": 0.1566604610064578, "Bleu_4": 0.09185887244314798, "METEOR": 0.23214701409937613, "ROUGE_L": 0.24413950829045164, "CIDEr": 1.6113115365264685e-14, "SPICE": {"All": {"pr": 0.20588235294117646, "re": 0.2692307692307692, "f": 0.23333333333333334, "fn": 19.0, "numImages": 1.0, "fp": 27.0, "tp": 7.0}, "Relation": {"pr": 0.07692307692307693, "re": 0.09090909090909091, "f": 0.08333333333333334, "fn": 10.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.5, "f": 0.41666666666666663, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows a group of people standing in a field with a large, white building in the background. There are several people standing in the field, including a man and a woman. The man is holding a red kite and the woman is holding a blue kite. There are also several people standing on the side"}, "514797": {"image_id": 514797, "Bleu_1": 0.2727272727223141, "Bleu_2": 0.14213381090113214, "Bleu_3": 7.250583813278434e-07, "Bleu_4": 1.645428051499804e-09, "METEOR": 0.16549719069886532, "ROUGE_L": 0.21095100864553315, "CIDEr": 2.3201502985806133e-13, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 28.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.08333333333333333, "f": 0.125, "fn": 11.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of people standing on a pier, looking up at a kite flying in the sky. The kite has a blue and white design and is being held by a person in the foreground. There are other people in the background, some of whom are also holding kites. The sky is"}, "258021": {"image_id": 258021, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.1313579154832357, "Bleu_3": 0.07061638347858736, "Bleu_4": 9.254837436221288e-06, "METEOR": 0.15192865914571357, "ROUGE_L": 0.22584228063680117, "CIDEr": 2.732472184654206e-11, "SPICE": {"All": {"pr": 0.04, "re": 0.05263157894736842, "f": 0.04545454545454545, "fn": 18.0, "numImages": 1.0, "fp": 24.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.08333333333333333, "re": 0.14285714285714285, "f": 0.10526315789473685, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}}, "caption": "The image shows a man standing on the sidewalk in front of a building. He is wearing a black shirt and pants, and has a helmet on his head. There is a bicycle parked next to him. The building has a large window on the top floor, and there are trees"}, "203085": {"image_id": 203085, "Bleu_1": 0.29999999999400007, "Bleu_2": 0.20701966779852365, "Bleu_3": 0.12132137515765275, "Bleu_4": 0.07851064200963404, "METEOR": 0.2334494433305158, "ROUGE_L": 0.2852466682253917, "CIDEr": 8.504024886925182e-09, "SPICE": {"All": {"pr": 0.125, "re": 0.09090909090909091, "f": 0.10526315789473685, "fn": 20.0, "numImages": 1.0, "fp": 14.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2857142857142857, "f": 0.26666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows a man sitting at a desk in a small room. He is wearing a white shirt and black pants. There is a computer monitor on the desk in front of him. The room is cluttered with various items such as a lamp, a bookshelf, and a chair"}, "441442": {"image_id": 441442, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.11651034560483009, "Bleu_3": 6.475197095245912e-07, "Bleu_4": 1.5342308957715506e-09, "METEOR": 0.1786941012443853, "ROUGE_L": 0.2610024449877751, "CIDEr": 1.6527145585771292e-11, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.125, "f": 0.13559322033898305, "fn": 28.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.3333333333333333, "f": 0.32, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a horse and rider in a field with a fence in the background. The horse is brown and has a saddle on its back. The rider is wearing a helmet and riding boots. The image is taken from a distance and shows the horse and rider in a field"}, "494759": {"image_id": 494759, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.19321055028292738, "Bleu_3": 0.16484697178623603, "Bleu_4": 0.1286891771270242, "METEOR": 0.2542714764464475, "ROUGE_L": 0.3286862812179268, "CIDEr": 2.6875221871273222e-11, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.20833333333333334, "f": 0.20833333333333334, "fn": 19.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4444444444444444, "f": 0.3636363636363637, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows two people standing on a beach with a kite in the sky. The sky is cloudy and there are some clouds in the background. The people are wearing sunglasses and are looking up at the kite. The kite is made of lightweight material and has a long tail that"}, "336937": {"image_id": 336937, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.10079509744376741, "Bleu_3": 5.840314225182713e-07, "Bleu_4": 1.4128129251678322e-09, "METEOR": 0.12473809759784717, "ROUGE_L": 0.21721068249258166, "CIDEr": 1.5457013217237687e-12, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.16666666666666666, "f": 0.17391304347826086, "fn": 20.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5714285714285714, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "This image appears to be a large group of white ceramic sinks arranged in rows. Each sink has a faucet on the top and a drain on the bottom. The sinks are arranged in rows, with some of them facing each other and others facing away from each other. There are no other"}, "157155": {"image_id": 157155, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.09712858623387621, "Bleu_3": 5.697810303144272e-07, "Bleu_4": 1.3868787330575517e-09, "METEOR": 0.1622755161540398, "ROUGE_L": 0.21356050881082975, "CIDEr": 3.997005601214855e-10, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.19047619047619047, "f": 0.17777777777777778, "fn": 17.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5, "f": 0.4210526315789474, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a group of people standing on the sidewalk in front of a building. They are all wearing different colored clothing and hats. There is a horse and carriage in the middle of the street. The horse is wearing a red and white striped blanket and the carriage is decorated with"}, "243600": {"image_id": 243600, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.20550493309104514, "Bleu_3": 0.11909718541937828, "Bleu_4": 0.07662616731096747, "METEOR": 0.25348906960651124, "ROUGE_L": 0.25722891566265055, "CIDEr": 1.0456772731334458e-10, "SPICE": {"All": {"pr": 0.2727272727272727, "re": 0.2222222222222222, "f": 0.24489795918367346, "fn": 21.0, "numImages": 1.0, "fp": 16.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.125, "f": 0.15384615384615385, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.45454545454545453, "f": 0.47619047619047616, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "This is a black and white photograph of a library with bookshelves and tables. There are several people sitting at tables, reading books. The walls are painted white and there are windows on the sides of the room.\n\nThe image is taken from a book or magazine.\n\nThe people in the image"}, "542147": {"image_id": 542147, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.14254579382569355, "Bleu_3": 0.07358334830308164, "Bleu_4": 9.448049464513672e-06, "METEOR": 0.1312854621387321, "ROUGE_L": 0.18100890207715134, "CIDEr": 2.596862966052526e-12, "SPICE": {"All": {"pr": 0.2727272727272727, "re": 0.47368421052631576, "f": 0.34615384615384615, "fn": 10.0, "numImages": 1.0, "fp": 24.0, "tp": 9.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.14285714285714285, "f": 0.1111111111111111, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.375, "re": 0.5, "f": 0.42857142857142855, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.35714285714285715, "re": 0.8333333333333334, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows a large, modern building with a large parking lot in front of it. There are several cars parked in the lot, and a few people are standing near the building. The building appears to be under construction, with scaffolding and construction equipment visible on the roof. The grass is green"}, "277227": {"image_id": 277227, "Bleu_1": 0.19999999999636367, "Bleu_2": 0.06085806194390174, "Bleu_3": 4.1189525480153644e-07, "Bleu_4": 1.0766864701111474e-09, "METEOR": 0.13839947443726, "ROUGE_L": 0.14361389052383755, "CIDEr": 1.1088562283862383e-13, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.2727272727272727, "f": 0.23999999999999996, "fn": 16.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.2222222222222222, "f": 0.25, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5, "f": 0.380952380952381, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a small boat docked at a pier on the water. The boat is white and has a blue stripe on the side. There are several other boats docked at the pier, some of which are also white and have blue stripes. The pier is made of wood and has a railing along"}, "323552": {"image_id": 323552, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.1565560727682472, "Bleu_3": 0.09934208621394808, "Bleu_4": 1.1893260654297589e-05, "METEOR": 0.23444927937155974, "ROUGE_L": 0.24653579676674361, "CIDEr": 1.9699354015350277e-09, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.25, "f": 0.1904761904761905, "fn": 12.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.5, "f": 0.3529411764705882, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The woman is sitting on the floor next to a suitcase. She is wearing a blue shirt and black pants. She has a backpack on her back and is holding a small bag in her hand. The airport is in the background.\n\nThe woman is sitting on the floor next to a"}, "319607": {"image_id": 319607, "Bleu_1": 0.3454545454482645, "Bleu_2": 0.22622654834606154, "Bleu_3": 0.0988409906791752, "Bleu_4": 1.1673524021794383e-05, "METEOR": 0.19009810924666026, "ROUGE_L": 0.3015323776569451, "CIDEr": 9.551195311977316e-10, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.15789473684210525, "f": 0.14285714285714288, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2857142857142857, "f": 0.30769230769230765, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.1111111111111111, "re": 0.125, "f": 0.11764705882352941, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}}, "caption": "The image shows a street with a red light at the corner. There are people walking on the sidewalk and bicycles parked on the side of the road. The building on the left has a large window with a sign that reads, \"The Building\". The building on the right has a large window with a"}, "581702": {"image_id": 581702, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.24468219368158126, "Bleu_3": 0.15214298632918113, "Bleu_4": 0.10894415888761377, "METEOR": 0.2711153745623451, "ROUGE_L": 0.32383921799949444, "CIDEr": 1.9242813675411528e-11, "SPICE": {"All": {"pr": 0.08695652173913043, "re": 0.09090909090909091, "f": 0.08888888888888888, "fn": 20.0, "numImages": 1.0, "fp": 21.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2222222222222222, "f": 0.1904761904761905, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image is of a bird perched on a rock in a garden. The bird has a red head and black and white body. The bird is looking down at something on the ground. The background is a rocky terrain with some greenery.\n\nThe bird is perched on a rock in a garden."}, "328818": {"image_id": 328818, "Bleu_1": 0.22448979591378598, "Bleu_2": 0.19342948582067712, "Bleu_3": 0.15847928398352254, "Bleu_4": 0.12693173928949333, "METEOR": 0.2221910743304184, "ROUGE_L": 0.3175245806824754, "CIDEr": 1.0486559990215499e-08, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.14285714285714285, "f": 0.10526315789473685, "fn": 12.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.3333333333333333, "f": 0.26666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The woman is sitting on a bench next to a bike rack. She is wearing a pink shirt and blue jeans. She is holding a bike helmet in her hand and appears to be preparing to ride her bike. The bench is made of wood and has a metal"}, "55981": {"image_id": 55981, "Bleu_1": 0.22448979591378598, "Bleu_2": 0.15291943545714587, "Bleu_3": 0.07923964199176128, "Bleu_4": 1.0198056661125237e-05, "METEOR": 0.18310015490488404, "ROUGE_L": 0.22679390259015986, "CIDEr": 1.7671615034939447e-09, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 20.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.2, "f": 0.12500000000000003, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.06666666666666667, "f": 0.08695652173913045, "fn": 14.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 1.0, "f": 0.4, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.2727272727272727, "re": 0.6, "f": 0.37499999999999994, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a young girl standing in front of a staircase, holding a suitcase. She is wearing a pink sweater and blue jeans. The staircase appears to be made of wood and has a railing on either side. There is a blue carpet on the floor leading up"}, "385918": {"image_id": 385918, "Bleu_1": 0.3541666666592882, "Bleu_2": 0.2455273669893519, "Bleu_3": 0.15782934179488486, "Bleu_4": 0.09668023125951691, "METEOR": 0.19925140464245977, "ROUGE_L": 0.2401574803149606, "CIDEr": 9.182746038294985e-09, "SPICE": {"All": {"pr": 0.07142857142857142, "re": 0.1111111111111111, "f": 0.08695652173913043, "fn": 16.0, "numImages": 1.0, "fp": 26.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2857142857142857, "f": 0.2105263157894737, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a group of people playing frisbee on a field. They are wearing black and white shirts and black pants. One person is throwing the frisbee while the others are running to catch it. The field is green and there are trees in the background.\n\nThe"}, "86625": {"image_id": 86625, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.20195126571496114, "Bleu_3": 0.16085047993120216, "Bleu_4": 0.13646505647648105, "METEOR": 0.28143743486290607, "ROUGE_L": 0.3051907442151345, "CIDEr": 5.376132802773495e-11, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.13636363636363635, "f": 0.1395348837209302, "fn": 19.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.1, "f": 0.14285714285714288, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2857142857142857, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a man standing in front of a refrigerator with a carton of eggs on the counter. The man is wearing a black hoodie and has a look of concentration on his face. The refrigerator has a white door and a black handle. The counter has a white top"}, "87429": {"image_id": 87429, "Bleu_1": 0.19999999999636367, "Bleu_2": 0.08606629658080774, "Bleu_3": 0.05189555018762678, "Bleu_4": 7.200236383861159e-06, "METEOR": 0.1357666410437518, "ROUGE_L": 0.22297650130548302, "CIDEr": 1.8157025572297896e-11, "SPICE": {"All": {"pr": 0.08695652173913043, "re": 0.10526315789473684, "f": 0.09523809523809525, "fn": 17.0, "numImages": 1.0, "fp": 21.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a large building with several cars parked in front of it. There are several trees in the background and a cloudy sky in the distance.\n\nThe building appears to be made of brick and has several windows on each floor. There are several cars parked in front of the building, including a"}, "330091": {"image_id": 330091, "Bleu_1": 0.43749999999088546, "Bleu_2": 0.28944186936439076, "Bleu_3": 0.1221204208765233, "Bleu_4": 1.4183652772678572e-05, "METEOR": 0.27513678473608766, "ROUGE_L": 0.37014563106796117, "CIDEr": 3.765842722860308e-05, "SPICE": {"All": {"pr": 0.32, "re": 0.2857142857142857, "f": 0.30188679245283023, "fn": 20.0, "numImages": 1.0, "fp": 17.0, "tp": 8.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.1111111111111111, "f": 0.125, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.46153846153846156, "f": 0.4444444444444445, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The image shows a person skiing down a snowy slope on a snowboard. The person is wearing a red and black jacket, black pants, and black ski goggles. The snow is covered in trees and there is a mountain in the background.\n\nThe image is in black and"}, "491851": {"image_id": 491851, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.16568337391262167, "Bleu_3": 8.243669902025871e-07, "Bleu_4": 1.8483326514023873e-09, "METEOR": 0.20331617111329478, "ROUGE_L": 0.2544392801811465, "CIDEr": 6.2680533818151e-10, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.1111111111111111, "f": 0.09523809523809525, "fn": 16.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2857142857142857, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a desk with a computer, keyboard, mouse, and speakers. There is also a chair and a lamp on the desk. The walls are painted a light blue color and there are some shelves on the wall with books and other items.\n\nThe room is well lit and there"}, "203661": {"image_id": 203661, "Bleu_1": 0.22916666666189242, "Bleu_2": 0.13965509692979164, "Bleu_3": 0.10834918977671004, "Bleu_4": 0.07291487281610555, "METEOR": 0.21154886865581646, "ROUGE_L": 0.28018372703412076, "CIDEr": 6.51179193299873e-10, "SPICE": {"All": {"pr": 0.3333333333333333, "re": 0.27586206896551724, "f": 0.3018867924528302, "fn": 21.0, "numImages": 1.0, "fp": 16.0, "tp": 8.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.09090909090909091, "f": 0.09999999999999999, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.6666666666666666, "re": 0.3333333333333333, "f": 0.4444444444444444, "fn": 4.0, "numImages": 1.0, "fp": 1.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.4166666666666667, "re": 0.4166666666666667, "f": 0.4166666666666667, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a small kitchen with a wooden table and chairs. There is a stove and oven on the countertop, as well as a sink and refrigerator. The walls are painted white and there are some potted plants on the windowsill. The floor is made of hardwood."}, "150875": {"image_id": 150875, "Bleu_1": 0.19230769230399414, "Bleu_2": 0.10635890745081387, "Bleu_3": 0.060933937607321825, "Bleu_4": 8.243191945354036e-06, "METEOR": 0.10888027375481185, "ROUGE_L": 0.15365239294710328, "CIDEr": 7.826762213970417e-13, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.14814814814814814, "f": 0.17777777777777776, "fn": 23.0, "numImages": 1.0, "fp": 14.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.4, "re": 0.2857142857142857, "f": 0.3333333333333333, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.15384615384615385, "f": 0.1904761904761905, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "This is a photograph of a bench in a park. The bench is made of stone and has a decorative design on it. There are trees and flowers around the bench.\n\nThe bench is in a park and has a decorative design on it. There are trees and flowers around the ben"}, "99707": {"image_id": 99707, "Bleu_1": 0.31372549018992696, "Bleu_2": 0.20957473279408315, "Bleu_3": 0.15305490466093202, "Bleu_4": 0.1105560791026379, "METEOR": 0.2726425150234403, "ROUGE_L": 0.2966050186680559, "CIDEr": 4.1936362150374926e-09, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.0967741935483871, "f": 0.11538461538461538, "fn": 28.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.25, "re": 0.1111111111111111, "f": 0.15384615384615383, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.16666666666666666, "f": 0.17391304347826086, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a person wearing a purple jacket and skiing gear standing next to a sign that reads superstar. The person is holding a snowboard and has a happy expression on their face. The background is a snowy mountain with trees and a blue sky.\n\nThe image is taken in"}, "124647": {"image_id": 124647, "Bleu_1": 0.2653061224435652, "Bleu_2": 0.18210783976741574, "Bleu_3": 0.12839858194733528, "Bleu_4": 1.4646393991657779e-05, "METEOR": 0.21069812658460182, "ROUGE_L": 0.23303196230739842, "CIDEr": 2.26152027331664e-09, "SPICE": {"All": {"pr": 0.23809523809523808, "re": 0.23809523809523808, "f": 0.23809523809523808, "fn": 16.0, "numImages": 1.0, "fp": 16.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5555555555555556, "re": 0.7142857142857143, "f": 0.6250000000000001, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 5.0}}, "caption": "The image shows a young boy wearing a black helmet and skateboarding on a sidewalk. The boy is wearing a black shirt and pants, and has a red scarf tied around his neck. There are several other people in the background, including a group of adults and children playing"}, "62167": {"image_id": 62167, "Bleu_1": 0.2340425531865098, "Bleu_2": 0.15949736033709896, "Bleu_3": 0.08268590713763764, "Bleu_4": 1.0646588104479637e-05, "METEOR": 0.1712213857742197, "ROUGE_L": 0.23297262889879056, "CIDEr": 2.303917471353771e-09, "SPICE": {"All": {"pr": 0.10344827586206896, "re": 0.2, "f": 0.13636363636363635, "fn": 12.0, "numImages": 1.0, "fp": 26.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.14285714285714285, "re": 0.3333333333333333, "f": 0.2, "fn": 4.0, "numImages": 1.0, "fp": 12.0, "tp": 2.0}}, "caption": "The image shows a man standing in a grassy field with several dogs around him. The man is wearing a tan shirt and khaki pants, and he is holding a leash in his right hand. The dogs are various breeds, including a black labrador retriever, a golden"}, "300221": {"image_id": 300221, "Bleu_1": 0.255813953482423, "Bleu_2": 0.11037036210960939, "Bleu_3": 6.672783373035958e-07, "Bleu_4": 1.6508791085804972e-09, "METEOR": 0.17732440768789956, "ROUGE_L": 0.20734194425560842, "CIDEr": 4.30427444562282e-08, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.14285714285714285, "f": 0.13636363636363635, "fn": 18.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.1111111111111111, "f": 0.13333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.3333333333333333, "f": 0.23529411764705885, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a variety of vegetables and fruits arranged in a wooden crate. The vegetables include carrots, beets, and lettuce, while the fruits include apples, oranges, and grapes. The crate is placed on a table in front of a building with a"}, "109537": {"image_id": 109537, "Bleu_1": 0.32608695651465036, "Bleu_2": 0.25537695922201126, "Bleu_3": 0.1140171677909247, "Bleu_4": 1.3625754904589812e-05, "METEOR": 0.24259795202052312, "ROUGE_L": 0.27619663648124193, "CIDEr": 2.305952670681084e-08, "SPICE": {"All": {"pr": 0.07142857142857142, "re": 0.07692307692307693, "f": 0.07407407407407408, "fn": 24.0, "numImages": 1.0, "fp": 26.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2222222222222222, "f": 0.18181818181818185, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a surfer riding a wave on a surfboard. The surfer is wearing a wetsuit and a helmet, and is standing on the board with his arms outstretched. The wave is large and white, and the surfer is jumping off the top of it"}, "382715": {"image_id": 382715, "Bleu_1": 0.2340425531865098, "Bleu_2": 0.12354612406992554, "Bleu_3": 6.973999503999067e-07, "Bleu_4": 1.6662810872712766e-09, "METEOR": 0.20840051176387378, "ROUGE_L": 0.23843648208469054, "CIDEr": 5.744023629204225e-09, "SPICE": {"All": {"pr": 0.16, "re": 0.2, "f": 0.17777777777777778, "fn": 16.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.1, "re": 0.125, "f": 0.11111111111111112, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.3333333333333333, "f": 0.23529411764705885, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a person on a skateboard riding down the sidewalk on a sunny day. The person is wearing a black jacket, black pants, and black sneakers. The sidewalk is made of concrete and there are cars parked on the street. The sky is blue and"}, "52596": {"image_id": 52596, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.2194268628641013, "Bleu_3": 0.17598878363779585, "Bleu_4": 0.13316595868552336, "METEOR": 0.28717205344161734, "ROUGE_L": 0.2513243084167157, "CIDEr": 7.235438881828713e-13, "SPICE": {"All": {"pr": 0.1875, "re": 0.25, "f": 0.21428571428571427, "fn": 18.0, "numImages": 1.0, "fp": 26.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.09090909090909091, "re": 0.16666666666666666, "f": 0.11764705882352942, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 0.5, "f": 0.28571428571428575, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Object": {"pr": 0.38461538461538464, "re": 0.5, "f": 0.4347826086956522, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a young girl holding a slice of pizza in her hand. She is standing in front of a group of people who are sitting on blankets and picnic tables. The sky is blue and there are trees in the background. The girl is wearing a purple shirt and has a small smile"}, "500718": {"image_id": 500718, "Bleu_1": 0.36734693876801333, "Bleu_2": 0.2142857142812956, "Bleu_3": 0.1250180897226835, "Bleu_4": 1.4356222678314862e-05, "METEOR": 0.185892798408072, "ROUGE_L": 0.2576946288473144, "CIDEr": 2.1687743547107492e-09, "SPICE": {"All": {"pr": 0.09523809523809523, "re": 0.09090909090909091, "f": 0.09302325581395349, "fn": 20.0, "numImages": 1.0, "fp": 19.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.1111111111111111, "re": 0.1111111111111111, "f": 0.1111111111111111, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}}, "caption": "The image shows a group of airplanes parked on the tarmac at an airport. The planes are painted in different colors and have different designs on their tails. Some of the planes have engines on the wings, while others have them on the tail. The planes are parked in"}, "182240": {"image_id": 182240, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.1447907475849832, "Bleu_3": 0.10654502541910188, "Bleu_4": 0.06978423773350045, "METEOR": 0.16951521592680752, "ROUGE_L": 0.17468499427262313, "CIDEr": 2.6001914626509153e-12, "SPICE": {"All": {"pr": 0.125, "re": 0.2222222222222222, "f": 0.16, "fn": 14.0, "numImages": 1.0, "fp": 28.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.21428571428571427, "re": 0.375, "f": 0.2727272727272727, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a group of zebras standing in a field of tall grass. They are all facing the same direction and appear to be grazing. The sky is clear and blue, with a few white clouds scattered across it. The ground is dry and cracked, with some small rocks and bushes visible in"}, "525083": {"image_id": 525083, "Bleu_1": 0.3265306122382341, "Bleu_2": 0.20203050890627616, "Bleu_3": 0.0954066585882441, "Bleu_4": 1.172180673188117e-05, "METEOR": 0.19196771346580116, "ROUGE_L": 0.23303196230739842, "CIDEr": 1.237579393196944e-10, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.13043478260869565, "f": 0.13636363636363635, "fn": 20.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a kitchen with wooden floors, white cabinets, and a black stove. There is a dining table and chairs in the center of the room, and a window on the left side. The room is well lit and has a large wooden door on the right side."}, "56821": {"image_id": 56821, "Bleu_1": 0.18181818181487608, "Bleu_2": 0.08206099398471602, "Bleu_3": 0.05027274032560819, "Bleu_4": 7.030700263657399e-06, "METEOR": 0.1710073551018892, "ROUGE_L": 0.14672279013830425, "CIDEr": 1.0096140954004983e-12, "SPICE": {"All": {"pr": 0.3684210526315789, "re": 0.25925925925925924, "f": 0.30434782608695654, "fn": 20.0, "numImages": 1.0, "fp": 12.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.4, "re": 0.25, "f": 0.3076923076923077, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.625, "re": 0.5, "f": 0.5555555555555556, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 5.0}}, "caption": "The image shows a street with tall buildings on either side. There are cars parked on the side of the road and people walking on the sidewalk. The sky is blue and there are clouds in the sky. The buildings are made of brick and have large windows on the upper floors. There are streetlights"}, "256367": {"image_id": 256367, "Bleu_1": 0.1666666666635803, "Bleu_2": 0.11215443081631235, "Bleu_3": 6.230790884875896e-07, "Bleu_4": 1.475756952410161e-09, "METEOR": 0.17280783889328488, "ROUGE_L": 0.21403508771929822, "CIDEr": 4.278880897852249e-13, "SPICE": {"All": {"pr": 0.125, "re": 0.10344827586206896, "f": 0.11320754716981132, "fn": 26.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.14285714285714285, "f": 0.18181818181818182, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 12.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a train traveling on a railroad track. The train is blue and white and has a number on the side. There are trees and buildings in the background.\n\nThe train is traveling on the track at a moderate speed. The train has a number on the side and is blue and"}, "42667": {"image_id": 42667, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.15861031714066393, "Bleu_3": 0.11322098426538772, "Bleu_4": 0.07303863538757409, "METEOR": 0.24566470554651612, "ROUGE_L": 0.2445589919816724, "CIDEr": 2.6457760056752254e-12, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.15384615384615385, "f": 0.1509433962264151, "fn": 22.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a baby girl sitting in a suitcase, looking up at the camera with a smile on her face. She is wearing a green shirt and has a small toy in her hand. The background is a dark brown wall with a window in the background.\n\nThe image is taken in a"}, "388453": {"image_id": 388453, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.18164975363435543, "Bleu_3": 0.1460117883952727, "Bleu_4": 0.12439562377602817, "METEOR": 0.20212126004699343, "ROUGE_L": 0.27006087437742116, "CIDEr": 1.7112874508014825e-12, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.3, "f": 0.25531914893617025, "fn": 14.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.1, "f": 0.10526315789473685, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.5555555555555556, "f": 0.4545454545454546, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a group of people standing in a large room with chandeliers hanging from the ceiling. There are several tables set up with people sitting at them, and there are chairs set up around the room for people to sit in. The walls are painted a light blue color and there are windows"}, "38118": {"image_id": 38118, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.1628822035823003, "Bleu_3": 0.11836112830783165, "Bleu_4": 0.07706946743067312, "METEOR": 0.21864138410564266, "ROUGE_L": 0.1937738246505718, "CIDEr": 4.0761652118752395e-11, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.18518518518518517, "f": 0.20408163265306123, "fn": 22.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.125, "re": 0.09090909090909091, "f": 0.10526315789473685, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a person skiing down a snowy slope. The person is wearing a red jacket and black pants, and has a backpack on their back. The sky is clear and blue, and there are mountains in the background.\n\nThe image is taken from a low angle, looking up"}, "305319": {"image_id": 305319, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.1748949264355777, "Bleu_3": 0.08546457042801381, "Bleu_4": 1.0678967780508542e-05, "METEOR": 0.2159936473706852, "ROUGE_L": 0.26704190118824267, "CIDEr": 7.274601368589507e-11, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.17142857142857143, "f": 0.1935483870967742, "fn": 29.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.42857142857142855, "f": 0.42857142857142855, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The image shows a person surfing on a wave in the ocean. The person is wearing a black wetsuit and has a surfboard under their arm. The wave is large and white, with a lot of foam on top. The sky is blue and there are clouds in the background. The"}, "410484": {"image_id": 410484, "Bleu_1": 0.27999999999440006, "Bleu_2": 0.13093073413895012, "Bleu_3": 0.0709491705970709, "Bleu_4": 9.336541408128833e-06, "METEOR": 0.19083034410994232, "ROUGE_L": 0.26228501228501233, "CIDEr": 1.3405908652000286e-07, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.1111111111111111, "f": 0.11320754716981132, "fn": 24.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a bride and groom cutting a cake together. The cake is blue and white with a red star on top. The bride is wearing a white dress and the groom is wearing a black suit. They are both smiling and holding the cake together. There are several"}, "454252": {"image_id": 454252, "Bleu_1": 0.27083333332769105, "Bleu_2": 0.16974114626800244, "Bleu_3": 0.12339926173807332, "Bleu_4": 0.08038616175390124, "METEOR": 0.2681737071482838, "ROUGE_L": 0.27799479166666663, "CIDEr": 6.6528889644895824e-09, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.36363636363636365, "f": 0.24242424242424246, "fn": 7.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.6666666666666666, "f": 0.5714285714285715, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.6666666666666666, "f": 0.8, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.18181818181818182, "re": 0.5, "f": 0.26666666666666666, "fn": 2.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a group of people standing in front of a barrel of wine. They are all smiling and holding glasses of wine. The barrel is made of wood and has a label on it that reads \"2012 Cabernet Sauvignon\". The room is dimly lit and has"}, "245153": {"image_id": 245153, "Bleu_1": 0.28571428570845486, "Bleu_2": 0.1543033499589101, "Bleu_3": 0.07971700074705254, "Bleu_4": 1.0244098719420824e-05, "METEOR": 0.19540168708644906, "ROUGE_L": 0.19690122659780504, "CIDEr": 6.494696747970195e-10, "SPICE": {"All": {"pr": 0.125, "re": 0.2, "f": 0.15384615384615385, "fn": 12.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.6, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows two puffins sitting on top of a grassy hill overlooking a body of water. The puffins are looking out towards the water and appear to be in a state of relaxation. The sky is cloudy and there are some trees in the background.\n\nThe puffins are"}, "528980": {"image_id": 528980, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.1577557537051747, "Bleu_3": 0.07925015807505884, "Bleu_4": 1.0039245690493998e-05, "METEOR": 0.2167519303113494, "ROUGE_L": 0.22536945812807885, "CIDEr": 3.161733753232134e-11, "SPICE": {"All": {"pr": 0.2962962962962963, "re": 0.25806451612903225, "f": 0.27586206896551724, "fn": 23.0, "numImages": 1.0, "fp": 19.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.16666666666666666, "f": 0.14285714285714288, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 0.5, "f": 0.28571428571428575, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Object": {"pr": 0.5833333333333334, "re": 0.5833333333333334, "f": 0.5833333333333334, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 7.0}}, "caption": "The image shows a red and white umbrella on a street corner in a city. The umbrella is open and has a handle on the side. There are buildings in the background and cars parked on the street.\n\nThe image is taken from a bird's eye view, looking down on the"}, "402118": {"image_id": 402118, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.1288070335219516, "Bleu_3": 0.08722608547574774, "Bleu_4": 0.06066465987455706, "METEOR": 0.21794813476303823, "ROUGE_L": 0.25176886792452824, "CIDEr": 2.51575482683591e-11, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.21052631578947367, "f": 0.1951219512195122, "fn": 15.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.3, "re": 0.42857142857142855, "f": 0.3529411764705882, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a person snowboarding down a mountain. The person is wearing a white snowboarding suit and a black helmet. The person is jumping off a ramp and flying through the air. The background is a mountain range with trees and snow on the ground. The sky is clear and blue"}, "303893": {"image_id": 303893, "Bleu_1": 0.28947368420290864, "Bleu_2": 0.12508887309711966, "Bleu_3": 0.07574924114606951, "Bleu_4": 1.0556421043203983e-05, "METEOR": 0.1880496289244585, "ROUGE_L": 0.23940345368916802, "CIDEr": 6.46654771063279e-05, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.29411764705882354, "f": 0.25641025641025644, "fn": 12.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.16666666666666666, "f": 0.11764705882352942, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.6666666666666666, "f": 0.5333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows two children sitting on the ground, one holding a bowl of food and the other holding a spoon. They are both wearing green shirts and shorts. The background is a park with trees and grass."}, "491836": {"image_id": 491836, "Bleu_1": 0.26315789473222534, "Bleu_2": 0.18136906252429263, "Bleu_3": 0.1337440815614704, "Bleu_4": 0.09702083064171878, "METEOR": 0.2153744047765719, "ROUGE_L": 0.2733893557422969, "CIDEr": 5.5095403040971606e-14, "SPICE": {"All": {"pr": 0.25806451612903225, "re": 0.24242424242424243, "f": 0.25, "fn": 25.0, "numImages": 1.0, "fp": 23.0, "tp": 8.0}, "Relation": {"pr": 0.1, "re": 0.07142857142857142, "f": 0.08333333333333333, "fn": 13.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.18181818181818182, "re": 0.25, "f": 0.2105263157894737, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}, "Size": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.45454545454545453, "f": 0.47619047619047616, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "The image shows a body of water with a small boat in the distance. The sky is clear and blue, with some clouds in the distance. There are trees on the shore and a small island in the distance. The water is calm and there are no waves.\n\nThe image is taken from the perspective of someone"}, "456816": {"image_id": 456816, "Bleu_1": 0.2448979591786756, "Bleu_2": 0.17496355305233346, "Bleu_3": 0.10921336092038615, "Bleu_4": 1.2972311576394404e-05, "METEOR": 0.19235019947207738, "ROUGE_L": 0.236281471917366, "CIDEr": 1.3848742526059538e-09, "SPICE": {"All": {"pr": 0.125, "re": 0.19047619047619047, "f": 0.1509433962264151, "fn": 17.0, "numImages": 1.0, "fp": 28.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.4, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a plate with a doughnut on it. The doughnut is covered in powdered sugar and has a hole in the middle. There is a fork on the plate. The background is a wooden table with a white tablecloth.\n\nThe image shows a plate with a dough"}, "495825": {"image_id": 495825, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.14002800840008178, "Bleu_3": 0.07319587495056674, "Bleu_4": 9.458362067959735e-06, "METEOR": 0.17335506758636698, "ROUGE_L": 0.22332635983263593, "CIDEr": 6.268983321335053e-09, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.1724137931034483, "f": 0.19607843137254902, "fn": 24.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.125, "re": 0.08333333333333333, "f": 0.1, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.2727272727272727, "f": 0.28571428571428564, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a herd of cows grazing in a green field with mountains in the background. The sky is a bright blue with fluffy white clouds. The grass is tall and green, and there are no trees in the field. The fence is made of wooden posts and barbed wire. The"}, "174740": {"image_id": 174740, "Bleu_1": 0.36734693876801333, "Bleu_2": 0.2142857142812956, "Bleu_3": 0.1250180897226835, "Bleu_4": 1.4356222678314862e-05, "METEOR": 0.20008983657426724, "ROUGE_L": 0.25957446808510637, "CIDEr": 1.9844296837744904e-08, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.16666666666666666, "f": 0.18867924528301885, "fn": 25.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.5, "re": 0.25, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a box with two doughnuts inside, sitting on top of a wooden table. There is a bottle of soda next to the box. The table is made of wood and has a rough texture. The doughnuts are covered in powdered sugar and appear to be freshly"}, "507037": {"image_id": 507037, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.20597146021399534, "Bleu_3": 0.16871867607116212, "Bleu_4": 0.12901845797361647, "METEOR": 0.28891921541133264, "ROUGE_L": 0.2872277810476751, "CIDEr": 4.01508816799731e-12, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.19047619047619047, "f": 0.16666666666666666, "fn": 17.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4444444444444444, "f": 0.3636363636363637, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a group of people standing outside a building with bicycles parked next to them. The building appears to be a church with a large cross on top of it. There are several people standing in front of the building, some of them are holding bicycles and others are just standing there. The"}, "57323": {"image_id": 57323, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.20550493309104517, "Bleu_3": 0.11909718541937829, "Bleu_4": 1.36262735599291e-05, "METEOR": 0.29420029528110764, "ROUGE_L": 0.26293103448275856, "CIDEr": 4.536405870108674e-12, "SPICE": {"All": {"pr": 0.25, "re": 0.2727272727272727, "f": 0.2608695652173913, "fn": 16.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.5, "f": 0.47619047619047616, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a group of people riding motorcycles down a street. They are all wearing helmets and riding in a line. There are cars parked on the side of the road and a few people walking on the sidewalk. The sky is clear and there are no buildings in the background"}, "516038": {"image_id": 516038, "Bleu_1": 0.30188679244713423, "Bleu_2": 0.2015901948875348, "Bleu_3": 0.11680628450365417, "Bleu_4": 0.07513757183070931, "METEOR": 0.25159914837016356, "ROUGE_L": 0.25894481503941785, "CIDEr": 4.688514386397579e-12, "SPICE": {"All": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 24.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.09090909090909091, "f": 0.09999999999999999, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.125, "f": 0.16666666666666666, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.36363636363636365, "f": 0.36363636363636365, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a baseball game in progress on a green field. There are several players on the field, including a pitcher and a catcher. The pitcher is throwing the ball to the catcher, who is holding his glove up to catch it. The other players are standing on the field, watching the"}, "433915": {"image_id": 433915, "Bleu_1": 0.16071428571141586, "Bleu_2": 0.07644707871426631, "Bleu_3": 4.765509535913812e-07, "Bleu_4": 1.1953994172638397e-09, "METEOR": 0.18894226433515965, "ROUGE_L": 0.19593147751605997, "CIDEr": 1.0074413341640648e-12, "SPICE": {"All": {"pr": 0.07407407407407407, "re": 0.11764705882352941, "f": 0.0909090909090909, "fn": 15.0, "numImages": 1.0, "fp": 25.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2857142857142857, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image is a television screen showing a man and woman in suits standing in front of a white wall. The man is wearing a red tie and the woman is wearing a blue dress. They are both looking directly at the camera.\n\nThe image is in black and white and appears to be a still"}, "17899": {"image_id": 17899, "Bleu_1": 0.3199999999936, "Bleu_2": 0.24243661068763242, "Bleu_3": 0.1542974711761775, "Bleu_4": 1.672032219050298e-05, "METEOR": 0.1919261630078363, "ROUGE_L": 0.25386444708680145, "CIDEr": 4.3288804638918854e-10, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.16129032258064516, "f": 0.18518518518518517, "fn": 26.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3, "re": 0.3, "f": 0.3, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.6, "re": 1.0, "f": 0.7499999999999999, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 3.0}, "Object": {"pr": 0.2857142857142857, "re": 0.18181818181818182, "f": 0.2222222222222222, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The image shows a woman in a pink apron standing at a table with a tray of baked goods in front of her. She is using a rolling pin to roll out dough on a floured surface. There are several other baked goods on the table, including cookies, muffins, and"}, "298252": {"image_id": 298252, "Bleu_1": 0.2857142857074831, "Bleu_2": 0.25043516133286925, "Bleu_3": 0.21110278158177162, "Bleu_4": 0.16401540050025415, "METEOR": 0.2752036187965392, "ROUGE_L": 0.2543786488740617, "CIDEr": 1.0445581595362261e-06, "SPICE": {"All": {"pr": 0.15151515151515152, "re": 0.38461538461538464, "f": 0.21739130434782608, "fn": 8.0, "numImages": 1.0, "fp": 28.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.5, "f": 0.25, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.26666666666666666, "re": 0.6666666666666666, "f": 0.3809523809523809, "fn": 2.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "The image shows a variety of donuts on display in a bakery. There are several different types of donuts, including chocolate, glazed, and sprinkles. The donuts are arranged on a tray in a display case.\n\nThe image is taken in a bakery."}, "222370": {"image_id": 222370, "Bleu_1": 0.18181818181487608, "Bleu_2": 0.10050378152407698, "Bleu_3": 5.754792186290893e-07, "Bleu_4": 1.3836345500640894e-09, "METEOR": 0.15188630544087336, "ROUGE_L": 0.17951736315479697, "CIDEr": 3.5632021202386195e-14, "SPICE": {"All": {"pr": 0.3333333333333333, "re": 0.34782608695652173, "f": 0.3404255319148936, "fn": 15.0, "numImages": 1.0, "fp": 16.0, "tp": 8.0}, "Relation": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.125, "f": 0.11764705882352941, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.6, "re": 0.6666666666666666, "f": 0.631578947368421, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 6.0}}, "caption": "The image shows a group of people walking down a narrow street in a city. The buildings on either side of the street are tall and made of brick and stone. There are no cars parked on the street, but there are some parked on the sidewalk. The people in the image are wearing casual"}, "374448": {"image_id": 374448, "Bleu_1": 0.24999999999553577, "Bleu_2": 0.11677484162212426, "Bleu_3": 6.320745022725613e-07, "Bleu_4": 1.477431097986996e-09, "METEOR": 0.20152590720360739, "ROUGE_L": 0.19978165938864628, "CIDEr": 3.578630424821948e-12, "SPICE": {"All": {"pr": 0.2, "re": 0.15151515151515152, "f": 0.17241379310344826, "fn": 28.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.3076923076923077, "f": 0.34782608695652173, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a large room with a high ceiling and wooden beams. There are several tables and chairs set up in the room, and people are sitting at them. The walls are painted a warm color and there are several large windows that let in natural light. The floor is made of hardwood and there"}, "181278": {"image_id": 181278, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.09356392870058139, "Bleu_3": 5.486731100334785e-07, "Bleu_4": 1.3350097003360843e-09, "METEOR": 0.20556581930717888, "ROUGE_L": 0.24610951008645532, "CIDEr": 2.4640367196734114e-12, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.15384615384615385, "f": 0.16666666666666669, "fn": 22.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 1.0, "re": 0.125, "f": 0.2222222222222222, "fn": 7.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a group of young boys playing baseball on a field. They are wearing white and blue uniforms with the number 1 on the back. One of the boys is throwing a ball while the others watch. There is a large tree in the background.\n\nThe image is taken in a park with"}, "373789": {"image_id": 373789, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.17458222992646102, "Bleu_3": 0.13370981749106947, "Bleu_4": 0.09888660568274774, "METEOR": 0.1834054794942187, "ROUGE_L": 0.22195269860521533, "CIDEr": 1.8047858634397157e-09, "SPICE": {"All": {"pr": 0.25, "re": 0.21428571428571427, "f": 0.23076923076923075, "fn": 22.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.5555555555555556, "f": 0.4761904761904762, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a plate of toast with two pieces of fish on it. The fish is cut into small pieces and is placed on top of the toast. The plate is on a countertop in a kitchen. The fish is a type of white fish, such as cod or haddock. The toast"}, "251124": {"image_id": 251124, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.16816819849581244, "Bleu_3": 8.270007684490464e-07, "Bleu_4": 1.843233754139035e-09, "METEOR": 0.21432378051706308, "ROUGE_L": 0.28175519630484985, "CIDEr": 1.0586846500240043e-10, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 21.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a bicycle parked on a bench in front of a city skyline at night. The skyline is made up of tall buildings with lights on them. The bicycle is black and has a red light on the front. The bench is made of wood and has a red light"}, "162249": {"image_id": 162249, "Bleu_1": 0.24999999999553577, "Bleu_2": 0.1348399724902187, "Bleu_3": 0.08765119647604921, "Bleu_4": 0.059703448811482686, "METEOR": 0.24013397851607038, "ROUGE_L": 0.2717149220489977, "CIDEr": 4.889966020566197e-13, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.1, "f": 0.10526315789473685, "fn": 18.0, "numImages": 1.0, "fp": 16.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.2857142857142857, "f": 0.23529411764705882, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "This is a bathroom with a large tub in the center of the room. There is a sink and toilet in the corner of the room. The walls are painted a light pink color and there are two windows on either side of the room. The floor is made of hardwood and there is a rug"}, "551908": {"image_id": 551908, "Bleu_1": 0.27083333332769105, "Bleu_2": 0.18594210949553755, "Bleu_3": 0.1311312829167715, "Bleu_4": 0.10005391875419713, "METEOR": 0.2504755217677302, "ROUGE_L": 0.34961790393013104, "CIDEr": 1.889009300725246e-07, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.2857142857142857, "f": 0.25, "fn": 15.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.7142857142857143, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a plate of food with a piece of fish, broccoli, and potatoes. The fish is cooked and has a golden brown color. The broccoli is steamed and has a green color. The potatoes are boiled and have a white color. There are no other ingred"}, "366367": {"image_id": 366367, "Bleu_1": 0.33333333332716053, "Bleu_2": 0.2747211278922427, "Bleu_3": 0.2057361820493597, "Bleu_4": 0.15044258140713676, "METEOR": 0.2825947969494865, "ROUGE_L": 0.25507765830346474, "CIDEr": 1.8441878996954174e-12, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.13793103448275862, "f": 0.1509433962264151, "fn": 25.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.09090909090909091, "f": 0.12500000000000003, "fn": 10.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2727272727272727, "f": 0.2608695652173913, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a young girl standing in front of a whiteboard with various words and numbers written on it. She is holding a small tablet in her hand and looking at it intently. The room is filled with various educational materials and posters on the walls.\n\nThe girl is wearing a red swe"}, "231343": {"image_id": 231343, "Bleu_1": 0.3207547169750801, "Bleu_2": 0.24836183164306555, "Bleu_3": 0.18218943070465532, "Bleu_4": 0.1380162415922358, "METEOR": 0.2553473216472224, "ROUGE_L": 0.25341246290801184, "CIDEr": 8.313892980202618e-08, "SPICE": {"All": {"pr": 0.23809523809523808, "re": 0.4166666666666667, "f": 0.30303030303030304, "fn": 7.0, "numImages": 1.0, "fp": 16.0, "tp": 5.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.75, "f": 0.4285714285714285, "fn": 1.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a baseball player in the middle of a game, holding a bat and wearing a helmet. The other players are standing on the sidelines, watching him play. The field is made of grass and there are spectators in the stands. The sky is cloudy and there are trees in the"}, "236290": {"image_id": 236290, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.13719886811129003, "Bleu_3": 0.07269451760681338, "Bleu_4": 9.458362067955946e-06, "METEOR": 0.17733537854903528, "ROUGE_L": 0.19513755598208574, "CIDEr": 9.48842376952579e-12, "SPICE": {"All": {"pr": 0.12903225806451613, "re": 0.13793103448275862, "f": 0.13333333333333333, "fn": 25.0, "numImages": 1.0, "fp": 27.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.08333333333333333, "f": 0.09523809523809525, "fn": 11.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.21428571428571427, "re": 0.375, "f": 0.2727272727272727, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a man standing on a staircase with a suitcase in his hand. The staircase is made of metal and has a railing on either side. The man is wearing a black suit and a white shirt, and he is looking down at the suitcase in his hand. There"}, "189095": {"image_id": 189095, "Bleu_1": 0.19999999999555562, "Bleu_2": 0.13483997248961796, "Bleu_3": 0.09456554943341472, "Bleu_4": 1.191207420209852e-05, "METEOR": 0.1561862655113871, "ROUGE_L": 0.2350674373795761, "CIDEr": 4.7368318797489944e-08, "SPICE": {"All": {"pr": 0.16, "re": 0.16666666666666666, "f": 0.16326530612244897, "fn": 20.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4, "f": 0.380952380952381, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows two brown bears standing on a rocky outcropping, one of them is biting the other's ear. The bears are standing in a clearing surrounded by trees and rocks. The sky is blue and there are no other animals in the image."}, "426546": {"image_id": 426546, "Bleu_1": 0.34146341462581803, "Bleu_2": 0.2065984676302324, "Bleu_3": 0.12983940805705807, "Bleu_4": 0.08711822053257058, "METEOR": 0.22366240159971734, "ROUGE_L": 0.3165307635285397, "CIDEr": 5.927435829430394e-05, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.15384615384615385, "f": 0.16326530612244897, "fn": 22.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.125, "re": 0.09090909090909091, "f": 0.10526315789473685, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.2727272727272727, "f": 0.28571428571428564, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a plate of food with two sandwiches, fries, and a side of coleslaw. The sandwiches are made with meat and cheese, and the fries are crispy and golden. The coleslaw is a creamy, tangy side dish. The plate"}, "114119": {"image_id": 114119, "Bleu_1": 0.37999999999240003, "Bleu_2": 0.23299294899957962, "Bleu_3": 0.10418729749884431, "Bleu_4": 1.2454795908039963e-05, "METEOR": 0.2478256595725193, "ROUGE_L": 0.2357639783560938, "CIDEr": 3.258764585046225e-10, "SPICE": {"All": {"pr": 0.047619047619047616, "re": 0.047619047619047616, "f": 0.047619047619047616, "fn": 20.0, "numImages": 1.0, "fp": 20.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.1, "re": 0.1111111111111111, "f": 0.10526315789473685, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}}, "caption": "The image is of a cat sleeping on a shelf next to a clock. The cat is lying on its side with its paws tucked under its body. The clock is on the wall behind the cat, with the numbers 12 and 3 visible. The cat's eyes are closed,"}, "290828": {"image_id": 290828, "Bleu_1": 0.20408163264889634, "Bleu_2": 0.11293848786082757, "Bleu_3": 0.06474337870309639, "Bleu_4": 8.764094705114603e-06, "METEOR": 0.18265856781251746, "ROUGE_L": 0.19690122659780504, "CIDEr": 1.8587509434383777e-09, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.35294117647058826, "f": 0.3, "fn": 11.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5555555555555556, "re": 0.5, "f": 0.5263157894736842, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 5.0}}, "caption": "The image shows a giraffe standing on a concrete walkway in front of a fence. The giraffe is looking down at the ground and appears to be eating something. There are people standing on the walkway, looking at the giraffe. The sky is cloudy and there are trees in"}, "64103": {"image_id": 64103, "Bleu_1": 0.1551724137904281, "Bleu_2": 0.09037128496774492, "Bleu_3": 0.05263697982682622, "Bleu_4": 7.1759193592657005e-06, "METEOR": 0.18100561246558067, "ROUGE_L": 0.17548906789413118, "CIDEr": 4.529402874817056e-16, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.20833333333333334, "f": 0.1851851851851852, "fn": 19.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.1, "f": 0.10526315789473685, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows two birds standing in the water, one of them is looking at the other and the other is looking away. The water is calm and there are no other objects in the image. The birds are black and white with long beaks. The sky is blue and there are no clouds. The image is taken"}, "194756": {"image_id": 194756, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.14793756076371775, "Bleu_3": 7.446639778133948e-07, "Bleu_4": 1.67868582615359e-09, "METEOR": 0.22058373144070167, "ROUGE_L": 0.24110671936758893, "CIDEr": 1.0490393987782815e-12, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.21739130434782608, "f": 0.20408163265306123, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4, "f": 0.3636363636363636, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a small boat with a blue hull and white sides, tied up to a dock on the side of a river. There are people standing on the dock and walking along the sidewalk. The sky is cloudy and there are trees and buildings visible in the background.\n\nThe image is taken from"}, "412879": {"image_id": 412879, "Bleu_1": 0.32692307691678996, "Bleu_2": 0.19611613513437562, "Bleu_3": 0.13214760629865424, "Bleu_4": 0.0985151344761547, "METEOR": 0.2630379610710066, "ROUGE_L": 0.25722891566265055, "CIDEr": 6.453134287589401e-11, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.25, "f": 0.22641509433962265, "fn": 18.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.16666666666666666, "f": 0.1111111111111111, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.4, "re": 0.2222222222222222, "f": 0.2857142857142857, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The woman in the image is playing tennis with a racket and a ball on a tennis court. She is wearing a black top and white pants, and her hair is tied back in a ponytail. The background is a green grassy field with trees in the distance.\n\nThe woman in the"}, "51741": {"image_id": 51741, "Bleu_1": 0.3090909090852893, "Bleu_2": 0.20016827936980822, "Bleu_3": 9.109716901600834e-07, "Bleu_4": 1.9526653721081553e-09, "METEOR": 0.21791963566388456, "ROUGE_L": 0.24610951008645532, "CIDEr": 2.7973634805692417e-11, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.46153846153846156, "f": 0.2926829268292683, "fn": 7.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.2222222222222222, "re": 0.4, "f": 0.2857142857142857, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.5, "f": 0.25, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.5, "f": 0.3157894736842105, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a bathroom with a toilet, sink, and shower. The walls are made of wood and the floor is made of tile. There is a window on the left side of the room and a door on the right side. The room is well lit and there is a fan in the ce"}, "111448": {"image_id": 111448, "Bleu_1": 0.22916666666189242, "Bleu_2": 0.15613914506966814, "Bleu_3": 8.092609555849529e-07, "Bleu_4": 1.8525217352695958e-09, "METEOR": 0.15614701125686403, "ROUGE_L": 0.2147887323943662, "CIDEr": 7.10884005100953e-09, "SPICE": {"All": {"pr": 0.2916666666666667, "re": 0.2692307692307692, "f": 0.27999999999999997, "fn": 19.0, "numImages": 1.0, "fp": 17.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.14285714285714285, "f": 0.22222222222222224, "fn": 6.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.46153846153846156, "re": 0.6, "f": 0.5217391304347826, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a horse and jockey on a racetrack. The horse is wearing a black and white striped blanket and a black and white saddle. The jockey is wearing a black and white striped shirt and black pants. There are people in the background watching the race."}, "255279": {"image_id": 255279, "Bleu_1": 0.1960784313687044, "Bleu_2": 0.15339299776643633, "Bleu_3": 0.07830779525362613, "Bleu_4": 1.0001000249872884e-05, "METEOR": 0.2079540574859182, "ROUGE_L": 0.22889305816135083, "CIDEr": 7.403623639342519e-11, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.13333333333333333, "f": 0.14035087719298248, "fn": 26.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.36363636363636365, "f": 0.39999999999999997, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a black motorcycle parked on the side of a road. The motorcycle has a black helmet on the handlebars and a black jacket on the rider's back. There is a car parked in the background. The image is taken in a residential area with trees and houses"}, "243626": {"image_id": 243626, "Bleu_1": 0.31249999999348965, "Bleu_2": 0.21573723181113055, "Bleu_3": 0.1264855137960998, "Bleu_4": 1.4562215827951557e-05, "METEOR": 0.23428743828669005, "ROUGE_L": 0.2812911725955204, "CIDEr": 5.226845566973905e-08, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.22727272727272727, "f": 0.22222222222222224, "fn": 17.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.5, "f": 0.47058823529411764, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a plate of food with a piece of meat, green beans, and orange slices. The meat appears to be cooked and the green beans are steamed. The orange slices are sliced and placed on top of the meat. The plate is on a white tablecloth."}, "171255": {"image_id": 171255, "Bleu_1": 0.3584905660309719, "Bleu_2": 0.21967782188085846, "Bleu_3": 0.1236927036724413, "Bleu_4": 0.07843602219341915, "METEOR": 0.23997841850080362, "ROUGE_L": 0.25341246290801184, "CIDEr": 2.0465570156234797e-10, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.35714285714285715, "f": 0.27027027027027023, "fn": 9.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.25, "f": 0.28571428571428575, "fn": 3.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.36363636363636365, "re": 0.8, "f": 0.5000000000000001, "fn": 1.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a group of people standing on the beach, looking out at the ocean. There are several horses grazing on the sand nearby. The sky is clear and blue, with a few clouds scattered across it. The beach is lined with palm trees and there are several beach umbrellas set up"}, "41119": {"image_id": 41119, "Bleu_1": 0.3207547169750801, "Bleu_2": 0.23561672155300986, "Bleu_3": 0.163292915906478, "Bleu_4": 0.09660130805021472, "METEOR": 0.22737230226004868, "ROUGE_L": 0.28103661044837513, "CIDEr": 3.243740412589057e-11, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.2, "f": 0.15789473684210528, "fn": 12.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.125, "re": 0.25, "f": 0.16666666666666666, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.4, "f": 0.26666666666666666, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a green traffic light hanging from a pole in the middle of a busy street. The light is on, indicating that it is safe to cross the street. The building in the background is a tall, modern skyscraper with large windows and a sleek, modern design. The street is l"}}}