{"overall": {"Bleu_1": 0.2622451218942167, "Bleu_2": 0.17618410336069293, "Bleu_3": 0.1138062542641272, "Bleu_4": 0.07438727132294368, "METEOR": 0.2178134465030627, "ROUGE_L": 0.257464917383314, "CIDEr": 2.8865853336166508e-05, "SPICE": 0.18389452996463307}, "imgToEval": {"244575": {"image_id": 244575, "Bleu_1": 0.20370370369993146, "Bleu_2": 0.175350303278286, "Bleu_3": 0.08393373828029471, "Bleu_4": 1.0376714806170423e-05, "METEOR": 0.16720079861290693, "ROUGE_L": 0.2053872053872054, "CIDEr": 3.58697751898256e-12, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.2857142857142857, "f": 0.25, "fn": 15.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.625, "f": 0.5, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows two horses standing in a clearing surrounded by trees. The horses are brown and black in color and have long manes and tails. They are grazing on the grass in the clearing. The trees in the background are tall and green, with branches reaching up towards the sky. The sky is"}, "464737": {"image_id": 464737, "Bleu_1": 0.27083333332769105, "Bleu_2": 0.21470745394187096, "Bleu_3": 0.15885423636837526, "Bleu_4": 0.09715070856447153, "METEOR": 0.23059914718319155, "ROUGE_L": 0.31077147016011636, "CIDEr": 4.827710706850639e-08, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.17391304347826086, "f": 0.1702127659574468, "fn": 19.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5, "f": 0.4210526315789474, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows two zebras standing in a grassy field with an ostrich in the background. The zebras are black and white with white stripes on their backs. The ostrich is brown with a long neck and legs. The image is taken from a bird's eye view."}, "284379": {"image_id": 284379, "Bleu_1": 0.27999999999440006, "Bleu_2": 0.19999999999595922, "Bleu_3": 0.09410360288618189, "Bleu_4": 1.1539320184366715e-05, "METEOR": 0.21546860016245742, "ROUGE_L": 0.22732919254658387, "CIDEr": 1.1788706683269816e-10, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.3076923076923077, "f": 0.29629629629629634, "fn": 18.0, "numImages": 1.0, "fp": 20.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.5, "f": 0.25, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5833333333333334, "re": 0.5384615384615384, "f": 0.5599999999999999, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 7.0}}, "caption": "The image shows a young boy riding a wave on a surfboard in the ocean. The boy is wearing a life jacket and has his arms outstretched to balance himself on the board. The water is blue and there are waves crashing against the shore in the background.\n\nThe image"}, "252940": {"image_id": 252940, "Bleu_1": 0.3199999999936, "Bleu_2": 0.19794866371815806, "Bleu_3": 9.345903743205193e-07, "Bleu_4": 2.0414630012523327e-09, "METEOR": 0.23368297398969995, "ROUGE_L": 0.3104501244062429, "CIDEr": 1.0035189150349571e-05, "SPICE": {"All": {"pr": 0.037037037037037035, "re": 0.043478260869565216, "f": 0.039999999999999994, "fn": 22.0, "numImages": 1.0, "fp": 26.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.08333333333333333, "re": 0.1, "f": 0.0909090909090909, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}}, "caption": "The image shows a man and woman standing under an umbrella in a park. The man is wearing a black suit and the woman is wearing a purple dress. They are both holding umbrellas and looking at each other. The background is a green tree with leaves and branches.\n\nThe"}, "101068": {"image_id": 101068, "Bleu_1": 0.27586206896076104, "Bleu_2": 0.20870354281255962, "Bleu_3": 0.14598577364278748, "Bleu_4": 1.5422054353314752e-05, "METEOR": 0.24079621721200486, "ROUGE_L": 0.2973997833152763, "CIDEr": 1.1264541984077991e-14, "SPICE": {"All": {"pr": 0.3333333333333333, "re": 0.2962962962962963, "f": 0.3137254901960785, "fn": 19.0, "numImages": 1.0, "fp": 16.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1, "re": 0.14285714285714285, "f": 0.11764705882352941, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 1.0, "f": 0.33333333333333337, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Object": {"pr": 0.7777777777777778, "re": 0.6363636363636364, "f": 0.7000000000000001, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 7.0}}, "caption": "The image shows a man in a baseball uniform holding a bat and throwing water on the ground. The man is standing on a baseball field with other players in the background. The image is in black and white.\n\nThe image shows a man in a baseball uniform holding a bat and throwing water on the ground. The"}, "455261": {"image_id": 455261, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.22228757208608238, "Bleu_3": 0.182219637500742, "Bleu_4": 0.1584450133694868, "METEOR": 0.3606851503093324, "ROUGE_L": 0.3122200895713372, "CIDEr": 9.478049084494364e-10, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.21739130434782608, "f": 0.22222222222222224, "fn": 18.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.25, "f": 0.18181818181818182, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a person riding a motorcycle down a road. The person is wearing a helmet and has their hands on the handlebars. The road is straight and there are trees on either side. The sky is clear and the sun is shining. The image is in black and white."}, "107964": {"image_id": 107964, "Bleu_1": 0.24999999999431824, "Bleu_2": 0.15249857032909853, "Bleu_3": 0.1034595892554256, "Bleu_4": 0.07209117403210415, "METEOR": 0.15877155989960795, "ROUGE_L": 0.214185393258427, "CIDEr": 5.0607950403224016e-08, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.13636363636363635, "f": 0.12244897959183673, "fn": 19.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3333333333333333, "f": 0.27272727272727276, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a skateboarder performing a trick on a halfpipe at sunset. The skateboarder is wearing a black shirt and jeans, and has his arms outstretched as he jumps off the halfpipe. The halfpipe is made of concrete and has a smooth surface"}, "292363": {"image_id": 292363, "Bleu_1": 0.34146341462581803, "Bleu_2": 0.2771809306010938, "Bleu_3": 0.2143601397219743, "Bleu_4": 0.16699075879047595, "METEOR": 0.30711126685934587, "ROUGE_L": 0.39638989169675093, "CIDEr": 2.419375343177793e-06, "SPICE": {"All": {"pr": 0.1935483870967742, "re": 0.2222222222222222, "f": 0.20689655172413793, "fn": 21.0, "numImages": 1.0, "fp": 25.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.5, "f": 0.4615384615384615, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The image shows a slice of pizza on a plate with a fork and knife on the side. The pizza appears to be topped with cheese, pepperoni, and other toppings. The plate is on a wooden table with a white tablecloth."}, "233848": {"image_id": 233848, "Bleu_1": 0.2264150943353507, "Bleu_2": 0.11429089766174176, "Bleu_3": 0.06350643060633274, "Bleu_4": 8.460008059351496e-06, "METEOR": 0.17945445070014787, "ROUGE_L": 0.2919547215680316, "CIDEr": 1.2739551541187307e-08, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.16666666666666666, "f": 0.1904761904761905, "fn": 20.0, "numImages": 1.0, "fp": 14.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4, "f": 0.4210526315789474, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a small town with several buildings, including a church, a school, and a few houses. The sky is cloudy and there are some trees in the background.\n\nThe town is surrounded by a fence and there is a small flagpole in the center of the town. The buildings are made"}, "480179": {"image_id": 480179, "Bleu_1": 0.2264150943353507, "Bleu_2": 0.1475489144223407, "Bleu_3": 0.11952367853238388, "Bleu_4": 0.10060701777161159, "METEOR": 0.17942458447604684, "ROUGE_L": 0.2633567188343227, "CIDEr": 6.167413168361301e-10, "SPICE": {"All": {"pr": 0.07407407407407407, "re": 0.1111111111111111, "f": 0.08888888888888888, "fn": 16.0, "numImages": 1.0, "fp": 25.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.3333333333333333, "f": 0.23529411764705885, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a stack of old suitcases on the sidewalk. The suitcases are made of wood and have a worn, weathered appearance. They are stacked on top of each other, with some of them open and others closed. There are some small plants growing out of the cracks in the concrete side"}, "499727": {"image_id": 499727, "Bleu_1": 0.3207547169750801, "Bleu_2": 0.1756183353427233, "Bleu_3": 0.08456484271609976, "Bleu_4": 1.0486966556129234e-05, "METEOR": 0.17419646746502987, "ROUGE_L": 0.2334609075997813, "CIDEr": 7.156605846924201e-11, "SPICE": {"All": {"pr": 0.05555555555555555, "re": 0.05555555555555555, "f": 0.05555555555555555, "fn": 17.0, "numImages": 1.0, "fp": 17.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}}, "caption": "The image shows a group of people flying kites in the sky. They are all wearing different colored clothing and have different expressions on their faces. The sky is cloudy and there are some birds flying in the background.\n\nThe image is taken from a low angle, looking up at the kite fly"}, "226220": {"image_id": 226220, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.22778672580037096, "Bleu_3": 0.19239549383000237, "Bleu_4": 0.15449650836451262, "METEOR": 0.2773580108410874, "ROUGE_L": 0.2959369314736203, "CIDEr": 1.0656974327704027e-11, "SPICE": {"All": {"pr": 0.07142857142857142, "re": 0.07692307692307693, "f": 0.07407407407407408, "fn": 24.0, "numImages": 1.0, "fp": 26.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2222222222222222, "f": 0.18181818181818185, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a cat sleeping on a couch in a living room. The cat is lying on its side with its head resting on the arm of the couch. The cat's eyes are closed and its paws are tucked under its body. The room is well lit with natural light coming"}, "375211": {"image_id": 375211, "Bleu_1": 0.2444444444390124, "Bleu_2": 0.1666666666629209, "Bleu_3": 8.6445624194464e-07, "Bleu_4": 1.9803636141572585e-09, "METEOR": 0.15157740506652978, "ROUGE_L": 0.2053872053872054, "CIDEr": 9.873752621123387e-09, "SPICE": {"All": {"pr": 0.07407407407407407, "re": 0.1111111111111111, "f": 0.08888888888888888, "fn": 16.0, "numImages": 1.0, "fp": 25.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2222222222222222, "f": 0.1904761904761905, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a bowl of red sauce with a spoon in it. The sauce is made with tomato sauce, garlic, and olive oil. The spoon is made of stainless steel and has a red handle. The bowl is made of ceramic and has a"}, "534669": {"image_id": 534669, "Bleu_1": 0.27999999999440006, "Bleu_2": 0.15118578920063636, "Bleu_3": 0.07808966656031394, "Bleu_4": 1.0032766664400675e-05, "METEOR": 0.20255189602916793, "ROUGE_L": 0.23252858958068615, "CIDEr": 1.9078622286119503e-09, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.16666666666666666, "f": 0.186046511627907, "fn": 20.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.4444444444444444, "f": 0.47058823529411764, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 4.0}}, "caption": "The image shows a large airplane parked on the tarmac at an airport. The plane has a white and blue paint job with red and white stripes on the tail. The plane is parked next to a large building with a sign that reads \"airport\". There are several other airplanes"}, "568425": {"image_id": 568425, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.15070298814247132, "Bleu_3": 0.12043890313249785, "Bleu_4": 0.07650371936962706, "METEOR": 0.21693938167860288, "ROUGE_L": 0.25326215895610915, "CIDEr": 6.579473700630177e-13, "SPICE": {"All": {"pr": 0.13636363636363635, "re": 0.13636363636363635, "f": 0.13636363636363635, "fn": 19.0, "numImages": 1.0, "fp": 19.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a stop sign on the side of the road. The sign has a red and white background with the words \"stop\" written in black letters. The sign is surrounded by trees and houses in the background.\n\nThe image is taken from a bird's eye view, looking down on the street."}, "146541": {"image_id": 146541, "Bleu_1": 0.2857142857074831, "Bleu_2": 0.16695677422191285, "Bleu_3": 8.865761273541133e-07, "Bleu_4": 2.0559894081168643e-09, "METEOR": 0.1556568826170604, "ROUGE_L": 0.25957446808510637, "CIDEr": 1.1608095253240245e-06, "SPICE": {"All": {"pr": 0.12121212121212122, "re": 0.17391304347826086, "f": 0.14285714285714288, "fn": 19.0, "numImages": 1.0, "fp": 29.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.2727272727272727, "f": 0.23999999999999996, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a zebra lying on the ground in the shade of a tree. The zebra has a long mane and a stripe on its back. The zebra is looking up at the sky. The image is taken in a zoo."}, "240323": {"image_id": 240323, "Bleu_1": 0.28571428570845486, "Bleu_2": 0.18898223650071672, "Bleu_3": 0.09125318612878565, "Bleu_4": 1.1336958836408244e-05, "METEOR": 0.24320830398591126, "ROUGE_L": 0.3010487353485503, "CIDEr": 4.937790250470434e-10, "SPICE": {"All": {"pr": 0.23809523809523808, "re": 0.2777777777777778, "f": 0.25641025641025644, "fn": 13.0, "numImages": 1.0, "fp": 16.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.8333333333333334, "f": 0.5263157894736842, "fn": 1.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a plate with a slice of turkey, carrots, and an orange on it. The plate is on a table with a white background. The turkey is sliced in half and there are carrot sticks on the side of the plate. The orange is sliced in half"}, "343954": {"image_id": 343954, "Bleu_1": 0.2264150943353507, "Bleu_2": 0.174582229926461, "Bleu_3": 0.144034534600296, "Bleu_4": 0.12434332373336152, "METEOR": 0.27648740335919986, "ROUGE_L": 0.3402119353039598, "CIDEr": 6.61766339800671e-11, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.1875, "f": 0.19999999999999998, "fn": 26.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.38461538461538464, "f": 0.38461538461538464, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a group of people standing on a hillside, looking up at a kite flying in the sky. The people are dressed in casual clothing and are standing in a line, looking up at the kite. The kite is made of white and purple fabric and has a long tail that"}, "359203": {"image_id": 359203, "Bleu_1": 0.3090909090852893, "Bleu_2": 0.16917307297107978, "Bleu_3": 0.10259799351869228, "Bleu_4": 0.06750770045883794, "METEOR": 0.2500722040652839, "ROUGE_L": 0.24610951008645532, "CIDEr": 1.884576389615579e-13, "SPICE": {"All": {"pr": 0.125, "re": 0.17391304347826086, "f": 0.14545454545454545, "fn": 19.0, "numImages": 1.0, "fp": 28.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.26666666666666666, "re": 0.4, "f": 0.32, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "The image shows a bench made of wood sitting in front of a stone building with large windows and a steeple on top. The building appears to be old and has a lot of stone detailing on it. The bench is old and has a lot of wear and tear on it. The grass is"}, "496541": {"image_id": 496541, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.14254579382569355, "Bleu_3": 7.358334830308166e-07, "Bleu_4": 1.6801271827773928e-09, "METEOR": 0.20394033921951457, "ROUGE_L": 0.25341246290801184, "CIDEr": 5.353650854526112e-12, "SPICE": {"All": {"pr": 0.15, "re": 0.125, "f": 0.13636363636363635, "fn": 21.0, "numImages": 1.0, "fp": 17.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3, "f": 0.3, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a sheep and a lamb standing in a barn. The sheep is brown and white, with a black and white face. The lamb is black and white, with a white face. The barn is made of wood and has a wooden roof. There are bales of hay in the background"}, "499835": {"image_id": 499835, "Bleu_1": 0.19230769230399414, "Bleu_2": 1.9418390934138346e-09, "Bleu_3": 4.224923263061476e-12, "Bleu_4": 1.9806835695200343e-13, "METEOR": 0.12446958981612447, "ROUGE_L": 0.11268472906403942, "CIDEr": 6.6677494595284835e-12, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.18518518518518517, "f": 0.17857142857142858, "fn": 22.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.06666666666666667, "re": 0.09090909090909091, "f": 0.07692307692307691, "fn": 10.0, "numImages": 1.0, "fp": 14.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.2727272727272727, "f": 0.28571428571428564, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a truck parked on the side of a road at night. The truck is black and has a large logo on the side. There are buildings in the background, but they are not visible because of the darkness.\n\nThe image is taken at night, and the streetlights are on"}, "258388": {"image_id": 258388, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.14819171472405493, "Bleu_3": 0.07652745233046604, "Bleu_4": 9.829979956142771e-06, "METEOR": 0.12539163514773324, "ROUGE_L": 0.1958266452648475, "CIDEr": 3.80460976432445e-08, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.1724137931034483, "f": 0.17543859649122806, "fn": 24.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.1, "re": 0.1, "f": 0.10000000000000002, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.1111111111111111, "f": 0.13333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows two people standing in a room with a skateboard ramp in the background. One person is holding a lighter and the other is holding a skateboard. The room appears to be a warehouse or garage with exposed brick walls and a high ceiling. There are several boxes and"}, "772": {"image_id": 772, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.15639188177115004, "Bleu_3": 0.07776932968046642, "Bleu_4": 9.799721254702973e-06, "METEOR": 0.19347881240820114, "ROUGE_L": 0.1862026862026862, "CIDEr": 5.12375491875265e-13, "SPICE": {"All": {"pr": 0.1388888888888889, "re": 0.22727272727272727, "f": 0.1724137931034483, "fn": 17.0, "numImages": 1.0, "fp": 31.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1, "re": 0.125, "f": 0.11111111111111112, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5, "f": 0.380952380952381, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows three sheep standing in a field with an orange fence in the background. The sheep are white with black and brown patches on their wool. They are standing on grass and looking at the camera. The fence is made of orange ropes and has a sign on it that says \"Sheep"}, "563702": {"image_id": 563702, "Bleu_1": 0.3157894736759004, "Bleu_2": 0.20657747505062754, "Bleu_3": 0.13334123550086785, "Bleu_4": 1.6132673522067715e-05, "METEOR": 0.18020638604698105, "ROUGE_L": 0.1829085457271364, "CIDEr": 1.0898118470280379e-05, "SPICE": {"All": {"pr": 0.12, "re": 0.13636363636363635, "f": 0.1276595744680851, "fn": 19.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The fire trucks are parked on the side of the road. There are several firefighters standing around the trucks, looking at the scene. The firefighters are wearing their fire gear, including helmets, coats, and pants. The trucks are"}, "346275": {"image_id": 346275, "Bleu_1": 0.2653061224435652, "Bleu_2": 0.16624095289858323, "Bleu_3": 0.08377724227293193, "Bleu_4": 1.063298221330347e-05, "METEOR": 0.22824288742036397, "ROUGE_L": 0.2208811104405552, "CIDEr": 7.400752245053515e-10, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.15384615384615385, "f": 0.16666666666666669, "fn": 22.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.3333333333333333, "f": 0.32, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a person jumping off a skateboard on a ramp. The person is wearing a black shirt and jeans, and has a black and white skateboard. The ramp is made of concrete and has a metal railing on the side. There are trees and buildings in the"}, "302222": {"image_id": 302222, "Bleu_1": 0.27999999999440006, "Bleu_2": 0.19999999999595922, "Bleu_3": 0.14938015821552278, "Bleu_4": 0.12077457989654793, "METEOR": 0.22996722992942895, "ROUGE_L": 0.2594167679222357, "CIDEr": 9.701415365284243e-10, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.21428571428571427, "f": 0.21052631578947367, "fn": 22.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.46153846153846156, "re": 0.5, "f": 0.48000000000000004, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a group of people sitting on benches in a public space. They are all wearing sunglasses and hats, and one of them is holding a book. The benches are made of metal and have umbrellas on top of them. There are trees and buildings in the background"}, "334327": {"image_id": 334327, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.20597146021399534, "Bleu_3": 0.14738933945193888, "Bleu_4": 0.08858266023053885, "METEOR": 0.23857065872941496, "ROUGE_L": 0.21542083578575633, "CIDEr": 7.464218670860001e-13, "SPICE": {"All": {"pr": 0.13793103448275862, "re": 0.125, "f": 0.13114754098360656, "fn": 28.0, "numImages": 1.0, "fp": 25.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.3333333333333333, "f": 0.32, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a tennis player hitting a tennis ball with a racket on a tennis court. The player is wearing a red shirt and white shorts, and has a red headband on his head. The ball is flying through the air and the player is running towards it. The background is a blue sky"}, "21711": {"image_id": 21711, "Bleu_1": 0.2553191489307379, "Bleu_2": 0.19711142525707728, "Bleu_3": 0.15115540850568024, "Bleu_4": 0.11193401922301613, "METEOR": 0.26478180518209676, "ROUGE_L": 0.317915309446254, "CIDEr": 4.045336479750343e-09, "SPICE": {"All": {"pr": 0.09375, "re": 0.1875, "f": 0.125, "fn": 13.0, "numImages": 1.0, "fp": 29.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.2, "f": 0.14285714285714285, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.3333333333333333, "f": 0.2222222222222222, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a group of people riding motorcycles on a dirt road. They are wearing helmets and riding gear, and one person is holding a cell phone. The background is a grassy field with trees in the distance.\n\nThe image is taken in a rural area,"}, "146155": {"image_id": 146155, "Bleu_1": 0.18518518518175586, "Bleu_2": 0.10238251947041137, "Bleu_3": 0.05863398711091721, "Bleu_4": 7.929020238697525e-06, "METEOR": 0.15476343572922532, "ROUGE_L": 0.19038701622971285, "CIDEr": 8.73814281158972e-12, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.25, "f": 0.23529411764705882, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.6, "re": 0.3, "f": 0.4, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 3.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.4, "f": 0.5714285714285715, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of people standing at a bar in a restaurant. They are all wearing casual clothing and are holding glasses of wine. The bar has a wooden counter and a large selection of bottles behind it. There are also several tables and chairs in the background. The atmosphere is co"}, "507187": {"image_id": 507187, "Bleu_1": 0.27999999999440006, "Bleu_2": 0.19999999999595922, "Bleu_3": 0.11856311014724848, "Bleu_4": 1.3722641665543406e-05, "METEOR": 0.18682896779187994, "ROUGE_L": 0.27128335451080055, "CIDEr": 4.3937464322752765e-11, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.16666666666666666, "f": 0.15686274509803924, "fn": 20.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.125, "f": 0.2, "fn": 7.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}}, "caption": "The image shows a group of people standing around a green motorcycle on the grass\n\nThe motorcycle is a green and white 1950s style motorcycle with a white seat and a black engine. The people are standing around it, looking at it. There are several people in the background, some"}, "328352": {"image_id": 328352, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.17504208247906033, "Bleu_3": 0.1322380251708152, "Bleu_4": 0.09711223048924789, "METEOR": 0.227461743820155, "ROUGE_L": 0.33098209441128595, "CIDEr": 1.1750503308412182e-11, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.10714285714285714, "f": 0.14634146341463414, "fn": 25.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.3333333333333333, "f": 0.4, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 3.0}}, "caption": "The image shows a group of people sitting on a bench in front of a body of water. The people are dressed in casual clothing and are looking out at the water. There are boats in the distance and mountains in the background. The sky is clear and blue.\n\nThe image is taken from a"}, "92488": {"image_id": 92488, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.1211406307163407, "Bleu_3": 6.559319301112625e-07, "Bleu_4": 1.5337392624752068e-09, "METEOR": 0.15513956945574953, "ROUGE_L": 0.1783625730994152, "CIDEr": 4.4930752261795623e-13, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.21739130434782608, "f": 0.19999999999999998, "fn": 18.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.2857142857142857, "f": 0.30769230769230765, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a busy street with tall buildings on either side. The buildings are made of glass and steel, and have a modern design. There are cars and pedestrians on the street, and a clock tower in the distance. The sky is clear and blue, with a few clouds scattered across it. The"}, "256657": {"image_id": 256657, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.21271781490162775, "Bleu_3": 0.15354390128889114, "Bleu_4": 0.12201289225771661, "METEOR": 0.3140191267066254, "ROUGE_L": 0.3596698113207547, "CIDEr": 2.790199495842651e-10, "SPICE": {"All": {"pr": 0.23333333333333334, "re": 0.5384615384615384, "f": 0.32558139534883723, "fn": 6.0, "numImages": 1.0, "fp": 23.0, "tp": 7.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.4, "f": 0.23529411764705882, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.8333333333333334, "f": 0.5882352941176471, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a dog playing fetch on a grassy field. The dog is running towards the camera with a frisbee in its mouth. The sky is clear and blue, with some clouds in the distance. The grass is green and well maintained. The field is surrounded by trees and a fence."}, "560993": {"image_id": 560993, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.221323371633511, "Bleu_3": 0.15662032595896783, "Bleu_4": 0.11133996756281149, "METEOR": 0.228939250373439, "ROUGE_L": 0.25894481503941785, "CIDEr": 8.890896648198624e-11, "SPICE": {"All": {"pr": 0.125, "re": 0.11538461538461539, "f": 0.12000000000000001, "fn": 23.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2727272727272727, "f": 0.2608695652173913, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a man sitting at a desk in front of a television. The man is wearing a white shirt and black pants. The television is playing a video game. There are several other objects on the desk, including a computer and a book. The room is dimly lit and there are"}, "212384": {"image_id": 212384, "Bleu_1": 0.27272727272107444, "Bleu_2": 0.19507682662422146, "Bleu_3": 0.12191693142366875, "Bleu_4": 1.4499467512222074e-05, "METEOR": 0.2691096687449721, "ROUGE_L": 0.2924657534246575, "CIDEr": 9.096587460955801e-08, "SPICE": {"All": {"pr": 0.15, "re": 0.10714285714285714, "f": 0.125, "fn": 25.0, "numImages": 1.0, "fp": 17.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3, "f": 0.3, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image is a cake with a zebra and a giraffe on it. The cake is brown and has a zebra and a giraffe on it. The zebra and giraffe are made of fondant and are brown in color. The cake is decorated with"}, "48332": {"image_id": 48332, "Bleu_1": 0.15686274509496353, "Bleu_2": 0.07921180343656528, "Bleu_3": 5.04035633661648e-07, "Bleu_4": 1.2780140290443053e-09, "METEOR": 0.15499752492329127, "ROUGE_L": 0.18252543387193296, "CIDEr": 2.2891616122576412e-11, "SPICE": {"All": {"pr": 0.15, "re": 0.11538461538461539, "f": 0.13043478260869565, "fn": 23.0, "numImages": 1.0, "fp": 17.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.42857142857142855, "f": 0.375, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a group of people standing around a table with a cake on it. The cake has a red, white, and blue ribbon around it. The people are wearing military uniforms and are holding their hands together in a salute. The room is dimly lit and there are chairs"}, "443313": {"image_id": 443313, "Bleu_1": 0.31481481480898493, "Bleu_2": 0.17233546016556184, "Bleu_3": 0.10453426005393707, "Bleu_4": 0.06879413955125327, "METEOR": 0.22752914380159056, "ROUGE_L": 0.32275132275132273, "CIDEr": 1.5488823980715415e-10, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.21739130434782608, "f": 0.20408163265306123, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.3333333333333333, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.4, "re": 0.5, "f": 0.4444444444444445, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a man playing tennis on a blue court with a crowd of people watching from the stands. The man is wearing white clothing and has a white racket in his hand. The crowd is made up of people of different ages and races, all of whom are wearing casual clothing. The"}, "262476": {"image_id": 262476, "Bleu_1": 0.28571428570845486, "Bleu_2": 0.23145502493836512, "Bleu_3": 0.18981427626843136, "Bleu_4": 0.15616086103999416, "METEOR": 0.316680961188751, "ROUGE_L": 0.32253800396563115, "CIDEr": 1.022770134308372e-09, "SPICE": {"All": {"pr": 0.09375, "re": 0.14285714285714285, "f": 0.11320754716981132, "fn": 18.0, "numImages": 1.0, "fp": 29.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.09090909090909091, "re": 0.14285714285714285, "f": 0.1111111111111111, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 1.0, "f": 0.33333333333333337, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Object": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a group of people standing around a table in a kitchen. They are all smiling and laughing. There are several bottles of beer on the table.\n\nThe image is well lit and the colors are vibrant. The people in the image are all wearing casual clothing"}, "312213": {"image_id": 312213, "Bleu_1": 0.2105263157857803, "Bleu_2": 0.10619884880883856, "Bleu_3": 0.05896925524327533, "Bleu_4": 7.850020523433245e-06, "METEOR": 0.17934282460915912, "ROUGE_L": 0.23448654585392636, "CIDEr": 6.42097650061931e-14, "SPICE": {"All": {"pr": 0.13793103448275862, "re": 0.18181818181818182, "f": 0.1568627450980392, "fn": 18.0, "numImages": 1.0, "fp": 25.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a cat sitting on top of a laptop computer. The cat is looking at the screen with its eyes. The laptop has a keyboard and a mouse on it. The cat is wearing a collar with a tag on it. The background is a light gray wall with a window in the background. The"}, "444366": {"image_id": 444366, "Bleu_1": 0.2264150943353507, "Bleu_2": 0.174582229926461, "Bleu_3": 0.13370981749106947, "Bleu_4": 0.09888660568274774, "METEOR": 0.29620289311077797, "ROUGE_L": 0.2959369314736203, "CIDEr": 1.480803500486761e-11, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.16666666666666666, "f": 0.15686274509803924, "fn": 20.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.1111111111111111, "f": 0.125, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a wooden chair with a wicker seat and backrest. The chair is placed in front of a window with a view of the sky. There is a sheaf of dried grass on the floor next to the chair. The room is well lit by natural light coming through the window."}, "13061": {"image_id": 13061, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.15861031714066393, "Bleu_3": 0.12461581769751784, "Bleu_4": 0.09333506484889645, "METEOR": 0.22801647042381512, "ROUGE_L": 0.28367380833748546, "CIDEr": 7.962517894190533e-12, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.13793103448275862, "f": 0.14035087719298248, "fn": 25.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of remote controls on a wooden table. The remote controls are different colors and have different buttons on them. There is a television in the background.\n\nThe image is taken in a living room. The remote controls are on a wooden table. There is a television in the background."}, "414501": {"image_id": 414501, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.15041420939612582, "Bleu_3": 0.0767719506444456, "Bleu_4": 9.802862511748142e-06, "METEOR": 0.17141147901198098, "ROUGE_L": 0.21131639722863746, "CIDEr": 5.633677791434637e-10, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.3333333333333333, "f": 0.2608695652173913, "fn": 12.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.125, "re": 0.1, "f": 0.11111111111111112, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.7142857142857143, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a store with various products on display, including computers, phones, and other electronics. The store is well lit and has a large selection of products. The people in the store are looking at the products on display.\n\nThe image shows a store with various products on display, including computers,"}, "209346": {"image_id": 209346, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.17831418028047516, "Bleu_3": 0.12241366236264786, "Bleu_4": 1.3771456494365001e-05, "METEOR": 0.265400577487812, "ROUGE_L": 0.24970760233918127, "CIDEr": 1.774136262853523e-12, "SPICE": {"All": {"pr": 0.04, "re": 0.02702702702702703, "f": 0.03225806451612903, "fn": 36.0, "numImages": 1.0, "fp": 24.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 17.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.09090909090909091, "re": 0.07692307692307693, "f": 0.08333333333333334, "fn": 12.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}}, "caption": "The image shows a woman standing in front of a large red suitcase. She is wearing a white dress and has her hands on the handle of the suitcase. The suitcase is on the ground in front of her. There is a window in the background with a view of the city.\n\nThe image"}, "190014": {"image_id": 190014, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.21271781490162775, "Bleu_3": 0.12186787521464361, "Bleu_4": 1.3863341114193003e-05, "METEOR": 0.23715052331714892, "ROUGE_L": 0.28773584905660377, "CIDEr": 3.6735474410017964e-11, "SPICE": {"All": {"pr": 0.06451612903225806, "re": 0.09090909090909091, "f": 0.07547169811320754, "fn": 20.0, "numImages": 1.0, "fp": 29.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2222222222222222, "f": 0.1904761904761905, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a person standing on top of a snow covered mountain. They are wearing a black jacket and black pants. They are holding a snowboard in their hand. The background is a snow covered mountain with trees in the distance.\n\nThe person in the image is wearing a black jacket"}, "364853": {"image_id": 364853, "Bleu_1": 0.28571428570845486, "Bleu_2": 0.20412414522772235, "Bleu_3": 0.09606465633687986, "Bleu_4": 1.1782386561735055e-05, "METEOR": 0.21263909749359836, "ROUGE_L": 0.26940063091482647, "CIDEr": 4.218706093366953e-10, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.23529411764705882, "f": 0.19999999999999998, "fn": 13.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a man grilling hot dogs on a barbecue grill. The man is wearing a white shirt and blue jeans, and he is using a spatula to flip the hot dogs. The grill is made of metal and has a black handle. The hot dogs are cook"}, "453485": {"image_id": 453485, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.1456862718139933, "Bleu_3": 0.0761842752874376, "Bleu_4": 9.848600952552255e-06, "METEOR": 0.24925629903713833, "ROUGE_L": 0.2776332899869961, "CIDEr": 7.265135501947685e-11, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.19047619047619047, "f": 0.2, "fn": 17.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.375, "f": 0.375, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "This image shows an older woman in a kitchen preparing tea in a teapot. She is wearing a white apron and has a smile on her face. The kitchen is well lit and has a stove, sink, and refrigerator. The walls are painted white and there are no windows in"}, "47648": {"image_id": 47648, "Bleu_1": 0.3947368420948754, "Bleu_2": 0.230960638416735, "Bleu_3": 0.11400515270331847, "Bleu_4": 1.4344200576586565e-05, "METEOR": 0.24328564367912658, "ROUGE_L": 0.28249459709786967, "CIDEr": 7.985290402026723e-06, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.21052631578947367, "f": 0.186046511627907, "fn": 15.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.42857142857142855, "f": 0.375, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image is a bowl of vegetables, including broccoli, carrots, and cucumber. There are also slices of lemon in the bowl. The bowl is on a table with a white tablecloth.\n\n\nThe image is a bowl of vegetables,"}, "204650": {"image_id": 204650, "Bleu_1": 0.26785714285235973, "Bleu_2": 0.15604694598027796, "Bleu_3": 0.0766841478193614, "Bleu_4": 9.604178638196449e-06, "METEOR": 0.15597082353735145, "ROUGE_L": 0.22426470588235295, "CIDEr": 2.367289993778744e-12, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.20833333333333334, "f": 0.2127659574468085, "fn": 19.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.14285714285714285, "f": 0.18181818181818182, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5, "f": 0.4210526315789474, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a large military plane on the runway with people standing around it\n\nThe plane is white with a large red stripe on the side and has a large propeller on the front. There are several people standing around the plane, including one person in a yellow jacket and another in a red jack"}, "362696": {"image_id": 362696, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.1189176779999305, "Bleu_3": 0.06437822382780045, "Bleu_4": 8.463566423155675e-06, "METEOR": 0.17146333036180225, "ROUGE_L": 0.17951736315479697, "CIDEr": 1.585507048112249e-12, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.19047619047619047, "f": 0.18604651162790697, "fn": 17.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.4, "f": 0.4000000000000001, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a street sign with a no parking sign on it. The sign is mounted on a pole in the middle of the street. There are buildings on either side of the street, with windows and doors on them. The sky is clear and blue.\n\nThe image is taken from a high angle,"}, "477741": {"image_id": 477741, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.1447907475849832, "Bleu_3": 0.10654502541910188, "Bleu_4": 0.06978423773350045, "METEOR": 0.24065255840704203, "ROUGE_L": 0.2738496071829405, "CIDEr": 2.119037579070969e-11, "SPICE": {"All": {"pr": 0.2, "re": 0.2777777777777778, "f": 0.23255813953488372, "fn": 13.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.16666666666666666, "f": 0.13333333333333333, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5714285714285714, "f": 0.4444444444444444, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a brick building with graffiti on the walls. There is a stop sign on the side of the building. The building appears to be old and has been abandoned for some time. The graffiti on the walls is colorful and appears to be from different artists. The stop sign is faded"}, "413616": {"image_id": 413616, "Bleu_1": 0.24999999999479172, "Bleu_2": 0.17864740024886272, "Bleu_3": 0.1115377437347146, "Bleu_4": 0.07451835561525198, "METEOR": 0.2543956041719247, "ROUGE_L": 0.28696236559139787, "CIDEr": 6.114716714245756e-10, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.23076923076923078, "f": 0.22641509433962265, "fn": 20.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.3333333333333333, "f": 0.30769230769230765, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The dog is jumping up to catch a frisbee in the air. The dog is a black and white terrier with a red collar. The dog is jumping up to catch a frisbee in the air. The dog is a black and white terrier with a red collar."}, "146126": {"image_id": 146126, "Bleu_1": 0.3199999999936, "Bleu_2": 0.21380899352561972, "Bleu_3": 0.12395961884568825, "Bleu_4": 1.418847468492011e-05, "METEOR": 0.24244713941485194, "ROUGE_L": 0.26228501228501233, "CIDEr": 2.674465581479765e-07, "SPICE": {"All": {"pr": 0.24, "re": 0.2857142857142857, "f": 0.2608695652173913, "fn": 15.0, "numImages": 1.0, "fp": 19.0, "tp": 6.0}, "Relation": {"pr": 0.2, "re": 0.2222222222222222, "f": 0.2105263157894737, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5, "f": 0.36363636363636365, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a table with a television on it, a remote control, and a bag of food. The food appears to be french fries and a drink. There is also a cup of coffee on the table. The background is a white wall with a window in the background."}, "325331": {"image_id": 325331, "Bleu_1": 0.2075471698074048, "Bleu_2": 0.16714975946414357, "Bleu_3": 0.13991698775013336, "Bleu_4": 0.12166770939446522, "METEOR": 0.26981786140371167, "ROUGE_L": 0.28785708266621723, "CIDEr": 1.1391296886069898e-11, "SPICE": {"All": {"pr": 0.07142857142857142, "re": 0.07407407407407407, "f": 0.07272727272727272, "fn": 25.0, "numImages": 1.0, "fp": 26.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a black and white cat sleeping on top of a keyboard. The cat is lying on its back with its paws tucked under its body. The keyboard has a few keys pressed down and the cat's paws are resting on top of the keys. The cat's eyes are"}, "4286": {"image_id": 4286, "Bleu_1": 0.36734693876801333, "Bleu_2": 0.26244532957850014, "Bleu_3": 0.1638200413805792, "Bleu_4": 1.758271009782175e-05, "METEOR": 0.2552772874919791, "ROUGE_L": 0.26940063091482647, "CIDEr": 1.0070408319206353e-09, "SPICE": {"All": {"pr": 0.09375, "re": 0.25, "f": 0.13636363636363635, "fn": 9.0, "numImages": 1.0, "fp": 29.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.6, "f": 0.3, "fn": 2.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}}, "caption": "The image shows a giraffe standing in a field of tall grass. The giraffe is wearing a collar and has a long neck. The man in the image is standing next to the giraffe and looking at it. The background is a dense forest with tall trees and greenery."}, "515303": {"image_id": 515303, "Bleu_1": 0.1960784313687044, "Bleu_2": 0.10846522890718008, "Bleu_3": 0.06215293828145826, "Bleu_4": 8.409805259069613e-06, "METEOR": 0.14713700282114317, "ROUGE_L": 0.20998278829604128, "CIDEr": 4.0075052807313717e-10, "SPICE": {"All": {"pr": 0.125, "re": 0.1875, "f": 0.15, "fn": 13.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.42857142857142855, "f": 0.375, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a group of people standing in front of a horse-drawn carriage. The carriage is white with black wheels and has a red and white striped canopy. The people are dressed in various costumes, including a man in a white shirt and black pants, a woman in a red"}, "422677": {"image_id": 422677, "Bleu_1": 0.19999999999600004, "Bleu_2": 0.1428571428542566, "Bleu_3": 0.09473945732893252, "Bleu_4": 1.1597748989748828e-05, "METEOR": 0.22087984432761004, "ROUGE_L": 0.2594167679222357, "CIDEr": 1.0783857424178249e-10, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.13043478260869565, "f": 0.13043478260869565, "fn": 20.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.375, "f": 0.2727272727272727, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a young man in a white shirt and khaki pants standing on a skateboard in front of a brick wall with graffiti on it. The young man is holding onto the skateboard with his left hand and has his right hand in his pocket. The skateboard has"}, "169436": {"image_id": 169436, "Bleu_1": 0.3584905660309719, "Bleu_2": 0.2625652175482671, "Bleu_3": 0.1594685904285624, "Bleu_4": 0.09489948716012234, "METEOR": 0.270487639940353, "ROUGE_L": 0.25894481503941785, "CIDEr": 1.0153423090738476e-11, "SPICE": {"All": {"pr": 0.2, "re": 0.21739130434782608, "f": 0.20833333333333331, "fn": 18.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.07692307692307693, "f": 0.1, "fn": 12.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4444444444444444, "f": 0.39999999999999997, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a baseball player in a catcher's gear, standing on the field with his glove raised and a smile on his face. The player is wearing a black and white uniform with a white helmet and a black chest protector. The background is a green grass field with a d"}, "369386": {"image_id": 369386, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.25556221638112636, "Bleu_3": 0.17238297400489147, "Bleu_4": 0.11964258135320571, "METEOR": 0.2662845486792264, "ROUGE_L": 0.25894481503941785, "CIDEr": 3.346530279245623e-11, "SPICE": {"All": {"pr": 0.25806451612903225, "re": 0.47058823529411764, "f": 0.3333333333333333, "fn": 9.0, "numImages": 1.0, "fp": 23.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.42857142857142855, "f": 0.375, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.4166666666666667, "re": 0.7142857142857143, "f": 0.5263157894736842, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image is of a toilet with a red and green striped seat and a white toilet paper holder on the wall. There is also a window on the wall with a view of the outside.\n\nThe toilet is in a small room with a white sink and a mirror on the wall"}, "343004": {"image_id": 343004, "Bleu_1": 0.24999999999479172, "Bleu_2": 0.19296124624292718, "Bleu_3": 0.09319532338930898, "Bleu_4": 1.1580903993958116e-05, "METEOR": 0.2551824571738634, "ROUGE_L": 0.28018372703412076, "CIDEr": 4.043575293176716e-09, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.26666666666666666, "f": 0.19047619047619047, "fn": 11.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.3333333333333333, "f": 0.18181818181818182, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.16666666666666666, "re": 1.0, "f": 0.2857142857142857, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Object": {"pr": 0.3, "re": 0.5, "f": 0.37499999999999994, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a person sitting on a couch with a banana on their lap. The person is wearing a purple shirt and has a purple pillow on their lap. The banana is yellow and has a brown spot on it. The background is a light brown color."}, "10693": {"image_id": 10693, "Bleu_1": 0.3265306122382341, "Bleu_2": 0.23328473740311126, "Bleu_3": 0.15144863020774949, "Bleu_4": 1.6577138055742593e-05, "METEOR": 0.2880991551800112, "ROUGE_L": 0.3727629855958097, "CIDEr": 5.678541730016213e-09, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.19047619047619047, "f": 0.2, "fn": 17.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.2, "re": 0.125, "f": 0.15384615384615385, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a woman standing in the rain holding an umbrella. She is wearing a black jacket and black pants. The umbrella is white and has a black handle. There is a car parked in the background.\n\nThe image is in black and white. The woman is standing"}, "210458": {"image_id": 210458, "Bleu_1": 0.22448979591378598, "Bleu_2": 0.13677530110522793, "Bleu_3": 0.07355956748968669, "Bleu_4": 9.644726515398138e-06, "METEOR": 0.20426843752094057, "ROUGE_L": 0.236281471917366, "CIDEr": 1.4489327683410074e-10, "SPICE": {"All": {"pr": 0.08, "re": 0.08695652173913043, "f": 0.08333333333333333, "fn": 21.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2222222222222222, "f": 0.1904761904761905, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a zebra standing in the middle of a fenced in area. The zebra is wearing a halter and has a tag on its neck. The fence is made of metal and has a gate that is open. There are trees and bushes in the background.\n\nThe"}, "397135": {"image_id": 397135, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.21718612137747484, "Bleu_3": 0.16553008316495676, "Bleu_4": 0.11548422250765457, "METEOR": 0.17721649358022842, "ROUGE_L": 0.25326215895610915, "CIDEr": 2.0300265481449504e-11, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.19230769230769232, "f": 0.20408163265306123, "fn": 21.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.09090909090909091, "f": 0.09999999999999999, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of people riding horses on a beach with a body of water in the background. The horses are pulling a cart with a person sitting in it. The people are wearing hats and the horses are wearing saddles. The sky is cloudy and there are some trees in the"}, "176793": {"image_id": 176793, "Bleu_1": 0.2916666666605903, "Bleu_2": 0.24911189772076184, "Bleu_3": 0.18894426063218775, "Bleu_4": 0.14562215827951555, "METEOR": 0.32516617557414035, "ROUGE_L": 0.34398496240601506, "CIDEr": 3.852753505124369e-08, "SPICE": {"All": {"pr": 0.2413793103448276, "re": 0.25, "f": 0.24561403508771928, "fn": 21.0, "numImages": 1.0, "fp": 22.0, "tp": 7.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.08333333333333333, "f": 0.08333333333333333, "fn": 11.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.5, "f": 0.45454545454545453, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a woman sitting on the back of a motorcycle in front of a restaurant. She is wearing a white shirt and black pants, and has a red and white plaid skirt on. The restaurant has a sign that reads, \"River Riders Harley Davidson\". The woman"}, "347179": {"image_id": 347179, "Bleu_1": 0.27999999999440006, "Bleu_2": 0.22677868380095453, "Bleu_3": 0.12892319893629803, "Bleu_4": 1.4612476989078169e-05, "METEOR": 0.26886870297088034, "ROUGE_L": 0.34078212290502796, "CIDEr": 6.776576279428405e-09, "SPICE": {"All": {"pr": 0.13793103448275862, "re": 0.19047619047619047, "f": 0.16, "fn": 17.0, "numImages": 1.0, "fp": 25.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4444444444444444, "f": 0.380952380952381, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "This is an image of a group of people standing in a field, looking at a flock of sheep. The people are wearing hats and sunglasses, and the sheep are standing in a pen. The sky is cloudy and there are trees in the background.\n\nThe people are standing in"}, "43734": {"image_id": 43734, "Bleu_1": 0.3207547169750801, "Bleu_2": 0.2077944166525388, "Bleu_3": 0.13643933968471528, "Bleu_4": 1.5012817512995878e-05, "METEOR": 0.18201013155222528, "ROUGE_L": 0.24811156304474144, "CIDEr": 9.263668602833651e-11, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.2727272727272727, "f": 0.2790697674418604, "fn": 16.0, "numImages": 1.0, "fp": 15.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.2222222222222222, "f": 0.30769230769230765, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5714285714285714, "f": 0.4444444444444444, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a black dog standing in the water, with its head sticking out of the waves. The dog is wearing a collar and tag on its neck. The background is a beach with sand and rocks. The sky is cloudy and there are no other objects in the image.\n\nThe image"}, "334074": {"image_id": 334074, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.18857036045053577, "Bleu_3": 0.14075988150176658, "Bleu_4": 0.10277184831881524, "METEOR": 0.30194179922401504, "ROUGE_L": 0.28355607205113303, "CIDEr": 4.95296047027247e-11, "SPICE": {"All": {"pr": 0.16, "re": 0.14814814814814814, "f": 0.15384615384615383, "fn": 23.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.1, "f": 0.13333333333333333, "fn": 9.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a group of zebras standing under a tree. They are all facing the same direction and appear to be looking at something in the distance. The zebras are black and white with distinctive stripes on their backs. The tree is a large oak with branches that stretch out in all"}, "473057": {"image_id": 473057, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.20866567588950136, "Bleu_3": 0.1622185893042646, "Bleu_4": 0.13594021798767564, "METEOR": 0.316680961188751, "ROUGE_L": 0.33292904790782296, "CIDEr": 1.3656633829947776e-10, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.2631578947368421, "f": 0.23809523809523808, "fn": 14.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.16666666666666666, "f": 0.14285714285714288, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.5, "f": 0.47058823529411764, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image is a room with a bed, desk, and chair. There is a window with curtains and a door with a window on it. The walls are painted white and there is a rug on the floor. The room is well lit and there are no objects in the room.\n\nThe image"}, "350444": {"image_id": 350444, "Bleu_1": 0.2941176470530566, "Bleu_2": 0.21693045781436016, "Bleu_3": 0.16870903059432618, "Bleu_4": 0.13162056532389085, "METEOR": 0.2750987538975253, "ROUGE_L": 0.26704190118824267, "CIDEr": 6.351550032908969e-11, "SPICE": {"All": {"pr": 0.25, "re": 0.30434782608695654, "f": 0.27450980392156865, "fn": 16.0, "numImages": 1.0, "fp": 21.0, "tp": 7.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.125, "f": 0.125, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.625, "f": 0.5263157894736842, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a group of young boys playing soccer on a field with a goal at the end. They are wearing black and white jerseys and black shorts, and one of them is kicking the ball towards the goal. There are several people watching from the sidelines, including some adult"}, "96549": {"image_id": 96549, "Bleu_1": 0.16071428571141586, "Bleu_2": 1.7094086468637668e-09, "Bleu_3": 3.782387425237037e-12, "Bleu_4": 1.7875390616780398e-13, "METEOR": 0.1473088364053734, "ROUGE_L": 0.1769141531322506, "CIDEr": 7.15534584778086e-13, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.10714285714285714, "f": 0.10909090909090909, "fn": 25.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.25, "f": 0.23076923076923075, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a large airplane taking off from a runway at night. The plane has a white and blue body with a red tail and white wings. It is surrounded by a runway with lights on either side. The sky is dark and there are no clouds in sight. The airplane is taking off from"}, "119233": {"image_id": 119233, "Bleu_1": 0.32608695651465036, "Bleu_2": 0.22522130822577494, "Bleu_3": 0.15122694444080373, "Bleu_4": 0.09470104449999896, "METEOR": 0.2892903036912942, "ROUGE_L": 0.2794502617801047, "CIDEr": 4.0788802662454157e-08, "SPICE": {"All": {"pr": 0.10526315789473684, "re": 0.07692307692307693, "f": 0.08888888888888889, "fn": 24.0, "numImages": 1.0, "fp": 17.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2222222222222222, "f": 0.23529411764705882, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The cat is lying on top of the laptop, which is open on the desk. The cat is pawing at the keyboard.\n\nThe room is filled with various office supplies, including a printer, a scanner, and a stapler. There is also a plant on the windowsill."}, "1083": {"image_id": 1083, "Bleu_1": 0.19230769230399414, "Bleu_2": 0.12281268769488378, "Bleu_3": 6.706647632268701e-07, "Bleu_4": 1.575179665789121e-09, "METEOR": 0.16026376793218416, "ROUGE_L": 0.22536945812807885, "CIDEr": 5.0040257675624665e-12, "SPICE": {"All": {"pr": 0.14705882352941177, "re": 0.18518518518518517, "f": 0.1639344262295082, "fn": 22.0, "numImages": 1.0, "fp": 29.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.45454545454545453, "f": 0.3846153846153846, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 5.0}}, "caption": "The image shows a herd of cows grazing in a green field. The cows are black and white and are standing in a line, looking at the camera. The field is surrounded by trees and hills in the background. The sky is clear and blue.\n\nThe image is taken from above, looking"}, "265725": {"image_id": 265725, "Bleu_1": 0.32608695651465036, "Bleu_2": 0.20851441405249152, "Bleu_3": 0.12549213105688165, "Bleu_4": 1.4641826518869363e-05, "METEOR": 0.295972597265206, "ROUGE_L": 0.33107191316146534, "CIDEr": 9.617923905614748e-09, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.2, "f": 0.19607843137254902, "fn": 20.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.125, "re": 0.125, "f": 0.125, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.125, "f": 0.125, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a baseball player in a black and white uniform, holding a bat and ready to swing at the ball. The player is standing on the field, with the umpire standing behind him. The field is green and there are spectators in the stands."}, "432150": {"image_id": 432150, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.1647705109111272, "Bleu_3": 0.08158237122518461, "Bleu_4": 1.0260020371425162e-05, "METEOR": 0.2828044331246527, "ROUGE_L": 0.32370283018867924, "CIDEr": 2.6974101926045544e-11, "SPICE": {"All": {"pr": 0.3103448275862069, "re": 0.34615384615384615, "f": 0.32727272727272727, "fn": 17.0, "numImages": 1.0, "fp": 20.0, "tp": 9.0}, "Relation": {"pr": 0.07692307692307693, "re": 0.09090909090909091, "f": 0.08333333333333334, "fn": 10.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5333333333333333, "re": 0.6666666666666666, "f": 0.5925925925925926, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 8.0}}, "caption": "The image shows a small kitchen with white cabinets, a stove, and a refrigerator. There is a table and chairs in the corner of the room. The floor is made of hardwood and there is a rug in the middle of the room. The walls are painted white and there are windows"}, "5503": {"image_id": 5503, "Bleu_1": 0.1481481481454047, "Bleu_2": 0.07476962054420824, "Bleu_3": 4.7549833788592774e-07, "Bleu_4": 1.2049505059232175e-09, "METEOR": 0.10940376839033412, "ROUGE_L": 0.14575866188769412, "CIDEr": 5.746894252957012e-13, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.10526315789473684, "f": 0.13333333333333333, "fn": 17.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.2857142857142857, "f": 0.3333333333333333, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}}, "caption": "The image shows a toilet with a seat and a handle on the side. The toilet is in a small bathroom with a sink and a showerhead on the wall. There is a window on the left side of the image and a door on the right side. The floor is made of tile"}, "261906": {"image_id": 261906, "Bleu_1": 0.13725490195809306, "Bleu_2": 0.07409585736202748, "Bleu_3": 0.04820927404299032, "Bleu_4": 6.950845485916395e-06, "METEOR": 0.1275012300832928, "ROUGE_L": 0.17867603983596952, "CIDEr": 3.782002721684188e-11, "SPICE": {"All": {"pr": 0.12, "re": 0.0967741935483871, "f": 0.10714285714285714, "fn": 28.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.23076923076923078, "f": 0.27272727272727276, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a white stork standing on a wooden fence in a green area. The stork has a long beak and long legs, and is wearing a black and white striped hat. The background is a green area with trees and a small pond.\n\nThe image is taken from a"}, "112110": {"image_id": 112110, "Bleu_1": 0.19230769230399414, "Bleu_2": 0.15041420939612582, "Bleu_3": 0.11072431282861321, "Bleu_4": 0.07254929979746852, "METEOR": 0.1562011374513858, "ROUGE_L": 0.1920654911838791, "CIDEr": 2.339412972166974e-11, "SPICE": {"All": {"pr": 0.26666666666666666, "re": 0.16666666666666666, "f": 0.2051282051282051, "fn": 20.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.09090909090909091, "f": 0.14285714285714288, "fn": 10.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.2857142857142857, "f": 0.30769230769230765, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}}, "caption": "The image shows a person standing on a sidewalk with a laptop bag on their back. They are wearing a pair of sneakers and have their hands in their pockets. The background is a dark blue sky with some clouds.\n\nThe person in the image is wearing a black shirt and je"}, "243849": {"image_id": 243849, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.1313579154832357, "Bleu_3": 0.08897106801212075, "Bleu_4": 0.06189082809605832, "METEOR": 0.23056066079801046, "ROUGE_L": 0.25553560742070613, "CIDEr": 6.42479253200377e-11, "SPICE": {"All": {"pr": 0.1, "re": 0.15, "f": 0.12, "fn": 17.0, "numImages": 1.0, "fp": 27.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.2, "f": 0.15384615384615385, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.25, "f": 0.2105263157894737, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a bacon cheeseburger with a fried egg on top. The burger is on a white plate with a red and white checkered tablecloth. There is a glass of orange juice on the side of the plate. The background is a white wall with a window in the background"}, "299100": {"image_id": 299100, "Bleu_1": 0.29629629629080934, "Bleu_2": 0.19782182158264086, "Bleu_3": 0.1311863860222524, "Bleu_4": 0.08156879226605328, "METEOR": 0.2698886081616105, "ROUGE_L": 0.3364854215918045, "CIDEr": 2.4056627204723572e-09, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.23809523809523808, "f": 0.22222222222222224, "fn": 16.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.1111111111111111, "f": 0.1818181818181818, "fn": 8.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.26666666666666666, "re": 0.5714285714285714, "f": 0.36363636363636365, "fn": 3.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "The image shows a hot dog on a plate with a drink next to it\n\nThe hot dog is topped with vegetables and sauce. The drink is a glass of water with ice cubes in it. The table is made of wood and has a blue and white checkered pattern on it. There are"}, "159790": {"image_id": 159790, "Bleu_1": 0.27083333332769105, "Bleu_2": 0.18594210949553755, "Bleu_3": 0.1311312829167715, "Bleu_4": 0.08413498161249076, "METEOR": 0.2730138891429154, "ROUGE_L": 0.3057644110275689, "CIDEr": 4.5719648535315726e-08, "SPICE": {"All": {"pr": 0.2, "re": 0.2222222222222222, "f": 0.2105263157894737, "fn": 21.0, "numImages": 1.0, "fp": 24.0, "tp": 6.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.125, "f": 0.10526315789473685, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.4166666666666667, "f": 0.4166666666666667, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a group of people standing on a skateboard ramp, watching a person perform a trick on the board. The ramp is made of metal and has a red and white striped design on it. The people are wearing various types of clothing, including t-shirts, h"}, "371322": {"image_id": 371322, "Bleu_1": 0.21999999999560005, "Bleu_2": 0.09476070829395405, "Bleu_3": 5.719241732016864e-07, "Bleu_4": 1.412470464555783e-09, "METEOR": 0.11950879382586282, "ROUGE_L": 0.17753201396973226, "CIDEr": 5.903347947168818e-10, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.3333333333333333, "f": 0.22857142857142854, "fn": 8.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.75, "f": 0.375, "fn": 1.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of people surfing on the river. They are wearing wetsuits and standing on their surfboards. The water is choppy and there are waves crashing against the shore. The sky is overcast and there are dark clouds in the distance. The buildings on either side of"}, "240960": {"image_id": 240960, "Bleu_1": 0.36666666665444453, "Bleu_2": 0.2514326764768464, "Bleu_3": 0.18920628274676068, "Bleu_4": 0.12585217456631012, "METEOR": 0.24952892871047472, "ROUGE_L": 0.4128595600676818, "CIDEr": 0.007726170055981416, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.18181818181818182, "f": 0.16666666666666669, "fn": 18.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a pizza in a pie crust on a black table. The pizza has red sauce and cheese on top. There are no other objects in the image."}, "421401": {"image_id": 421401, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.1313579154832357, "Bleu_3": 7.061638347858738e-07, "Bleu_4": 1.6457686856089725e-09, "METEOR": 0.17229866081532502, "ROUGE_L": 0.22889305816135083, "CIDEr": 2.045247423428849e-11, "SPICE": {"All": {"pr": 0.15, "re": 0.15789473684210525, "f": 0.15384615384615385, "fn": 16.0, "numImages": 1.0, "fp": 17.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image is of a vase with red flowers in it. The vase is made of clay and has a spiral design on it. The flowers are red and have long stems. The vase is sitting on a white table.\n\nThe image is of a vase with red flowers in it"}, "305159": {"image_id": 305159, "Bleu_1": 0.25423728813128416, "Bleu_2": 0.14804393806281507, "Bleu_3": 7.271691864101287e-07, "Bleu_4": 1.6187493009212366e-09, "METEOR": 0.17144109324813253, "ROUGE_L": 0.1994550408719346, "CIDEr": 1.8254865610250753e-15, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.23076923076923078, "f": 0.23076923076923078, "fn": 20.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.36363636363636365, "f": 0.36363636363636365, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a train car with a door open and a window on the side. There are no people in the car. The train car is painted in a light blue and white color scheme. The floor is made of metal and has a smooth surface. The walls are made of glass and have a smooth surface. The"}, "379767": {"image_id": 379767, "Bleu_1": 0.3599999999928, "Bleu_2": 0.1714285714251079, "Bleu_3": 8.491317075202567e-07, "Bleu_4": 1.8997953886425763e-09, "METEOR": 0.21768707482993196, "ROUGE_L": 0.2852466682253917, "CIDEr": 1.2842898926184125e-08, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.23809523809523808, "f": 0.23255813953488372, "fn": 16.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a plate of food with rice, broccoli, and chicken on it. There are two glasses of wine on the table and a fork and knife on the right side of the plate. The background is a white tablecloth with a pattern of red and white flowers.\n\nThe"}, "485509": {"image_id": 485509, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.12539246564153583, "Bleu_3": 6.711916014355543e-07, "Bleu_4": 1.5604229933011285e-09, "METEOR": 0.16785129344221522, "ROUGE_L": 0.2694941462337088, "CIDEr": 2.976871317555128e-09, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.25, "f": 0.2162162162162162, "fn": 12.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.3333333333333333, "f": 0.2222222222222222, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.75, "f": 0.46153846153846156, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a person skiing down a snowy slope. The person is wearing a red and black ski suit and has a pair of skis on their feet. The snow is covered in trees and there are mountains in the background.\n\nThe image is taken from a high angle, looking down the slope"}, "57222": {"image_id": 57222, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.17797249265986642, "Bleu_3": 0.14685945835002276, "Bleu_4": 0.11800699061814818, "METEOR": 0.21253710969021072, "ROUGE_L": 0.25722891566265055, "CIDEr": 8.464885509651985e-12, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.05714285714285714, "f": 0.06779661016949153, "fn": 33.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 15.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows two stuffed animals sitting on the ground. One of the stuffed animals is wearing a pink hat and the other is wearing a blue hat. The stuffed animals are sitting next to each other and are both wearing pink and blue clothing. The background of the image is a"}, "523637": {"image_id": 523637, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.21381869073854032, "Bleu_3": 0.17520942675548123, "Bleu_4": 0.144025900173122, "METEOR": 0.2605457481452671, "ROUGE_L": 0.2959369314736203, "CIDEr": 1.0913484212583392e-11, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.15384615384615385, "f": 0.15384615384615385, "fn": 22.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of people sitting on a couch, looking at a laptop. They are all wearing casual clothing and appear to be working on something. The room is well lit and there are no other objects in the room.\n\nThe image is taken in a living room with a couch"}, "221433": {"image_id": 221433, "Bleu_1": 0.2448979591786756, "Bleu_2": 0.17496355305233346, "Bleu_3": 8.668270200700186e-07, "Bleu_4": 1.9398130305383298e-09, "METEOR": 0.20454802749838508, "ROUGE_L": 0.19690122659780504, "CIDEr": 3.064971603467352e-10, "SPICE": {"All": {"pr": 0.24, "re": 0.2608695652173913, "f": 0.24999999999999994, "fn": 17.0, "numImages": 1.0, "fp": 19.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.16666666666666666, "re": 0.5, "f": 0.25, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Object": {"pr": 0.45454545454545453, "re": 0.5555555555555556, "f": 0.5, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a person surfing on a wave in the ocean. The person is wearing a black and white wetsuit and has a surfboard under their arm. There are other people in the background watching the surfer. The sky is blue and there are trees on the shore."}, "246124": {"image_id": 246124, "Bleu_1": 0.2264150943353507, "Bleu_2": 0.174582229926461, "Bleu_3": 0.13370981749106947, "Bleu_4": 0.109436018609632, "METEOR": 0.22938389841936402, "ROUGE_L": 0.2896142433234421, "CIDEr": 3.919373476656482e-12, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.18518518518518517, "f": 0.18867924528301885, "fn": 22.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4444444444444444, "f": 0.39999999999999997, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a group of people standing on a stone bridge over a river. They are all wearing suits and ties, and one of them is holding a kite. The sky is clear and blue, with a few clouds scattered about. The landscape is hilly and green, with trees and buildings visible"}, "193863": {"image_id": 193863, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.19603921176003908, "Bleu_3": 0.13300573168298876, "Bleu_4": 1.487964117094474e-05, "METEOR": 0.2638399642916949, "ROUGE_L": 0.26704190118824267, "CIDEr": 6.835032733171351e-10, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.23529411764705882, "f": 0.21052631578947367, "fn": 13.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.5, "f": 0.4444444444444445, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a woman playing tennis on a court with a net in the background. The woman is wearing a black shirt and white shorts, and has a racket in her hand. The court is made of concrete and has a fence around it. There are trees in the background."}, "242411": {"image_id": 242411, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.09356392870058139, "Bleu_3": 5.486731100334785e-07, "Bleu_4": 1.3350097003360843e-09, "METEOR": 0.16951422026201815, "ROUGE_L": 0.20666290231507625, "CIDEr": 1.1238872627102683e-12, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.13043478260869565, "f": 0.12244897959183673, "fn": 20.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a city street with tall buildings on either side. There are cars parked on the side of the road and people walking on the sidewalk. The sky is clear and blue.\n\nThe image is taken from a high angle, looking down on the street. The buildings are white and have large windows"}, "538198": {"image_id": 538198, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.1940285000252239, "Bleu_3": 0.13209470492933909, "Bleu_4": 0.09899454101416073, "METEOR": 0.22777308116459596, "ROUGE_L": 0.24002248454187747, "CIDEr": 1.384424254680176e-09, "SPICE": {"All": {"pr": 0.12903225806451613, "re": 0.14285714285714285, "f": 0.13559322033898305, "fn": 24.0, "numImages": 1.0, "fp": 27.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.36363636363636365, "f": 0.33333333333333337, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a white and green truck parked in a parking lot. The truck has a large grill on the front and a black and white striped hood. The truck is parked next to a white car with a black roof. There are several other cars parked in the lot"}, "177065": {"image_id": 177065, "Bleu_1": 0.3617021276518787, "Bleu_2": 0.1254040647777547, "Bleu_3": 7.043744125815199e-07, "Bleu_4": 1.678763465804455e-09, "METEOR": 0.19180769959909677, "ROUGE_L": 0.3076091850517784, "CIDEr": 2.4037656700533925e-05, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.3, "f": 0.2926829268292683, "fn": 14.0, "numImages": 1.0, "fp": 15.0, "tp": 6.0}, "Relation": {"pr": 0.2857142857142857, "re": 0.2222222222222222, "f": 0.25, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4444444444444444, "f": 0.4444444444444444, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a group of people playing tennis on a court. They are all wearing tennis shoes and some are holding rackets. The court is made of concrete and has a net in the middle. There are trees in the background and a blue sky above."}, "271248": {"image_id": 271248, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.214201664184383, "Bleu_3": 0.15529943088229656, "Bleu_4": 0.12369364927108253, "METEOR": 0.253267946063776, "ROUGE_L": 0.29204069419509276, "CIDEr": 1.4392953911669214e-10, "SPICE": {"All": {"pr": 0.2, "re": 0.2608695652173913, "f": 0.22641509433962265, "fn": 17.0, "numImages": 1.0, "fp": 24.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1, "re": 0.14285714285714285, "f": 0.11764705882352941, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.45454545454545453, "f": 0.43478260869565216, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a bus parked on the side of the road. The bus is a blue and white color with the words \"British Airways\" written on the side. There are people standing around the bus, looking at it. The road is empty and there are no other vehicles in sight."}, "569452": {"image_id": 569452, "Bleu_1": 0.1739130434744802, "Bleu_2": 0.10767638040926633, "Bleu_3": 0.06411053581337094, "Bleu_4": 8.84769088329556e-06, "METEOR": 0.12042414217097819, "ROUGE_L": 0.19728331177231562, "CIDEr": 4.1481756150603016e-08, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.15384615384615385, "f": 0.16, "fn": 22.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3, "f": 0.3157894736842105, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a person skateboarding down a sidewalk in a park. The person is wearing a white shirt and black pants, and has a black and white skateboard. There are trees and grass in the background.\n\nThe person is performing a trick on the skateboard,"}, "152340": {"image_id": 152340, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.21969401454816964, "Bleu_3": 0.1568828531255759, "Bleu_4": 0.11204438267139144, "METEOR": 0.29482655408475766, "ROUGE_L": 0.32370283018867924, "CIDEr": 7.230589467365372e-11, "SPICE": {"All": {"pr": 0.12, "re": 0.11538461538461539, "f": 0.11764705882352941, "fn": 23.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.09090909090909091, "re": 0.125, "f": 0.10526315789473685, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows a table with several plates of food on it. There are several glasses of juice and a pitcher of orange juice on the table. The people in the image are sitting at the table and eating their food. The table has a white tablecloth on it. The walls of"}, "383420": {"image_id": 383420, "Bleu_1": 0.3090909090852893, "Bleu_2": 0.21398889174339228, "Bleu_3": 0.15118969592806583, "Bleu_4": 0.1073737962825756, "METEOR": 0.24109509755070058, "ROUGE_L": 0.24358243011979464, "CIDEr": 3.3523241645593368e-12, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.2, "f": 0.1923076923076923, "fn": 20.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4, "f": 0.3636363636363636, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a remote control sitting on a wooden table. The remote control has a black and white design with the words \"popcorn love\" written on it. The remote control is made of plastic and has a small screen on the front. The remote control is connected to a television set. The television set"}, "531135": {"image_id": 531135, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.13231937691727144, "Bleu_3": 0.08709642720538069, "Bleu_4": 1.0616941372973596e-05, "METEOR": 0.1955059587114379, "ROUGE_L": 0.17951736315479697, "CIDEr": 8.954067654997081e-13, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.15384615384615385, "f": 0.17777777777777778, "fn": 22.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.125, "f": 0.16666666666666666, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.25, "f": 0.3, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a baseball game in progress on a field with a large crowd watching from the stands. The players are wearing baseball uniforms and are playing on a green field with a dirt infield. The sky is clear and blue, with a few clouds scattered across it. The temperature is warm and sunny"}, "356490": {"image_id": 356490, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.17377018855424126, "Bleu_3": 8.509776518247445e-07, "Bleu_4": 1.8929027702829327e-09, "METEOR": 0.16096459711643676, "ROUGE_L": 0.2238532110091743, "CIDEr": 5.983322464426592e-11, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.3333333333333333, "f": 0.21052631578947367, "fn": 8.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.75, "f": 0.39999999999999997, "fn": 1.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a person swimming in the ocean. The person is wearing a wetsuit and has their arms outstretched to the side. The water is calm and clear, with a few waves breaking on the shore. The sky is blue and cloudy, with a few white clouds visible in the"}, "289633": {"image_id": 289633, "Bleu_1": 0.3333333333232324, "Bleu_2": 0.24999999999230593, "Bleu_3": 0.20053619558908814, "Bleu_4": 0.12804548369058757, "METEOR": 0.2507529881306639, "ROUGE_L": 0.3836477987421384, "CIDEr": 0.000245830296408723, "SPICE": {"All": {"pr": 0.08695652173913043, "re": 0.06896551724137931, "f": 0.07692307692307693, "fn": 27.0, "numImages": 1.0, "fp": 21.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.18181818181818182, "f": 0.18181818181818182, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a man with a beard holding a toothbrush in his mouth. He is wearing a white shirt and blue jeans. The background is a bathroom with a sink and toilet."}, "184791": {"image_id": 184791, "Bleu_1": 0.41666666665798613, "Bleu_2": 0.29774566708143785, "Bleu_3": 0.1975442319449809, "Bleu_4": 0.13605128106221753, "METEOR": 0.2623713508249288, "ROUGE_L": 0.3558333333333333, "CIDEr": 7.033207185026986e-05, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.15384615384615385, "f": 0.16, "fn": 22.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.3333333333333333, "f": 0.34782608695652173, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The painting depicts a still life of fruit and a vase on a table. The vase is made of ceramic and has a red and white striped pattern. The fruit includes oranges, lemons, and apples. The painting is framed in a gold frame with a red and white"}, "271032": {"image_id": 271032, "Bleu_1": 0.4081632652977926, "Bleu_2": 0.27664166758053954, "Bleu_3": 0.16967565256814732, "Bleu_4": 1.8051994319685392e-05, "METEOR": 0.23983731657122068, "ROUGE_L": 0.33132166566083276, "CIDEr": 6.663440790228731e-08, "SPICE": {"All": {"pr": 0.23529411764705882, "re": 0.16, "f": 0.19047619047619052, "fn": 21.0, "numImages": 1.0, "fp": 13.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.3, "f": 0.37499999999999994, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 3.0}}, "caption": "The image shows a group of seagulls perched on a rocky outcropping overlooking the ocean. The birds are standing on the edge of a cliff, looking out over the water. The sky is clear and blue, with a few clouds scattered across it. The sun is shining down on"}, "183757": {"image_id": 183757, "Bleu_1": 0.2142857142818878, "Bleu_2": 0.10811249552151898, "Bleu_3": 6.00416577777256e-07, "Bleu_4": 1.4215774922802648e-09, "METEOR": 0.14421214738643898, "ROUGE_L": 0.1689750692520776, "CIDEr": 1.3163913878264926e-12, "SPICE": {"All": {"pr": 0.23333333333333334, "re": 0.4117647058823529, "f": 0.2978723404255319, "fn": 10.0, "numImages": 1.0, "fp": 23.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.75, "f": 0.375, "fn": 1.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.4, "re": 0.6666666666666666, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a small boat with two dogs on the deck. The boat is parked on the side of a river with trees and grass on the banks. The dogs are looking out of the window and appear to be enjoying the view.\n\nThe image is taken from a low angle, with the camera position"}, "579593": {"image_id": 579593, "Bleu_1": 0.19999999999600004, "Bleu_2": 0.11065666703226194, "Bleu_3": 6.342171440889115e-07, "Bleu_4": 1.5263496054154453e-09, "METEOR": 0.15196348464783652, "ROUGE_L": 0.15501905972045746, "CIDEr": 2.830065698607459e-11, "SPICE": {"All": {"pr": 0.12903225806451613, "re": 0.12903225806451613, "f": 0.12903225806451613, "fn": 27.0, "numImages": 1.0, "fp": 27.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2857142857142857, "f": 0.30769230769230765, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.16666666666666666, "f": 0.16, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The giraffe is standing in the middle of a clearing surrounded by tall trees. It is looking down at something on the ground. The trees are tall and green, and there is a small stream running through the clearing. The sky is blue and cloudy.\n\nThe giraffe is standing in"}, "416765": {"image_id": 416765, "Bleu_1": 0.26315789473222534, "Bleu_2": 0.20565318641151553, "Bleu_3": 0.1566604610064578, "Bleu_4": 0.10923922468551896, "METEOR": 0.23516044899560642, "ROUGE_L": 0.24413950829045164, "CIDEr": 3.7491154742947107e-13, "SPICE": {"All": {"pr": 0.2727272727272727, "re": 0.3157894736842105, "f": 0.2926829268292683, "fn": 13.0, "numImages": 1.0, "fp": 16.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.375, "f": 0.42857142857142855, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 3.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a group of cows standing in a field with a small stream running through it. The cows are all black and white and appear to be grazing on the grass in the field. The sky is clear and blue with some clouds in the distance. The grass is green and the cows are standing"}, "307209": {"image_id": 307209, "Bleu_1": 0.24999999999553577, "Bleu_2": 0.19069251784568234, "Bleu_3": 0.14988143767147738, "Bleu_4": 0.10616941372977243, "METEOR": 0.25980759446827434, "ROUGE_L": 0.2830626450116009, "CIDEr": 1.6960974408702157e-12, "SPICE": {"All": {"pr": 0.0967741935483871, "re": 0.14285714285714285, "f": 0.11538461538461538, "fn": 18.0, "numImages": 1.0, "fp": 28.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a brown bear standing on a log in the water. The bear is looking up at the camera with its mouth open. The water is calm and there are some trees in the background.\n\nThe image is taken from a low angle, looking up at the bear. The lighting is bright and the"}, "463013": {"image_id": 463013, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.1842190315423256, "Bleu_3": 0.12674769789184412, "Bleu_4": 0.09548024812134782, "METEOR": 0.18158899831903666, "ROUGE_L": 0.270509977827051, "CIDEr": 6.28858260052816e-10, "SPICE": {"All": {"pr": 0.0967741935483871, "re": 0.08823529411764706, "f": 0.09230769230769231, "fn": 31.0, "numImages": 1.0, "fp": 28.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a baseball game in progress. The players are wearing baseball uniforms and are standing on the field. The crowd is watching from the stands. The umpire is standing behind home plate, ready to call the next pitch. The sun is shining down on the field, casting a warm glow"}, "173204": {"image_id": 173204, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.06440351676097583, "Bleu_3": 4.3613042737873907e-07, "Bleu_4": 1.1406785378264665e-09, "METEOR": 0.15122830741492, "ROUGE_L": 0.15024630541871922, "CIDEr": 1.6237289150884987e-12, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.12121212121212122, "f": 0.126984126984127, "fn": 29.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.08333333333333333, "f": 0.125, "fn": 11.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3333333333333333, "f": 0.27272727272727276, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a group of people skiing down a snowy hill. They are wearing skiing gear and have their skis on their feet. The sun is setting in the background, casting a warm glow over the scene. The snow is packed and there are tracks in the snow where the ski"}, "9262": {"image_id": 9262, "Bleu_1": 0.2916666666605903, "Bleu_2": 0.13644420572458105, "Bleu_3": 7.396917719335283e-07, "Bleu_4": 1.731749067280523e-09, "METEOR": 0.1592232206741085, "ROUGE_L": 0.25058685446009393, "CIDEr": 8.175074305964471e-08, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.2, "f": 0.20512820512820512, "fn": 16.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.2222222222222222, "f": 0.25, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image is a table with a variety of food items on it, including sandwiches, fruit, and drinks. There are also several stuffed animals on the table, including a teddy bear and a rabbit. The table is in a hospital waiting room, with chairs and a desk nearby."}, "365822": {"image_id": 365822, "Bleu_1": 0.18181818181487608, "Bleu_2": 0.14213381090113214, "Bleu_3": 0.12398323920069518, "Bleu_4": 0.11003640569767388, "METEOR": 0.2787399514616642, "ROUGE_L": 0.2872277810476751, "CIDEr": 3.5202691917874644e-12, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.22727272727272727, "f": 0.22222222222222224, "fn": 17.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.5, "f": 0.47619047619047616, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a kitchen with white cabinets and a stove. There is a table and chairs in the center of the room. The floor is made of wood and there are no windows in the room.\n\nThe image is taken from the perspective of someone standing in the kitchen looking out the window. The"}, "531111": {"image_id": 531111, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.21797089763394864, "Bleu_3": 0.18806516254438782, "Bleu_4": 0.168935164036795, "METEOR": 0.36361648141214664, "ROUGE_L": 0.3380541871921182, "CIDEr": 1.3719814410348923e-11, "SPICE": {"All": {"pr": 0.038461538461538464, "re": 0.041666666666666664, "f": 0.04, "fn": 23.0, "numImages": 1.0, "fp": 25.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.09090909090909091, "re": 0.1111111111111111, "f": 0.09999999999999999, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}}, "caption": "The image shows a bathroom with a black and white checkered floor, a white sink, a black toilet, and a white shower. There is a mirror on the wall above the sink and a window with curtains on the left side of the room. The room is well lit with natural light"}, "402798": {"image_id": 402798, "Bleu_1": 0.2765957446749661, "Bleu_2": 0.17339191115885713, "Bleu_3": 0.11014337576432262, "Bleu_4": 0.07423447100551293, "METEOR": 0.2380363959693076, "ROUGE_L": 0.32555036691127415, "CIDEr": 5.118861587975247e-08, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.23529411764705882, "f": 0.19999999999999998, "fn": 13.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.125, "f": 0.16666666666666666, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.6, "f": 0.35294117647058826, "fn": 2.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a large red and white airplane on the tarmac at an airport. The plane has a large tail and wings, and is parked on the tarmac next to a small building. There are several other airplanes in the background, parked on the tarmac. The"}, "578092": {"image_id": 578092, "Bleu_1": 0.18965517241052324, "Bleu_2": 0.11536522398785581, "Bleu_3": 6.194229136940919e-07, "Bleu_4": 1.441782592356079e-09, "METEOR": 0.1557247335444531, "ROUGE_L": 0.1908237747653806, "CIDEr": 1.422627994676143e-14, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.25, "f": 0.23529411764705882, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.3333333333333333, "f": 0.2222222222222222, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4, "f": 0.3636363636363636, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a street with a house on the left side and a car parked on the right side. The house has a red door and a white roof. The car has a black hood and a white body. There are no other cars or people in the image. The sky is blue and there are no"}, "208135": {"image_id": 208135, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.15639188177115004, "Bleu_3": 0.12345111574563407, "Bleu_4": 0.10256732621419824, "METEOR": 0.259075070927363, "ROUGE_L": 0.22344322344322343, "CIDEr": 1.8939242023666558e-11, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.17857142857142858, "f": 0.1851851851851852, "fn": 23.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.18181818181818182, "re": 0.3333333333333333, "f": 0.23529411764705885, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3, "f": 0.3157894736842105, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a person skiing down a snowy mountain slope. The person is wearing blue and white clothing and has a blue and white helmet on their head. They are holding a pair of skis and appear to be in good physical condition. The image is taken from a high angle, looking down"}, "99428": {"image_id": 99428, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.1624659147420524, "Bleu_3": 0.10182666119201575, "Bleu_4": 1.2115660853589845e-05, "METEOR": 0.20207424054223436, "ROUGE_L": 0.22536945812807885, "CIDEr": 2.8465988446105893e-12, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.21052631578947367, "f": 0.1702127659574468, "fn": 15.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5, "f": 0.36363636363636365, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a stack of laptops on a table. The laptops are all different sizes and shapes, with some having a black and white screen, while others have a color screen. The laptops are all connected to each other via cables. There is also a small speaker on the table next"}, "455464": {"image_id": 455464, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.1565560727682472, "Bleu_3": 0.09934208621394808, "Bleu_4": 0.06688071957171945, "METEOR": 0.22366675787656784, "ROUGE_L": 0.2167219327333018, "CIDEr": 1.3937259534585949e-11, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.18181818181818182, "f": 0.2222222222222222, "fn": 27.0, "numImages": 1.0, "fp": 15.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.125, "f": 0.16666666666666666, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.4166666666666667, "f": 0.4166666666666667, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a man standing in front of a window, holding a phone to his ear. He is wearing a black shirt and jeans, and has a beard. The background is a dark, dimly lit room with a red carpet on the floor. There are several other people in the room"}, "341645": {"image_id": 341645, "Bleu_1": 0.24999999999479172, "Bleu_2": 0.1458649914948234, "Bleu_3": 7.733595213409628e-07, "Bleu_4": 1.7905355543488007e-09, "METEOR": 0.19127335385341748, "ROUGE_L": 0.2582244799225931, "CIDEr": 1.8095089015987253e-08, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.19047619047619047, "f": 0.17777777777777778, "fn": 17.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.42857142857142855, "f": 0.3529411764705882, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a man standing on the sidewalk, holding a skateboard. He is wearing a yellow shirt and gray pants. The street is lined with trees and buildings.\n\nThe image is taken from inside a car, looking out the window. The car is parked on the side"}, "200058": {"image_id": 200058, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.20603150145092247, "Bleu_3": 0.15236855057487514, "Bleu_4": 0.11076550621858337, "METEOR": 0.1865774826741614, "ROUGE_L": 0.23797139141742527, "CIDEr": 7.910058398556555e-10, "SPICE": {"All": {"pr": 0.08695652173913043, "re": 0.08, "f": 0.08333333333333333, "fn": 23.0, "numImages": 1.0, "fp": 21.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a woman standing on a snowboard in the middle of a snowy field. She is wearing a black jacket and pants, and has a helmet on her head. There are trees in the background, and a mountain range in the distance. The sky is clear and blue."}, "132328": {"image_id": 132328, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.17504208247906033, "Bleu_3": 0.12014621934667156, "Bleu_4": 0.07599442723762243, "METEOR": 0.17891521725213833, "ROUGE_L": 0.22724853645556145, "CIDEr": 1.3313856453031942e-11, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.2222222222222222, "f": 0.22641509433962265, "fn": 21.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.45454545454545453, "f": 0.45454545454545453, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a group of people flying kites in a field. The kites are made of different colors and shapes, and they are being flown by people of different ages. The sky is clear and blue, and there are no clouds in sight. The grass is green and well maintained, and there are no"}, "235131": {"image_id": 235131, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.0714005547281277, "Bleu_3": 4.703296565188882e-07, "Bleu_4": 1.2133650006096253e-09, "METEOR": 0.09193588600560385, "ROUGE_L": 0.18654434250764526, "CIDEr": 9.752610750347719e-11, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.2857142857142857, "f": 0.25, "fn": 15.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.25, "f": 0.16666666666666666, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.5555555555555556, "f": 0.5, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a large, modern building with a large glass facade and a large, open courtyard in front of it. The building appears to be a museum or art gallery, with a large, open courtyard in front of it. The courtyard is filled with people walking around and looking at"}, "403078": {"image_id": 403078, "Bleu_1": 0.32608695651465036, "Bleu_2": 0.24077170616624616, "Bleu_3": 0.17402276782115267, "Bleu_4": 0.1251252444284137, "METEOR": 0.2762393752365406, "ROUGE_L": 0.35510996119016824, "CIDEr": 3.193234063822231e-08, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.13043478260869565, "f": 0.12, "fn": 20.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of people standing on a riverbank, wearing wetsuits and holding surfboards. The water is calm and there are trees in the background.\n\nThe man in the foreground is wearing a black wetsuit and holding a surfboard. He is standing on the"}, "249325": {"image_id": 249325, "Bleu_1": 0.2916666666605903, "Bleu_2": 0.24911189772076184, "Bleu_3": 0.21136985503897382, "Bleu_4": 0.17021356059512996, "METEOR": 0.27542793940725907, "ROUGE_L": 0.3420560747663552, "CIDEr": 5.311540849076044e-08, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.16666666666666666, "f": 0.1851851851851852, "fn": 25.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.1, "f": 0.11764705882352941, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4444444444444444, "f": 0.39999999999999997, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a man sitting on a bench reading a newspaper while a group of pigeons sit on the ground next to him. The man is wearing a black jacket and a pair of sunglasses, and the pigeons are wearing their natural feathers. The bench is made"}, "110626": {"image_id": 110626, "Bleu_1": 0.23728813558919856, "Bleu_2": 0.14302403209718587, "Bleu_3": 0.10249156970769513, "Bleu_4": 0.07874565515341544, "METEOR": 0.20090327405316197, "ROUGE_L": 0.26082308925708175, "CIDEr": 1.7046512992006804e-14, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.14814814814814814, "f": 0.15999999999999998, "fn": 23.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3, "f": 0.3157894736842105, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a stop sign on the side of a road in the middle of a field. The sign is red and white and has the words \"stop\" written on it in black letters. The road is empty and there are no cars or other vehicles visible in the image. The sky is blue and there are some"}, "1852": {"image_id": 1852, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.17842851423634928, "Bleu_3": 0.12577746725443983, "Bleu_4": 0.0806636127871779, "METEOR": 0.19109864071663096, "ROUGE_L": 0.30118144947980957, "CIDEr": 1.742460931711367e-10, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.13513513513513514, "f": 0.1639344262295082, "fn": 32.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.0625, "f": 0.08695652173913043, "fn": 15.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.3076923076923077, "f": 0.33333333333333337, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a pair of scissors on a table next to a red rose. The scissors are open and the rose is in the background. The image is in black and white.\n\nThe image shows a pair of scissors on a table next to a red rose. The scissors"}, "175024": {"image_id": 175024, "Bleu_1": 0.1960784313687044, "Bleu_2": 0.1252448582145496, "Bleu_3": 0.06840816017013766, "Bleu_4": 9.036923863887698e-06, "METEOR": 0.2177023109998594, "ROUGE_L": 0.2858816637375513, "CIDEr": 2.079508954648394e-09, "SPICE": {"All": {"pr": 0.2, "re": 0.15384615384615385, "f": 0.17391304347826086, "fn": 22.0, "numImages": 1.0, "fp": 16.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.4444444444444444, "f": 0.4210526315789474, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a man riding a black motorcycle on a road. The man is wearing a black helmet and riding gloves. The motorcycle has a pink teddy bear on the back of it. The road is straight and there are trees on both sides of it. The sky is blue"}, "557114": {"image_id": 557114, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.214201664184383, "Bleu_3": 0.17777347039139163, "Bleu_4": 0.1470973677938225, "METEOR": 0.25775179857109504, "ROUGE_L": 0.35735207967193905, "CIDEr": 3.031750189974775e-10, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.11538461538461539, "f": 0.11538461538461539, "fn": 23.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.2727272727272727, "f": 0.2727272727272727, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a zebra standing in front of a wooden fence. The zebra is wearing a halter and has a tag on its ear. The fence is made of wooden planks and has a gate in the center. The trees in the background are tall and green. The sky is"}, "34471": {"image_id": 34471, "Bleu_1": 0.2666666666607408, "Bleu_2": 0.19069251784483274, "Bleu_3": 0.15011345265055326, "Bleu_4": 0.09473323931830074, "METEOR": 0.2218875846324847, "ROUGE_L": 0.25258799171842644, "CIDEr": 1.3711190801074795e-07, "SPICE": {"All": {"pr": 0.0625, "re": 0.06451612903225806, "f": 0.06349206349206349, "fn": 29.0, "numImages": 1.0, "fp": 30.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 10.0, "numImages": 1.0, "fp": 12.0, "tp": 2.0}}, "caption": "The image shows a living room with a couch, coffee table, and television. There are two men standing in the room, one is holding a remote control and the other is looking at the television. The room is well lit and there are no windows."}, "182874": {"image_id": 182874, "Bleu_1": 0.29629629629080934, "Bleu_2": 0.19782182158264086, "Bleu_3": 0.15553865936093278, "Bleu_4": 0.13106937161107982, "METEOR": 0.29435089341262105, "ROUGE_L": 0.2853801169590643, "CIDEr": 2.034788311633979e-12, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.15384615384615385, "f": 0.16326530612244897, "fn": 22.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.36363636363636365, "f": 0.32, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a busy street with several buses and cars driving down the road. There are trees on either side of the road and a few pedestrians walking on the sidewalk. The sky is blue and there are no clouds in sight. The image is taken from a bird's eye view, giving"}, "300008": {"image_id": 300008, "Bleu_1": 0.2142857142818878, "Bleu_2": 0.12483755678422238, "Bleu_3": 0.06608439529502602, "Bleu_4": 8.590238521223447e-06, "METEOR": 0.15188224547609713, "ROUGE_L": 0.21229698375870068, "CIDEr": 1.3963704252169272e-12, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.19230769230769232, "f": 0.19230769230769232, "fn": 21.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.1111111111111111, "f": 0.125, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 1.0, "f": 0.33333333333333337, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a cow lying on the beach next to a group of people. The cow is brown and has a white patch on its forehead. The people are standing on the beach and looking at the cow. The sky is blue and there are palm trees in the background.\n\nThe image is taken from"}, "12014": {"image_id": 12014, "Bleu_1": 0.2653061224435652, "Bleu_2": 0.18210783976741574, "Bleu_3": 0.08902660440381881, "Bleu_4": 1.1128852820974656e-05, "METEOR": 0.2544845265937435, "ROUGE_L": 0.2622527944969905, "CIDEr": 2.008068598805026e-07, "SPICE": {"All": {"pr": 0.15789473684210525, "re": 0.1111111111111111, "f": 0.13043478260869565, "fn": 24.0, "numImages": 1.0, "fp": 16.0, "tp": 3.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.18181818181818182, "f": 0.2105263157894737, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows a double decker bus driving down a city street. The bus is red and has a white stripe on the side. There are people standing on the sidewalk and in the street. The sky is cloudy and there are buildings on either side of the street."}, "557907": {"image_id": 557907, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.2772413120279784, "Bleu_3": 0.23238241779243357, "Bleu_4": 0.19014488239668145, "METEOR": 0.3066629342970154, "ROUGE_L": 0.23416506717850286, "CIDEr": 4.609170372733888e-11, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.17857142857142858, "f": 0.17543859649122806, "fn": 23.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.09090909090909091, "re": 0.1, "f": 0.09523809523809525, "fn": 9.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.3333333333333333, "f": 0.34782608695652173, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows two sheep grazing in a green field. The sheep are white with black spots and have long, curly horns. They are standing in a grassy field with trees in the background. The sky is blue and there are clouds in the distance.\n\nThe sheep are grazing on the"}, "278463": {"image_id": 278463, "Bleu_1": 0.26190476189852613, "Bleu_2": 0.15984894146361395, "Bleu_3": 0.10850835457962009, "Bleu_4": 1.3453374496645484e-05, "METEOR": 0.17787314825245096, "ROUGE_L": 0.2543786488740617, "CIDEr": 5.078200475487727e-06, "SPICE": {"All": {"pr": 0.30434782608695654, "re": 0.3333333333333333, "f": 0.31818181818181823, "fn": 14.0, "numImages": 1.0, "fp": 16.0, "tp": 7.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.1111111111111111, "f": 0.125, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.8333333333333334, "f": 0.5882352941176471, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a desk with a laptop on it. There are several books and papers on the desk, as well as a cup of coffee on the side. The room is dimly lit and there is a window in the background."}, "303534": {"image_id": 303534, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.15041420939612582, "Bleu_3": 0.11072431282861321, "Bleu_4": 0.07254929979746852, "METEOR": 0.16156926967175284, "ROUGE_L": 0.2167219327333018, "CIDEr": 3.583084311756734e-11, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.2, "f": 0.20512820512820512, "fn": 16.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.4444444444444444, "f": 0.47058823529411764, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 4.0}}, "caption": "The image shows a bedroom with a bed, a desk, and a window. The walls are painted blue and there are curtains on the windows. The floor is covered in carpet and there is a rug on the bed. The room is well lit and there are no objects in the room."}, "257187": {"image_id": 257187, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.16803361008003348, "Bleu_3": 0.1200160042655936, "Bleu_4": 0.09212480089236007, "METEOR": 0.2261132798074201, "ROUGE_L": 0.22889305816135083, "CIDEr": 5.456343677896355e-10, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.29411764705882354, "f": 0.23255813953488372, "fn": 12.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.2222222222222222, "re": 0.4, "f": 0.2857142857142857, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.5, "f": 0.3529411764705882, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows two women holding up a bottle of wine and smiling at the camera. They are both wearing blue shirts and white pants, and one of them has a red scarf around her neck. The background is a wooden bar with a large mirror on the wall behind it."}, "268838": {"image_id": 268838, "Bleu_1": 0.2181818181778513, "Bleu_2": 0.14213381090113214, "Bleu_3": 0.09135163170376533, "Bleu_4": 0.06187801819921283, "METEOR": 0.17725543372801036, "ROUGE_L": 0.21542083578575633, "CIDEr": 7.042399819408069e-14, "SPICE": {"All": {"pr": 0.15625, "re": 0.19230769230769232, "f": 0.1724137931034483, "fn": 21.0, "numImages": 1.0, "fp": 27.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.4166666666666667, "f": 0.4, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a group of sheep grazing in a field surrounded by trees. The sheep are standing in a line, with their heads down and their tails wagging. The trees are tall and green, with branches that stretch up towards the sky. The ground is covered in yellow grass and there are no other"}, "1563": {"image_id": 1563, "Bleu_1": 0.31818181817458685, "Bleu_2": 0.1923483450237976, "Bleu_3": 0.13825567198350197, "Bleu_4": 0.10655498021900175, "METEOR": 0.22240711717100012, "ROUGE_L": 0.3042394014962594, "CIDEr": 4.56286276766545e-07, "SPICE": {"All": {"pr": 0.07142857142857142, "re": 0.08695652173913043, "f": 0.0784313725490196, "fn": 21.0, "numImages": 1.0, "fp": 26.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.18181818181818182, "f": 0.16666666666666669, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a woman standing in front of a ski race course. She is wearing a black and white ski suit and holding a pair of skis. The course is marked with flags and there are people in the background watching the race."}, "320642": {"image_id": 320642, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.17131872291347974, "Bleu_3": 0.0826422446045352, "Bleu_4": 1.0256732621419825e-05, "METEOR": 0.24223403572630328, "ROUGE_L": 0.2783481633584303, "CIDEr": 1.4651602093126717e-11, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.09090909090909091, "f": 0.08695652173913043, "fn": 20.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.25, "f": 0.2105263157894737, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows two women standing in front of a building with a sign that reads \"dream of life\" in white letters. The women are both wearing black dresses and white shoes. One of them is holding a black and white video game controller in her hand. The building is a large, modern structure"}, "152771": {"image_id": 152771, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.23329531792993197, "Bleu_3": 0.16221858930426464, "Bleu_4": 0.11431164199464498, "METEOR": 0.2694343853323258, "ROUGE_L": 0.27453186530579443, "CIDEr": 2.5955825992747413e-10, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.2608695652173913, "f": 0.23529411764705882, "fn": 17.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.1, "re": 0.16666666666666666, "f": 0.125, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.625, "f": 0.4761904761904762, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a parking lot with several cars parked in it. There are trees in the background and a bike parked in the middle of the lot. The parking lot is paved with asphalt and there are no other vehicles or people in the image. The sky is clear and there are"}, "450464": {"image_id": 450464, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.24228126143268136, "Bleu_3": 0.1892035088707753, "Bleu_4": 0.10735056776927296, "METEOR": 0.2617368011081236, "ROUGE_L": 0.31443298969072164, "CIDEr": 2.4563453497853586e-11, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.18181818181818182, "f": 0.17391304347826086, "fn": 18.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a train on the tracks in front of a building with a dome on top. There are people walking on the sidewalk and cars driving on the street. The sky is cloudy and there are buildings in the background.\n\nThe train is a black and silver subway car with the words \""}, "533517": {"image_id": 533517, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.18504536999744126, "Bleu_3": 0.12547526428859127, "Bleu_4": 0.07889074281873733, "METEOR": 0.227432014789647, "ROUGE_L": 0.25507765830346474, "CIDEr": 1.653468743939199e-12, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.21428571428571427, "f": 0.22222222222222224, "fn": 22.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.45454545454545453, "f": 0.45454545454545453, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a table with a vase of blue and orange flowers on it. There are also two chairs at the table. The room is dimly lit and there are several people sitting at the table.\n\nThe image shows a table with a vase of blue and orange flowers on it. There are"}, "151877": {"image_id": 151877, "Bleu_1": 0.32692307691678996, "Bleu_2": 0.1601281538019776, "Bleu_3": 8.004271223291966e-07, "Bleu_4": 1.7986320469414935e-09, "METEOR": 0.21622234782004907, "ROUGE_L": 0.24233825198637912, "CIDEr": 1.0460565718819102e-10, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.10714285714285714, "f": 0.12244897959183672, "fn": 25.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a truck parked on the side of a busy street. There are people walking on the sidewalk and a building in the background. The sky is clear and blue.\n\nThe truck is a large, white vehicle with a red and blue stripe on the side. It has a large"}, "262587": {"image_id": 262587, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.1313579154832357, "Bleu_3": 7.061638347858738e-07, "Bleu_4": 1.6457686856089725e-09, "METEOR": 0.23909593538127463, "ROUGE_L": 0.18654434250764526, "CIDEr": 4.015657045732369e-11, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.3333333333333333, "f": 0.25641025641025644, "fn": 10.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.8, "f": 0.5000000000000001, "fn": 1.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a person surfing on a wave in the ocean. The person is wearing a black wetsuit and holding onto the surfboard with one hand while riding the wave with the other. The wave is large and white, with a lot of foam on top. The sky is cloud"}, "542537": {"image_id": 542537, "Bleu_1": 0.31481481480898493, "Bleu_2": 0.23121228231834207, "Bleu_3": 0.16021117897522516, "Bleu_4": 0.11268978937259727, "METEOR": 0.29964186112242136, "ROUGE_L": 0.34231200897867564, "CIDEr": 1.983200825335252e-10, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.17391304347826086, "f": 0.1702127659574468, "fn": 19.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.14285714285714285, "f": 0.16666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a man in a blue shirt and black pants doing a handstand on a red and white checkered mat. He is holding a frisbee in his right hand and his left hand is in the air. There are people in the background watching him. The image is in black and white"}, "70258": {"image_id": 70258, "Bleu_1": 0.17647058823183395, "Bleu_2": 0.16803361008003348, "Bleu_3": 0.15121069009849425, "Bleu_4": 0.13775870873463825, "METEOR": 0.3397666793775082, "ROUGE_L": 0.3122200895713372, "CIDEr": 8.757403214432648e-10, "SPICE": {"All": {"pr": 0.06451612903225806, "re": 0.07692307692307693, "f": 0.07017543859649122, "fn": 24.0, "numImages": 1.0, "fp": 29.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.18181818181818182, "f": 0.16666666666666669, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a group of children playing with a ball in a courtyard. The children are wearing red shirts and blue pants, and they are standing in a line, holding the ball in their hands. The courtyard is made of concrete and has a few trees in the background. The"}, "396542": {"image_id": 396542, "Bleu_1": 0.31372549018992696, "Bleu_2": 0.19402850002522387, "Bleu_3": 0.09158935294477763, "Bleu_4": 1.124795146748506e-05, "METEOR": 0.18520475438654943, "ROUGE_L": 0.21997836278398844, "CIDEr": 7.497226837974107e-09, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.15151515151515152, "f": 0.16393442622950818, "fn": 28.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.4166666666666667, "f": 0.4, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a person skiing down a snowy slope with a tree on their back. The person is wearing a pink backpack and black ski boots. The tree is a small evergreen tree with branches that are wrapped around the person's back. The person is skiing down a snowy"}, "37325": {"image_id": 37325, "Bleu_1": 0.14893617020959712, "Bleu_2": 0.09855571262853866, "Bleu_3": 5.998606361814997e-07, "Bleu_4": 1.4882455879058203e-09, "METEOR": 0.1279345382098039, "ROUGE_L": 0.19303797468354428, "CIDEr": 1.8270695255872122e-09, "SPICE": {"All": {"pr": 0.26666666666666666, "re": 0.14285714285714285, "f": 0.18604651162790697, "fn": 24.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.4, "f": 0.4000000000000001, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a skateboarder performing a trick on a halfpipe. The skateboarder is wearing a white shirt and black pants, and has a black and white helmet on his head. The halfpipe is made of metal and has a flat top and a curved bottom. There"}, "21645": {"image_id": 21645, "Bleu_1": 0.23214285713871177, "Bleu_2": 0.18375590810914594, "Bleu_3": 0.1233303713651029, "Bleu_4": 1.3716192197467836e-05, "METEOR": 0.1858410843126373, "ROUGE_L": 0.23461538461538461, "CIDEr": 5.1371083485043734e-12, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.13333333333333333, "f": 0.1379310344827586, "fn": 26.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.07692307692307693, "re": 0.16666666666666666, "f": 0.10526315789473684, "fn": 5.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.25, "f": 0.2727272727272727, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The room is painted in a light beige color with white trim and dark wood floors. There is a large window on the left side of the room with a view of the backyard. The room has a large couch and a coffee table in the center of the room. There is a fireplace on the"}, "434479": {"image_id": 434479, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.18070977621815823, "Bleu_3": 0.08619148700244576, "Bleu_4": 1.0637896949215625e-05, "METEOR": 0.20170255070763377, "ROUGE_L": 0.22195269860521533, "CIDEr": 4.208803346481145e-12, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.0625, "f": 0.07407407407407407, "fn": 30.0, "numImages": 1.0, "fp": 20.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.15384615384615385, "f": 0.2, "fn": 11.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The image shows a table with two slices of pizza on it. The pizza has cheese and vegetables on it. There are also two glasses of milk on the table. The table is in a restaurant setting with chairs and a bar. The walls are painted a light brown color and there are"}, "104563": {"image_id": 104563, "Bleu_1": 0.3111111111041976, "Bleu_2": 0.23783535599888, "Bleu_3": 0.1580284245644431, "Bleu_4": 0.09845529669817564, "METEOR": 0.23068409987232788, "ROUGE_L": 0.32084155161078237, "CIDEr": 3.329927880788972e-08, "SPICE": {"All": {"pr": 0.25, "re": 0.42857142857142855, "f": 0.3157894736842105, "fn": 8.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.2222222222222222, "re": 0.2857142857142857, "f": 0.25, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 1.0, "f": 0.5333333333333333, "fn": 0.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a young man in a yellow shirt jumping over a skateboard on a sidewalk. The man is wearing a pair of sneakers and has his arms outstretched as he jumps. The skateboard is on the ground and the man is in mid-air."}, "203864": {"image_id": 203864, "Bleu_1": 0.39393939392745647, "Bleu_2": 0.24809883171680105, "Bleu_3": 0.12568860753791988, "Bleu_4": 1.6039529419560237e-05, "METEOR": 0.2542749465121147, "ROUGE_L": 0.27403414195867026, "CIDEr": 0.00035748333336886964, "SPICE": {"All": {"pr": 0.16129032258064516, "re": 0.21739130434782608, "f": 0.18518518518518517, "fn": 18.0, "numImages": 1.0, "fp": 26.0, "tp": 5.0}, "Relation": {"pr": 0.1, "re": 0.125, "f": 0.11111111111111112, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.125, "re": 0.16666666666666666, "f": 0.14285714285714288, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3333333333333333, "f": 0.27272727272727276, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a group of people playing tennis on a court. They are wearing tennis shoes and holding rackets. The court is made of grass and there are trees in the background."}, "463601": {"image_id": 463601, "Bleu_1": 0.3599999999928, "Bleu_2": 0.2571428571376619, "Bleu_3": 0.17662651284533257, "Bleu_4": 0.10405607890038332, "METEOR": 0.24917504157583614, "ROUGE_L": 0.30118144947980957, "CIDEr": 5.367928745757915e-10, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.125, "f": 0.10526315789473685, "fn": 14.0, "numImages": 1.0, "fp": 20.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2857142857142857, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a man and a child standing on the beach, holding a kite. The man is wearing a green shirt and shorts, and the child is wearing a yellow shirt and shorts. They are both standing in the sand, with the ocean in the background. The kite is"}, "355263": {"image_id": 355263, "Bleu_1": 0.24137931034066593, "Bleu_2": 0.19522428815262632, "Bleu_3": 0.15041225572379988, "Bleu_4": 0.12542576072165543, "METEOR": 0.21662599645415107, "ROUGE_L": 0.2335886214442013, "CIDEr": 2.468129068058135e-14, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.15, "f": 0.13043478260869565, "fn": 17.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of stuffed animals hanging from a metal beam on the side of a building. The animals are all different colors and have different expressions on their faces. One of the animals is wearing a hat and has a big smile on its face. The other animals are looking at each other and seem"}, "573223": {"image_id": 573223, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.20180183819497494, "Bleu_3": 0.1482450067653591, "Bleu_4": 0.10738497851612415, "METEOR": 0.2626849651654455, "ROUGE_L": 0.29397590361445786, "CIDEr": 6.674970535266165e-11, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.2857142857142857, "f": 0.25531914893617025, "fn": 15.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.1111111111111111, "f": 0.15384615384615383, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.7142857142857143, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a young girl standing in front of a toilet in a bathroom. She is wearing a blue dress with white stripes and has her hair tied back in a ponytail. The toilet is white and has a seat and lid. There is a sink to the left of the"}, "52007": {"image_id": 52007, "Bleu_1": 0.2727272727223141, "Bleu_2": 0.17407765595250352, "Bleu_3": 0.13175185157163616, "Bleu_4": 0.08143605172498537, "METEOR": 0.2277894013046672, "ROUGE_L": 0.3038184836745988, "CIDEr": 1.7744902877243435e-12, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.2, "f": 0.20408163265306126, "fn": 20.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.5, "f": 0.45454545454545453, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a bus stop with a bus parked at it. There are people standing on the sidewalk and a sign on the bus stop that reads \"bus stop\". The sky is cloudy and there are trees and buildings in the background.\n\nThe image is taken from the perspective of someone standing on the"}, "403943": {"image_id": 403943, "Bleu_1": 0.3333333333263889, "Bleu_2": 0.23819653366515028, "Bleu_3": 0.1702384584471886, "Bleu_4": 0.12168796515241928, "METEOR": 0.2824970009201224, "ROUGE_L": 0.3202099737532808, "CIDEr": 1.1565708531825192e-06, "SPICE": {"All": {"pr": 0.1, "re": 0.07142857142857142, "f": 0.08333333333333333, "fn": 26.0, "numImages": 1.0, "fp": 18.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.18181818181818182, "f": 0.18181818181818182, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a man standing on the beach holding an umbrella. The man is wearing a black shirt and shorts, and has a tan. The umbrella is striped with black and white stripes. The beach is sandy and has a few palm trees. There are several beach"}, "303250": {"image_id": 303250, "Bleu_1": 0.1403508771905202, "Bleu_2": 0.0867109969508772, "Bleu_3": 0.051514389400573894, "Bleu_4": 7.0932942733526836e-06, "METEOR": 0.17925427084076273, "ROUGE_L": 0.14244016345592528, "CIDEr": 3.7172192054618325e-15, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.29411764705882354, "f": 0.22727272727272727, "fn": 12.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.625, "f": 0.5263157894736842, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a brown bear and its cub in the water. The bear is standing on its hind legs and the cub is standing on its front legs. They are both looking at each other. The water is murky and there are some plants growing in it. The sky is cloudy and there are some trees"}, "397211": {"image_id": 397211, "Bleu_1": 0.21739130434310025, "Bleu_2": 0.1390096093683277, "Bleu_3": 0.07601144519394981, "Bleu_4": 1.0052917730118351e-05, "METEOR": 0.16110275278999173, "ROUGE_L": 0.24830393487109906, "CIDEr": 2.8396887034993844e-09, "SPICE": {"All": {"pr": 0.125, "re": 0.2, "f": 0.15384615384615385, "fn": 12.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.42857142857142855, "f": 0.3529411764705882, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a group of people playing skateboarding on a ramp in a park. The people are wearing skateboarding gear, including helmets and knee pads. The ramp is made of wood and has a smooth surface. There are trees and buildings in the background of"}, "2239": {"image_id": 2239, "Bleu_1": 0.29999999999400007, "Bleu_2": 0.22131333406452386, "Bleu_3": 0.17215301886614504, "Bleu_4": 0.12138611630520416, "METEOR": 0.2585782474149534, "ROUGE_L": 0.30902840600687537, "CIDEr": 8.604569645048064e-10, "SPICE": {"All": {"pr": 0.13636363636363635, "re": 0.2, "f": 0.16216216216216214, "fn": 12.0, "numImages": 1.0, "fp": 19.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.3333333333333333, "f": 0.2, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows a young girl holding a tennis racket and standing on a tennis court. She is wearing a white tennis shirt and white shorts. The background is a green grass tennis court with a net in the center. There are several tennis balls on the ground around her."}, "566314": {"image_id": 566314, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.194191754997615, "Bleu_3": 0.11247911738077475, "Bleu_4": 1.2861839592392576e-05, "METEOR": 0.2610059551999672, "ROUGE_L": 0.18340348767288037, "CIDEr": 7.52203039571615e-13, "SPICE": {"All": {"pr": 0.2777777777777778, "re": 0.1724137931034483, "f": 0.21276595744680854, "fn": 24.0, "numImages": 1.0, "fp": 13.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.38461538461538464, "f": 0.41666666666666663, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image is a stained glass window in a church. The window is made up of many different colors, including red, blue, green, and yellow. The window is surrounded by a wooden frame and has a clock on the wall. The clock is made of metal and has numbers and hands. The image is very"}, "291537": {"image_id": 291537, "Bleu_1": 0.29999999999400007, "Bleu_2": 0.17496355305240635, "Bleu_3": 0.08607650943307255, "Bleu_4": 1.0792921564506728e-05, "METEOR": 0.22656136740951058, "ROUGE_L": 0.23252858958068615, "CIDEr": 1.5094145148864143e-10, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.3125, "f": 0.2325581395348837, "fn": 11.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.125, "re": 0.5, "f": 0.2, "fn": 1.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.4444444444444444, "f": 0.34782608695652173, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a person snowboarding down a mountain slope. The person is wearing a black and red jacket, black pants, and black snowboarding boots. The person is holding onto the snowboard with both hands and is turning to the left. The snow on the mountain is white and there"}, "262605": {"image_id": 262605, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.16568337391262167, "Bleu_3": 8.243669902025871e-07, "Bleu_4": 1.8483326514023873e-09, "METEOR": 0.24078840201291735, "ROUGE_L": 0.33608815426997246, "CIDEr": 6.7537622902824765e-09, "SPICE": {"All": {"pr": 0.25, "re": 0.35294117647058826, "f": 0.2926829268292683, "fn": 11.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.125, "re": 0.16666666666666666, "f": 0.14285714285714288, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5, "f": 0.4210526315789474, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a woman standing in front of a mirror, wearing a black dress and holding a hat in her hand. The room is dimly lit and there is a chandelier hanging from the ceiling.\n\nThe woman is looking at her reflection in the mirror and appears to be adjusting"}, "118778": {"image_id": 118778, "Bleu_1": 0.18518518518175586, "Bleu_2": 0.059110575176554124, "Bleu_3": 4.065453602348427e-07, "Bleu_4": 1.0713693373996204e-09, "METEOR": 0.2074391966926388, "ROUGE_L": 0.14575866188769412, "CIDEr": 1.3012471560513588e-12, "SPICE": {"All": {"pr": 0.14705882352941177, "re": 0.20833333333333334, "f": 0.1724137931034483, "fn": 19.0, "numImages": 1.0, "fp": 29.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.23076923076923078, "re": 0.375, "f": 0.2857142857142857, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2, "f": 0.17391304347826086, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a train on the tracks, with a lot of cargo containers on the side of the train. There are buildings in the background, with a lot of greenery in the foreground. The sky is clear and blue.\n\nThe train is moving slowly, with the cargo containers on the side of the"}, "121673": {"image_id": 121673, "Bleu_1": 0.3399999999932, "Bleu_2": 0.18626292585916956, "Bleu_3": 8.974367873592764e-07, "Bleu_4": 1.980288368094831e-09, "METEOR": 0.22903959772557253, "ROUGE_L": 0.27595566613888267, "CIDEr": 4.910070614734869e-08, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 12.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.16666666666666666, "f": 0.13333333333333333, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.2222222222222222, "re": 0.5, "f": 0.30769230769230765, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a person surfing on a wooden surfboard in the ocean. The person is wearing a red shirt and black pants, and is holding onto the surfboard with one hand while riding the wave with the other. The wave is large and white, and the person is jump"}, "91670": {"image_id": 91670, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.1596564940032875, "Bleu_3": 0.10132945277550723, "Bleu_4": 0.06823252823824055, "METEOR": 0.14759694260059275, "ROUGE_L": 0.14923547400611623, "CIDEr": 9.301907507207673e-11, "SPICE": {"All": {"pr": 0.15, "re": 0.13043478260869565, "f": 0.13953488372093023, "fn": 20.0, "numImages": 1.0, "fp": 17.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a street with several businesses, including a pizza parlor, a restaurant, and a convenience store. There are cars parked on the side of the road and people walking on the sidewalk. The sky is clear and there are no clouds in sight.\n\nThe image is taken from a"}, "162829": {"image_id": 162829, "Bleu_1": 0.2608695652117203, "Bleu_2": 0.2014440620697061, "Bleu_3": 0.1664467895387212, "Bleu_4": 0.13392750150279648, "METEOR": 0.22455815001178114, "ROUGE_L": 0.2121001390820584, "CIDEr": 2.4195191039900364e-08, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.2, "f": 0.16, "fn": 16.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5714285714285714, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a group of people sitting at tables outside under a canopy of trees. There are several tables set up with white tablecloths and green napkins. The people are dressed in casual attire and are enjoying their meals. The atmosphere is relaxed and social."}, "360101": {"image_id": 360101, "Bleu_1": 0.19999999999555562, "Bleu_2": 0.16514456476524256, "Bleu_3": 0.10825053129131099, "Bleu_4": 0.07413276603873248, "METEOR": 0.18925263361470104, "ROUGE_L": 0.29468599033816417, "CIDEr": 1.2876589994937802e-08, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.14814814814814814, "f": 0.14035087719298248, "fn": 23.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.18181818181818182, "re": 0.2222222222222222, "f": 0.19999999999999998, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.18181818181818182, "f": 0.1904761904761905, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a plate with two sandwiches on it. The sandwiches are made with bread, tomato, and cheese. The plate is on a table in a kitchen.\n\nThe image shows a plate with two sandwiches on it. The sandwiches are made with bread, tom"}, "580410": {"image_id": 580410, "Bleu_1": 0.339622641503026, "Bleu_2": 0.2680359137985001, "Bleu_3": 0.1916881510700032, "Bleu_4": 0.15407030704019273, "METEOR": 0.29541567964424525, "ROUGE_L": 0.39503507825148415, "CIDEr": 1.8256650601381183e-06, "SPICE": {"All": {"pr": 0.26666666666666666, "re": 0.23529411764705882, "f": 0.25, "fn": 13.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.25, "f": 0.28571428571428575, "fn": 3.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.42857142857142855, "f": 0.39999999999999997, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a living room with a white couch, a coffee table, and a bookshelf. There is a window on the left side of the room with curtains open. The floor is made of hardwood and there is a rug in the center of the room. The walls are painted white and"}, "97427": {"image_id": 97427, "Bleu_1": 0.29787234041919425, "Bleu_2": 0.16094080478419465, "Bleu_3": 0.08318402702711739, "Bleu_4": 1.0694655186909581e-05, "METEOR": 0.23653905318084054, "ROUGE_L": 0.29983323093127356, "CIDEr": 7.0468288192842306e-09, "SPICE": {"All": {"pr": 0.28, "re": 0.28, "f": 0.28, "fn": 18.0, "numImages": 1.0, "fp": 18.0, "tp": 7.0}, "Relation": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.1111111111111111, "f": 0.16666666666666666, "fn": 8.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.2857142857142857, "re": 0.4, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a kitchen with white appliances, a wooden table, and a window with curtains. There is a refrigerator, stove, and sink in the kitchen. The floor is made of tiles and there is a wooden countertop. The walls are painted white and there are no"}, "437898": {"image_id": 437898, "Bleu_1": 0.30232558138831805, "Bleu_2": 0.18971331171167066, "Bleu_3": 0.09574966238312985, "Bleu_4": 1.2171327283748823e-05, "METEOR": 0.24778138019205145, "ROUGE_L": 0.3400696864111499, "CIDEr": 7.634832559489247e-08, "SPICE": {"All": {"pr": 0.07692307692307693, "re": 0.09090909090909091, "f": 0.08333333333333334, "fn": 20.0, "numImages": 1.0, "fp": 24.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.2, "f": 0.2105263157894737, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a kitchen with white appliances, wooden cabinets, and a white refrigerator. There is a sink and stove in the kitchen. The floor is made of hardwood.\n\nThe image shows a kitchen with white appliances, wooden cabinets, and a white refr"}, "149550": {"image_id": 149550, "Bleu_1": 0.2608695652117203, "Bleu_2": 0.17025130614800751, "Bleu_3": 0.12549213105688165, "Bleu_4": 0.09791579531640562, "METEOR": 0.22484772040551293, "ROUGE_L": 0.2969401947148818, "CIDEr": 1.1490129775899007e-08, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.20833333333333334, "f": 0.19607843137254902, "fn": 19.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.1, "re": 0.1111111111111111, "f": 0.10526315789473685, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5, "f": 0.36363636363636365, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image is of a bathroom with a sink, toilet, and shower. The sink is made of white porcelain and has a faucet on the right side. The toilet is made of white porcelain and has a seat on the left side. The shower is made"}, "516422": {"image_id": 516422, "Bleu_1": 0.3333333333263889, "Bleu_2": 0.16843038420975753, "Bleu_3": 0.1072435086496333, "Bleu_4": 0.07235609698473011, "METEOR": 0.26992592390225073, "ROUGE_L": 0.29047619047619044, "CIDEr": 1.7556409325867586e-06, "SPICE": {"All": {"pr": 0.30434782608695654, "re": 0.20588235294117646, "f": 0.2456140350877193, "fn": 27.0, "numImages": 1.0, "fp": 16.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.375, "re": 0.2727272727272727, "f": 0.3157894736842105, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}, "Size": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Color": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.4444444444444444, "re": 0.3076923076923077, "f": 0.3636363636363637, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a couple sitting on a bench in a park, reading a newspaper. The woman is wearing a white shirt and black pants, while the man is wearing a black shirt and white pants. They are both holding their dog, a white terrier, on a leash."}, "543065": {"image_id": 543065, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.24763027826710712, "Bleu_3": 0.22118717485262898, "Bleu_4": 0.1898301835582179, "METEOR": 0.3398914208951915, "ROUGE_L": 0.3819009675583381, "CIDEr": 5.990673327413861e-11, "SPICE": {"All": {"pr": 0.2, "re": 0.14814814814814814, "f": 0.1702127659574468, "fn": 23.0, "numImages": 1.0, "fp": 16.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4, "f": 0.380952380952381, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a man sitting at a table with a pizza on it. The man is wearing glasses and has a shirt on. There are other people in the background of the image. The table has a white tablecloth on it and there are two chairs at the table. The image is"}, "432085": {"image_id": 432085, "Bleu_1": 0.2264150943353507, "Bleu_2": 0.16163173752903065, "Bleu_3": 8.001308872460666e-07, "Bleu_4": 1.7890738820867687e-09, "METEOR": 0.1895569542009238, "ROUGE_L": 0.2943699731903485, "CIDEr": 1.1822764032556943e-07, "SPICE": {"All": {"pr": 0.24, "re": 0.2, "f": 0.2181818181818182, "fn": 24.0, "numImages": 1.0, "fp": 19.0, "tp": 6.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.08333333333333333, "f": 0.10526315789473685, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.125, "f": 0.125, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.4, "f": 0.4000000000000001, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a man sitting at a desk with a plate of food in front of him. The man is wearing a white shirt and black pants. The desk has a computer and a lamp on it. The room has a window with a view of the outside.\n\nThe image shows a"}, "117869": {"image_id": 117869, "Bleu_1": 0.26190476189852613, "Bleu_2": 0.17871654962201275, "Bleu_3": 0.11688708163230677, "Bleu_4": 0.07999424336103482, "METEOR": 0.21126202938223176, "ROUGE_L": 0.22197962154294032, "CIDEr": 3.071553957228505e-07, "SPICE": {"All": {"pr": 0.041666666666666664, "re": 0.045454545454545456, "f": 0.043478260869565216, "fn": 21.0, "numImages": 1.0, "fp": 23.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.09090909090909091, "re": 0.1, "f": 0.09523809523809525, "fn": 9.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}}, "caption": "The image shows a giraffe standing on a rocky outcropping, looking down at the ground. The giraffe has a long neck and legs, and its fur is brown and white. The background is a green and brown landscape with trees and rocks."}, "413538": {"image_id": 413538, "Bleu_1": 0.1874999999960938, "Bleu_2": 0.0631613940786591, "Bleu_3": 4.426378293466139e-07, "Bleu_4": 1.178238656172971e-09, "METEOR": 0.1597352167975409, "ROUGE_L": 0.15641025641025638, "CIDEr": 4.6107198298577715e-10, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.2608695652173913, "f": 0.2608695652173913, "fn": 17.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.2857142857142857, "f": 0.26666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 2.0}, "Object": {"pr": 0.3333333333333333, "re": 0.375, "f": 0.35294117647058826, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a group of people standing around a grill on the grass. There are several sausages on the grill, and one person is holding a plate of sausages. The people are wearing casual clothing, and there are several trees in the background.\n\nThe image is in"}, "71667": {"image_id": 71667, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.1650868521781302, "Bleu_3": 0.10158210507506187, "Bleu_4": 0.06733182132555685, "METEOR": 0.17591132796453657, "ROUGE_L": 0.26321467098166124, "CIDEr": 9.416912469421726e-11, "SPICE": {"All": {"pr": 0.1, "re": 0.12, "f": 0.1090909090909091, "fn": 22.0, "numImages": 1.0, "fp": 27.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.3, "f": 0.25, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "This is a black and white photograph of a city street. There are several buildings on either side of the street, including a large department store and several smaller shops. There are also several cars parked on the street. The sky is cloudy and there are no people in the image.\n\nThe image is"}, "162763": {"image_id": 162763, "Bleu_1": 0.22727272726756204, "Bleu_2": 0.14540168172223333, "Bleu_3": 0.07954810829018738, "Bleu_4": 1.0526316596948142e-05, "METEOR": 0.1842229634191436, "ROUGE_L": 0.2197406340057637, "CIDEr": 2.8820086002117725e-08, "SPICE": {"All": {"pr": 0.13793103448275862, "re": 0.13793103448275862, "f": 0.13793103448275862, "fn": 25.0, "numImages": 1.0, "fp": 25.0, "tp": 4.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.09090909090909091, "f": 0.09090909090909091, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "This is a photograph of a bicycle that is parked in a garage. The bicycle has a black frame and white tires. There are tools and other bicycle parts on the floor next to the bicycle.\n\nThe bicycle is parked in a garage with"}, "334170": {"image_id": 334170, "Bleu_1": 0.32692307691678996, "Bleu_2": 0.17902871850638163, "Bleu_3": 8.622339795942076e-07, "Bleu_4": 1.901821839939215e-09, "METEOR": 0.24714897939642003, "ROUGE_L": 0.25176886792452824, "CIDEr": 2.8566562271802367e-11, "SPICE": {"All": {"pr": 0.3157894736842105, "re": 0.2727272727272727, "f": 0.2926829268292683, "fn": 16.0, "numImages": 1.0, "fp": 13.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.6, "re": 0.75, "f": 0.6666666666666665, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 6.0}}, "caption": "This is a bathroom with a white sink, toilet, and shower. The walls are painted a warm beige color and the floor is made of white tiles. There is a large window on the left side of the room that lets in natural light. The bathroom is well lit and has a"}, "193407": {"image_id": 193407, "Bleu_1": 0.2653061224435652, "Bleu_2": 0.24657518067596285, "Bleu_3": 0.20843291422678475, "Bleu_4": 0.17712414574291133, "METEOR": 0.23825789401272163, "ROUGE_L": 0.27062999112688557, "CIDEr": 2.4648014181259182e-09, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.25, "f": 0.24000000000000002, "fn": 18.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.25, "re": 0.18181818181818182, "f": 0.2105263157894737, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5, "f": 0.36363636363636365, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a person standing in the snow, holding an umbrella. The person is wearing a black coat and black boots. The umbrella is open and the person is standing in the middle of a snowy sidewalk. There are buildings in the background.\n\nThe image is taken in"}, "553931": {"image_id": 553931, "Bleu_1": 0.29629629629080934, "Bleu_2": 0.22430886163262467, "Bleu_3": 0.16912968543226087, "Bleu_4": 0.1298831014470632, "METEOR": 0.2917573451142172, "ROUGE_L": 0.2814302191464821, "CIDEr": 2.5355514109147532e-12, "SPICE": {"All": {"pr": 0.06060606060606061, "re": 0.125, "f": 0.0816326530612245, "fn": 14.0, "numImages": 1.0, "fp": 31.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.14285714285714285, "re": 0.2857142857142857, "f": 0.19047619047619047, "fn": 5.0, "numImages": 1.0, "fp": 12.0, "tp": 2.0}}, "caption": "The image shows a display case filled with various types of donuts. The donuts are arranged on shelves in rows, with some of them being stacked on top of each other. The display case has a glass front and is surrounded by a wooden frame. The donuts are arranged in rows on the sh"}, "186927": {"image_id": 186927, "Bleu_1": 0.18867924527945892, "Bleu_2": 0.10433283794475068, "Bleu_3": 5.976183926619197e-07, "Bleu_4": 1.4373928004601428e-09, "METEOR": 0.11675470847601761, "ROUGE_L": 0.2381483547127719, "CIDEr": 2.1554635872209817e-10, "SPICE": {"All": {"pr": 0.3076923076923077, "re": 0.2962962962962963, "f": 0.3018867924528302, "fn": 19.0, "numImages": 1.0, "fp": 18.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.4, "f": 0.3636363636363636, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.46153846153846156, "re": 0.46153846153846156, "f": 0.46153846153846156, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a bedroom with a television on the floor and a shelf with various items on it. The room is dimly lit and there are several bags of food on the shelf.\n\nThe image is taken in a bedroom with a television on the floor and a shelf with various items"}, "353510": {"image_id": 353510, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.1942571724677524, "Bleu_3": 0.14357582278478706, "Bleu_4": 0.08771391269821045, "METEOR": 0.2288156078437604, "ROUGE_L": 0.27774615822424586, "CIDEr": 1.863544183485975e-11, "SPICE": {"All": {"pr": 0.12, "re": 0.1875, "f": 0.14634146341463414, "fn": 13.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.6, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a man and woman standing in a kitchen. The man is wearing a black shirt and the woman is wearing a white shirt. They are both holding cups of coffee and looking at something on the counter. The kitchen has wooden cabinets and a white countertop. There is a window"}, "202810": {"image_id": 202810, "Bleu_1": 0.33928571427965565, "Bleu_2": 0.19238759578427378, "Bleu_3": 0.12716293502632556, "Bleu_4": 0.09385532954706814, "METEOR": 0.22486207421479149, "ROUGE_L": 0.30049261083743845, "CIDEr": 2.8179847004917487e-07, "SPICE": {"All": {"pr": 0.3076923076923077, "re": 0.34782608695652173, "f": 0.32653061224489793, "fn": 15.0, "numImages": 1.0, "fp": 18.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.6363636363636364, "re": 0.7777777777777778, "f": 0.7000000000000001, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 7.0}}, "caption": "The image shows a street with several buildings on either side. The buildings are made of brick and have a mix of old and new architecture. There are cars parked on the side of the road and snow on the ground. The sky is clear and blue.\n\nThe image is taken from a high angle, looking"}, "133279": {"image_id": 133279, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.12366938847771936, "Bleu_3": 6.783327450171972e-07, "Bleu_4": 1.5968781455284126e-09, "METEOR": 0.25965146993700955, "ROUGE_L": 0.17498565691336773, "CIDEr": 3.0506130808318013e-10, "SPICE": {"All": {"pr": 0.25, "re": 0.23076923076923078, "f": 0.24000000000000002, "fn": 20.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.46153846153846156, "re": 0.5454545454545454, "f": 0.4999999999999999, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a parking lot with several cars parked in it. There are also some airplanes in the background. The sky is clear and there are no clouds in sight. The ground is dry and there are some cacti growing in the dirt.\n\nThe image is taken from a low"}, "169169": {"image_id": 169169, "Bleu_1": 0.4655172413712842, "Bleu_2": 0.3258383017807723, "Bleu_3": 0.2116388606440527, "Bleu_4": 0.15079474334619494, "METEOR": 0.27920953319435216, "ROUGE_L": 0.3489037178265014, "CIDEr": 1.1377443862448361e-07, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.16666666666666666, "f": 0.17857142857142855, "fn": 25.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.07692307692307693, "re": 0.08333333333333333, "f": 0.08, "fn": 11.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.36363636363636365, "f": 0.380952380952381, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a group of people standing in front of a building with a green and white sign that reads \"Spring Kitchen\". The building has a large window on the side and a door on the front. There are several trees in the area, including a large tree in the center of the image. The people in"}, "143503": {"image_id": 143503, "Bleu_1": 0.1224489795893378, "Bleu_2": 0.07142857142709853, "Bleu_3": 4.770332929412207e-07, "Bleu_4": 1.23943015938953e-09, "METEOR": 0.07990295096943748, "ROUGE_L": 0.11289327575570633, "CIDEr": 6.363012272705682e-12, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.2608695652173913, "f": 0.23529411764705882, "fn": 17.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.6666666666666666, "f": 0.5714285714285715, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 6.0}}, "caption": "The image shows a horse and rider standing on the sidewalk in front of a building. The horse is wearing a saddle and bridle, and the rider is wearing a helmet and riding boots. The building in the background is a large, white stone structure with arched windows and"}, "551454": {"image_id": 551454, "Bleu_1": 0.22807017543459532, "Bleu_2": 0.15632047282649467, "Bleu_3": 0.0961390165871351, "Bleu_4": 0.06369074319031066, "METEOR": 0.2098626342531017, "ROUGE_L": 0.2733893557422969, "CIDEr": 1.947765966244794e-14, "SPICE": {"All": {"pr": 0.25, "re": 0.4375, "f": 0.3181818181818182, "fn": 9.0, "numImages": 1.0, "fp": 21.0, "tp": 7.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.6, "f": 0.42857142857142855, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.4, "re": 1.0, "f": 0.5714285714285715, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Object": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a large clock tower in the middle of a city. The clock tower is made of stone and has a large clock face on the front. There are people walking on the sidewalk in front of the clock tower. The sky is blue and there are trees and buildings in the background.\n\nThe clock"}, "304044": {"image_id": 304044, "Bleu_1": 0.20370370369993146, "Bleu_2": 0.06199569426559689, "Bleu_3": 4.1966869140147383e-07, "Bleu_4": 1.0972040044962828e-09, "METEOR": 0.10080346728811328, "ROUGE_L": 0.13974799541809851, "CIDEr": 1.5415737127449299e-12, "SPICE": {"All": {"pr": 0.3157894736842105, "re": 0.2857142857142857, "f": 0.3, "fn": 15.0, "numImages": 1.0, "fp": 13.0, "tp": 6.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.1111111111111111, "f": 0.13333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.5714285714285714, "re": 0.5714285714285714, "f": 0.5714285714285714, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 4.0}}, "caption": "The image shows a street with a red light at the intersection. There are cars parked on both sides of the street and a few pedestrians walking in the crosswalk. The sky is a deep orange color and there are clouds in the distance.\n\nThe image is taken from the perspective of a driver"}, "536741": {"image_id": 536741, "Bleu_1": 0.3773584905589178, "Bleu_2": 0.2555622163811263, "Bleu_3": 0.17238297400489144, "Bleu_4": 1.7890738820867685e-05, "METEOR": 0.25995247179158126, "ROUGE_L": 0.3814486711829078, "CIDEr": 4.193834910928083e-06, "SPICE": {"All": {"pr": 0.4090909090909091, "re": 0.32142857142857145, "f": 0.36000000000000004, "fn": 19.0, "numImages": 1.0, "fp": 13.0, "tp": 9.0}, "Relation": {"pr": 0.375, "re": 0.25, "f": 0.3, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5454545454545454, "re": 0.46153846153846156, "f": 0.4999999999999999, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 6.0}}, "caption": "The man in the image is wearing a suit and tie, and holding a bottle of wine. He is standing in front of a bar with several bottles of wine on it. The room is dimly lit and there are several chairs and tables in it.\n\nThe man in the image is we"}, "505144": {"image_id": 505144, "Bleu_1": 0.3399999999932, "Bleu_2": 0.23560603574482045, "Bleu_3": 0.1794873574718552, "Bleu_4": 0.13860574992309266, "METEOR": 0.24348284782879015, "ROUGE_L": 0.279862362772407, "CIDEr": 3.166350948450172e-08, "SPICE": {"All": {"pr": 0.25806451612903225, "re": 0.25806451612903225, "f": 0.25806451612903225, "fn": 23.0, "numImages": 1.0, "fp": 23.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.6666666666666666, "f": 0.4444444444444444, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Object": {"pr": 0.5454545454545454, "re": 0.42857142857142855, "f": 0.4799999999999999, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 6.0}}, "caption": "The image shows a person standing next to a suitcase with a tag on it. The person is wearing a pair of boots and has a backpack on their back. The suitcase has a tag on it with the words `Luggage` and `Check` written on it. The person is standing"}, "15883": {"image_id": 15883, "Bleu_1": 0.1694915254208561, "Bleu_2": 0.05405800291926767, "Bleu_3": 3.7149107099468515e-07, "Bleu_4": 9.781698624583787e-10, "METEOR": 0.11937172774869109, "ROUGE_L": 0.15993707393812268, "CIDEr": 8.97678568793585e-16, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.4, "f": 0.23529411764705882, "fn": 6.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.3333333333333333, "f": 0.14285714285714288, "fn": 2.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.75, "f": 0.4285714285714285, "fn": 1.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a group of people standing around a table with various food items on it. One person is cutting up a piece of meat with a knife while another person is holding a plate with a piece of bread on it. There are several other people in the background, some of whom are holding plates and ut"}, "478857": {"image_id": 478857, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.17733172552966228, "Bleu_3": 0.12196360807045269, "Bleu_4": 0.09184122179475868, "METEOR": 0.17226324122609724, "ROUGE_L": 0.23487348734873487, "CIDEr": 1.424076670953108e-11, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.2727272727272727, "f": 0.24489795918367346, "fn": 16.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.625, "f": 0.4761904761904762, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a street with a few cars parked on the side of the road. There are some trees and houses in the background. The sky is clear and blue.\n\nThe image is taken from a bird's eye view, looking down on the street. The camera is positioned at a slight angle"}, "36252": {"image_id": 36252, "Bleu_1": 0.22448979591378598, "Bleu_2": 0.13677530110522793, "Bleu_3": 7.355956748968672e-07, "Bleu_4": 1.7151018577788952e-09, "METEOR": 0.12617422701098027, "ROUGE_L": 0.19242902208201892, "CIDEr": 5.935410137860084e-11, "SPICE": {"All": {"pr": 0.25, "re": 0.21739130434782608, "f": 0.23255813953488372, "fn": 18.0, "numImages": 1.0, "fp": 15.0, "tp": 5.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.125, "f": 0.15384615384615385, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.375, "f": 0.35294117647058826, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a group of people surfing on the ocean. They are wearing wetsuits and standing on their surfboards, waiting for the next wave to come in. The sky is clear and the sun is shining. The water is choppy and the waves are crashing against the shore."}, "145747": {"image_id": 145747, "Bleu_1": 0.22413793103061835, "Bleu_2": 0.1659087424695837, "Bleu_3": 0.09943215161816608, "Bleu_4": 0.06502112142721321, "METEOR": 0.18801619054189536, "ROUGE_L": 0.2406989853438557, "CIDEr": 8.744430547724745e-15, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.21739130434782608, "f": 0.20408163265306123, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.5, "f": 0.4347826086956522, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a group of boats on a lake surrounded by trees in the fall. The boats are white and have red and blue stripes on them. The sky is cloudy and there are some clouds in the distance. The water is calm and there are no waves. The trees are orange and yellow and there are"}, "119677": {"image_id": 119677, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.20603150145092247, "Bleu_3": 0.15236855057487514, "Bleu_4": 0.11076550621858337, "METEOR": 0.20151848505589015, "ROUGE_L": 0.30118144947980957, "CIDEr": 1.657682027209932e-09, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.29411764705882354, "f": 0.22727272727272727, "fn": 12.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2222222222222222, "re": 0.2857142857142857, "f": 0.25, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.6, "f": 0.37499999999999994, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image is a small piece of cheesecake on a plate with a knife and fork on the side. The cheesecake has a square shape and is made of a light brown color. There are no other ingredients visible on the plate.\n\nThe plate is made of a dark brown"}, "145101": {"image_id": 145101, "Bleu_1": 0.18518518518175586, "Bleu_2": 0.1182211503531082, "Bleu_3": 6.453505325095797e-07, "Bleu_4": 1.5151450472612192e-09, "METEOR": 0.15428517807589515, "ROUGE_L": 0.1821983273596177, "CIDEr": 5.922168395491567e-13, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.3157894736842105, "f": 0.2553191489361702, "fn": 13.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.2857142857142857, "f": 0.23529411764705882, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.5, "f": 0.3529411764705882, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The room is empty except for a couch, a television, and a lamp. There is a window on the left side of the room with blinds. The floor is made of hardwood and the walls are painted white. The ceiling is made of wood and has a fan in it. There is a door"}, "27476": {"image_id": 27476, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.17149858513917848, "Bleu_3": 0.10556671919572966, "Bleu_4": 0.06999971125159092, "METEOR": 0.22535501008837164, "ROUGE_L": 0.26293103448275856, "CIDEr": 4.076671442499263e-09, "SPICE": {"All": {"pr": 0.2, "re": 0.2727272727272727, "f": 0.23076923076923075, "fn": 16.0, "numImages": 1.0, "fp": 24.0, "tp": 6.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.1111111111111111, "f": 0.09523809523809525, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.4, "re": 0.2857142857142857, "f": 0.3333333333333333, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.23076923076923078, "re": 0.5, "f": 0.3157894736842105, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a woman walking down the street on her cell phone. She is wearing a plaid shirt and shorts, and has a pair of sunglasses on her face. The woman is walking in the direction of the camera, and there are several cars parked on the side of the road"}, "463534": {"image_id": 463534, "Bleu_1": 0.2653061224435652, "Bleu_2": 0.16624095289858323, "Bleu_3": 8.377724227293196e-07, "Bleu_4": 1.890841333722765e-09, "METEOR": 0.21909070163070557, "ROUGE_L": 0.2945081472540736, "CIDEr": 5.547938142442802e-09, "SPICE": {"All": {"pr": 0.06896551724137931, "re": 0.08333333333333333, "f": 0.07547169811320754, "fn": 22.0, "numImages": 1.0, "fp": 27.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2222222222222222, "f": 0.19999999999999998, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a man lying in bed with his dog on his chest. The man is wearing a black shirt and pajama pants, and the dog is wearing a green collar. The bed is covered in a green and white striped blanket, and there are pillows and a"}, "195086": {"image_id": 195086, "Bleu_1": 0.2553191489307379, "Bleu_2": 0.12903971808302675, "Bleu_3": 0.07179233837245348, "Bleu_4": 9.576248453510562e-06, "METEOR": 0.13818394327382252, "ROUGE_L": 0.22775357809583074, "CIDEr": 1.1948108380093677e-08, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.13333333333333333, "f": 0.1212121212121212, "fn": 13.0, "numImages": 1.0, "fp": 16.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows a young girl in pink pajamas standing on a yoga mat, stretching her arms up towards the ceiling. She is wearing a pink headband and has a pink scarf tied around her waist. The floor is made of hardwood and there is a white"}, "143359": {"image_id": 143359, "Bleu_1": 0.27083333332769105, "Bleu_2": 0.13148092653242613, "Bleu_3": 7.216432604089977e-07, "Bleu_4": 1.6999603483372018e-09, "METEOR": 0.1290156260343927, "ROUGE_L": 0.19110275689223058, "CIDEr": 8.695236973063018e-10, "SPICE": {"All": {"pr": 0.32142857142857145, "re": 0.28125, "f": 0.30000000000000004, "fn": 23.0, "numImages": 1.0, "fp": 19.0, "tp": 9.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.1, "f": 0.11764705882352941, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.25, "f": 0.26666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.42857142857142855, "re": 0.42857142857142855, "f": 0.42857142857142855, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The image shows a skateboarder performing a trick on a ledge in a park. The skateboarder is wearing a black shirt and black pants, and has a black helmet on his head. The ledge is made of concrete and has a small gap in it. The park is"}, "13383": {"image_id": 13383, "Bleu_1": 0.3157894736786704, "Bleu_2": 0.2601329908526316, "Bleu_3": 0.20497870245473493, "Bleu_4": 0.1589271563113917, "METEOR": 0.2406251970607217, "ROUGE_L": 0.2733893557422969, "CIDEr": 2.0784124040533197e-14, "SPICE": {"All": {"pr": 0.2, "re": 0.3125, "f": 0.24390243902439027, "fn": 11.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.14285714285714285, "f": 0.1111111111111111, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.16666666666666666, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.25, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.2727272727272727, "re": 1.0, "f": 0.42857142857142855, "fn": 0.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a black and white cat sleeping on a table next to a cup of coffee. The cat is lying on its side with its paws tucked under its body. The table has a laptop on it with a keyboard and mouse. There is a cup of coffee on the table next to the laptop"}, "392915": {"image_id": 392915, "Bleu_1": 0.21999999999560005, "Bleu_2": 0.13401187884939042, "Bleu_3": 0.07205793047605258, "Bleu_4": 9.44575929237223e-06, "METEOR": 0.25042454482198345, "ROUGE_L": 0.22938079719227877, "CIDEr": 9.449994816890984e-10, "SPICE": {"All": {"pr": 0.27586206896551724, "re": 0.4, "f": 0.32653061224489793, "fn": 12.0, "numImages": 1.0, "fp": 21.0, "tp": 8.0}, "Relation": {"pr": 0.0625, "re": 0.1111111111111111, "f": 0.08, "fn": 8.0, "numImages": 1.0, "fp": 15.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.45454545454545453, "re": 0.7142857142857143, "f": 0.5555555555555556, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a kitchen with white cabinets and countertops. There is a stainless steel sink in the center of the room, and a refrigerator on the right side. The floor is made of hardwood and there are plants on the windowsill. The walls are painted white and there are"}, "286544": {"image_id": 286544, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.15041420939612582, "Bleu_3": 7.677195064444563e-07, "Bleu_4": 1.7432228564084153e-09, "METEOR": 0.1912475607881641, "ROUGE_L": 0.18373493975903615, "CIDEr": 3.658260259096832e-12, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.16666666666666666, "f": 0.1702127659574468, "fn": 20.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of people walking down a sidewalk in a city. The people are walking in different directions, some on bicycles and some on foot. There are street signs and traffic lights on the sidewalk. The sky is clear and blue.\n\nThe image is taken from a low angle,"}, "59108": {"image_id": 59108, "Bleu_1": 0.1960784313687044, "Bleu_2": 0.15339299776643633, "Bleu_3": 0.11293938405626278, "Bleu_4": 0.07401568312653971, "METEOR": 0.16972062088202833, "ROUGE_L": 0.21997836278398844, "CIDEr": 1.2011113412409633e-10, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.15789473684210525, "f": 0.13333333333333333, "fn": 16.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.375, "f": 0.3157894736842105, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a red fire hydrant on the sidewalk.\n\nThe hydrant has a hose attached to it and is surrounded by a metal grate. The grate is covered in rust and has a small hole in the center. The hydrant is located on the sidewalk in front of a building"}, "318596": {"image_id": 318596, "Bleu_1": 0.20338983050502735, "Bleu_2": 0.11843515045025206, "Bleu_3": 0.06266554082896969, "Bleu_4": 8.141877941016982e-06, "METEOR": 0.24022491478052646, "ROUGE_L": 0.19192448872574724, "CIDEr": 1.4701656100898478e-12, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.4, "f": 0.2857142857142857, "fn": 9.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.5, "re": 0.3333333333333333, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 1.0, "f": 0.5555555555555556, "fn": 0.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a street sign with the words keep off dogs and medium written on it. The sign is on the side of the road and there are trees and buildings in the background.\n\nThe image is in black and white and the text is written in white. The sign is made of metal and has a rust"}, "346112": {"image_id": 346112, "Bleu_1": 0.2181818181778513, "Bleu_2": 0.19069251784561927, "Bleu_3": 0.16871867607116212, "Bleu_4": 0.14659327058139618, "METEOR": 0.22990395861436042, "ROUGE_L": 0.24956165984804207, "CIDEr": 9.393257730811938e-13, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.08823529411764706, "f": 0.10000000000000002, "fn": 31.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.08333333333333333, "f": 0.1111111111111111, "fn": 11.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.13333333333333333, "f": 0.14814814814814814, "fn": 13.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image is a black and white cat lying on a white bed. The cat has a white patch on its nose and is looking up at the camera. The background is a white bed with a white blanket on it.\n\nThe cat is lying on its side with its front paws tucked under its"}, "572861": {"image_id": 572861, "Bleu_1": 0.19999999999636367, "Bleu_2": 0.13608276348545723, "Bleu_3": 7.043309782608963e-07, "Bleu_4": 1.6100218008380818e-09, "METEOR": 0.14919805224722962, "ROUGE_L": 0.24358243011979464, "CIDEr": 2.6781523382254002e-12, "SPICE": {"All": {"pr": 0.2, "re": 0.22727272727272727, "f": 0.21276595744680854, "fn": 17.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5714285714285714, "f": 0.38095238095238093, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a horse and carriage in front of a building with a sign that reads \"The Stables\"\n\nThe horse is a brown and white draft horse with a red and white harness. The carriage is a white and gold carriage with a red and white canopy. The building in the background is a"}, "348905": {"image_id": 348905, "Bleu_1": 0.26315789473222534, "Bleu_2": 0.20565318641151553, "Bleu_3": 0.1566604610064578, "Bleu_4": 0.10923922468551896, "METEOR": 0.24058127302891263, "ROUGE_L": 0.3231176693151721, "CIDEr": 8.163836130775627e-13, "SPICE": {"All": {"pr": 0.2, "re": 0.21739130434782608, "f": 0.20833333333333331, "fn": 18.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.4, "re": 0.5714285714285714, "f": 0.47058823529411764, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a person on a jet ski in the water. The person is wearing a life jacket and is holding onto the handlebars of the jet ski. The water is calm and there are no other boats or people in the area. The sky is clear and there are no clouds. The sun is sh"}, "355881": {"image_id": 355881, "Bleu_1": 0.340909090901343, "Bleu_2": 0.23557764908855597, "Bleu_3": 0.174191231399375, "Bleu_4": 0.12671594061541108, "METEOR": 0.2192582055184445, "ROUGE_L": 0.30886075949367087, "CIDEr": 1.1928517292734986e-05, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.14285714285714285, "f": 0.12765957446808512, "fn": 18.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image is a bowl of food with various ingredients such as vegetables, fruits, and meat. The bowl is made of plastic and has a pink and white striped pattern on the side. There are also various condiments such as soy sauce, mayonnaise, and"}, "402909": {"image_id": 402909, "Bleu_1": 0.26315789473222534, "Bleu_2": 0.1187339225343928, "Bleu_3": 0.06352270457072846, "Bleu_4": 8.300386118896969e-06, "METEOR": 0.16471367655942257, "ROUGE_L": 0.2627894453419494, "CIDEr": 4.666047468770333e-12, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.2, "f": 0.1923076923076923, "fn": 20.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.2857142857142857, "f": 0.2857142857142857, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.2727272727272727, "f": 0.2727272727272727, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a train station with a platform and tracks leading to the tracks. There are several buildings in the background, including a large building with a clock tower. The sky is cloudy and there is snow on the ground.\n\nThe train station is a large building with a platform and tracks leading to the tracks"}, "421286": {"image_id": 421286, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.23302069120966015, "Bleu_3": 0.16316474245036913, "Bleu_4": 0.09703328174476994, "METEOR": 0.2977106728783362, "ROUGE_L": 0.37089387239418825, "CIDEr": 4.09358243931148e-09, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.16666666666666666, "f": 0.17391304347826086, "fn": 20.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.2857142857142857, "f": 0.36363636363636365, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a plate with two slices of pizza on it. The pizza has mozzarella cheese on top and is being held by a fork. There is a glass of water on the table next to the plate. The background is a wooden table with a red and white checkered table"}, "559956": {"image_id": 559956, "Bleu_1": 0.222222222217284, "Bleu_2": 0.1421338109005459, "Bleu_3": 0.09794571638096423, "Bleu_4": 0.06877439247188379, "METEOR": 0.16558147829946387, "ROUGE_L": 0.21048999309868874, "CIDEr": 5.7622255009757154e-09, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.2, "f": 0.1951219512195122, "fn": 16.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.5, "f": 0.4444444444444445, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a group of people standing around a flock of sheep in a barn. The people are wearing different types of clothing, including jeans, t-shirts, and jackets. The sheep are wearing collars and are standing in a line. The barn has a wooden"}, "275661": {"image_id": 275661, "Bleu_1": 0.2264150943353507, "Bleu_2": 0.174582229926461, "Bleu_3": 0.08423190681730008, "Bleu_4": 1.0455985518811585e-05, "METEOR": 0.20282938059985517, "ROUGE_L": 0.24811156304474144, "CIDEr": 7.84636926504437e-11, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.1, "f": 0.11764705882352941, "fn": 27.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a room with a wooden floor, a desk with a computer on it, and a chair in the corner. There is a window on the wall with a curtain hanging from it. The room is well lit and has a white ceiling.\n\nThe image shows a room with a wooden"}, "318063": {"image_id": 318063, "Bleu_1": 0.3061224489733445, "Bleu_2": 0.23957871187003724, "Bleu_3": 0.19423013610928908, "Bleu_4": 0.1335997058279157, "METEOR": 0.23099741859456155, "ROUGE_L": 0.25957446808510637, "CIDEr": 1.7022274330004266e-09, "SPICE": {"All": {"pr": 0.24242424242424243, "re": 0.5714285714285714, "f": 0.3404255319148936, "fn": 6.0, "numImages": 1.0, "fp": 25.0, "tp": 8.0}, "Relation": {"pr": 0.18181818181818182, "re": 0.5, "f": 0.26666666666666666, "fn": 2.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.5, "f": 0.22222222222222224, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.625, "f": 0.43478260869565216, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 5.0}}, "caption": "The image shows a surfer riding a wave on a surfboard. The surfer is wearing a red shirt and has his arms outstretched as he rides the wave. The wave is white and has a lot of foam on it. The sky is blue and there are clouds in"}, "118739": {"image_id": 118739, "Bleu_1": 0.333333333325926, "Bleu_2": 0.21320071635081886, "Bleu_3": 0.1283451966525187, "Bleu_4": 1.4978640318582358e-05, "METEOR": 0.1800772667243267, "ROUGE_L": 0.26804770872567485, "CIDEr": 9.637607283434865e-08, "SPICE": {"All": {"pr": 0.2413793103448276, "re": 0.25, "f": 0.24561403508771928, "fn": 21.0, "numImages": 1.0, "fp": 22.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2222222222222222, "re": 0.18181818181818182, "f": 0.19999999999999998, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.4166666666666667, "re": 0.5555555555555556, "f": 0.4761904761904762, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a pizza with red sauce, mozzarella cheese, and various toppings on a white plate. There are two slices of pizza on the plate, one with red sauce and the other with white sauce. The pizza slices are placed on top of a"}, "80666": {"image_id": 80666, "Bleu_1": 0.25423728813128416, "Bleu_2": 0.21958464589702661, "Bleu_3": 0.19672393261118726, "Bleu_4": 0.1756387758090616, "METEOR": 0.34598294136949287, "ROUGE_L": 0.35863174772848744, "CIDEr": 1.7663524485101734e-14, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.18518518518518517, "f": 0.17857142857142858, "fn": 22.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.09090909090909091, "f": 0.09999999999999999, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4, "f": 0.34782608695652173, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a white and orange cat sitting on a bench in front of a building with a red roof. The cat is looking up at the camera with its ears perked up. The building has a large window on the top floor with a view of the surrounding area. There are trees and grass in the background"}, "44767": {"image_id": 44767, "Bleu_1": 0.3061224489733445, "Bleu_2": 0.17857142856774635, "Bleu_3": 0.11070962028887511, "Bleu_4": 0.07369695820474931, "METEOR": 0.2623792831254499, "ROUGE_L": 0.26940063091482647, "CIDEr": 2.1864947960103357e-07, "SPICE": {"All": {"pr": 0.2, "re": 0.2777777777777778, "f": 0.23255813953488372, "fn": 13.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.125, "re": 0.3333333333333333, "f": 0.18181818181818182, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.6666666666666666, "f": 0.4705882352941177, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a plate of fish with vegetables and a fork. The fish is white and has a flaky texture. The vegetables are carrots, onions, and parsley. The fork is on the side of the plate. The plate is white and has a pattern of lines on it."}, "31092": {"image_id": 31092, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.1456862718139933, "Bleu_3": 0.0761842752874376, "Bleu_4": 9.848600952552255e-06, "METEOR": 0.1821312203911, "ROUGE_L": 0.28405122235157165, "CIDEr": 5.963972683816881e-10, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.38461538461538464, "f": 0.2702702702702703, "fn": 8.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.1, "re": 0.2, "f": 0.13333333333333333, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 1.0, "f": 0.5333333333333333, "fn": 0.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a person riding a bicycle on the sidewalk. The person is wearing a black helmet and has a backpack on their back. The bicycle is red and has a white basket on the front. The sidewalk is made of concrete and has a few cracks in it"}, "6589": {"image_id": 6589, "Bleu_1": 0.24561403508341031, "Bleu_2": 0.21964884254960187, "Bleu_3": 0.1519561844166874, "Bleu_4": 0.10676965611634169, "METEOR": 0.2757322316471729, "ROUGE_L": 0.2718230285668815, "CIDEr": 6.488917617986506e-14, "SPICE": {"All": {"pr": 0.2631578947368421, "re": 0.16666666666666666, "f": 0.2040816326530612, "fn": 25.0, "numImages": 1.0, "fp": 14.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 1.0, "re": 0.2, "f": 0.33333333333333337, "fn": 8.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.2, "f": 0.33333333333333337, "fn": 4.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.2727272727272727, "f": 0.3, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a cat sitting on a couch with a green tie around its neck. The cat is looking up at the camera with its eyes. The couch is covered in a patterned fabric and there is a lamp on the table next to the couch. The room is well lit and there are no other"}, "207937": {"image_id": 207937, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.19897929585627183, "Bleu_3": 0.13343067301000014, "Bleu_4": 1.4838593493954004e-05, "METEOR": 0.18116452660721097, "ROUGE_L": 0.21580188679245285, "CIDEr": 7.987261889754052e-11, "SPICE": {"All": {"pr": 0.12903225806451613, "re": 0.16666666666666666, "f": 0.14545454545454548, "fn": 20.0, "numImages": 1.0, "fp": 27.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.06666666666666667, "re": 0.16666666666666666, "f": 0.09523809523809522, "fn": 5.0, "numImages": 1.0, "fp": 14.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a group of people standing in front of a building with an umbrella. The building has a green roof and white walls. The people are wearing raincoats and standing in the rain. The image is taken from a low angle, looking up at the people. The lighting is natural,"}, "378454": {"image_id": 378454, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.13026252642719696, "Bleu_3": 6.97519462339574e-07, "Bleu_4": 1.6222516606715129e-09, "METEOR": 0.19935648397663241, "ROUGE_L": 0.24151583710407235, "CIDEr": 2.4895913947978594e-10, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.17391304347826086, "f": 0.15999999999999998, "fn": 19.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4, "f": 0.4210526315789474, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a person jumping off a sandy beach into the ocean. The person is wearing a white shirt and shorts, and has a red and white striped ball in their hand. The sky is blue and there are some clouds in the background. The beach is lined with palm trees"}, "252495": {"image_id": 252495, "Bleu_1": 0.19148936169805347, "Bleu_2": 0.12903971808302675, "Bleu_3": 0.09045267833662955, "Bleu_4": 1.1388142795948561e-05, "METEOR": 0.20605828329816708, "ROUGE_L": 0.24416277518345564, "CIDEr": 1.2415073339857426e-09, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.1875, "f": 0.14285714285714285, "fn": 13.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.08333333333333333, "re": 0.3333333333333333, "f": 0.13333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.125, "re": 0.5, "f": 0.2, "fn": 1.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.2222222222222222, "f": 0.23529411764705882, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows two people on surfboards in the water. They are wearing wetsuits and sunglasses. The water is calm and there are trees in the background.\n\nThe image is taken from a distance, so the details are not very clear. The people on the surfboards are"}, "368280": {"image_id": 368280, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.11895773785545558, "Bleu_3": 0.08217664461402947, "Bleu_4": 0.05771900053824921, "METEOR": 0.25343254624827066, "ROUGE_L": 0.21721068249258166, "CIDEr": 5.1081454883432024e-12, "SPICE": {"All": {"pr": 0.06896551724137931, "re": 0.1, "f": 0.08163265306122448, "fn": 18.0, "numImages": 1.0, "fp": 27.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.25, "f": 0.1904761904761905, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a group of young boys playing soccer on a grass field. They are wearing black and white jerseys and black shorts, and one of them is kicking the ball with his left foot. The other boys are watching him and trying to get the ball. There are trees in the"}, "557323": {"image_id": 557323, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.14254579382569355, "Bleu_3": 0.09270920944879872, "Bleu_4": 0.06318291480331194, "METEOR": 0.23179597040226235, "ROUGE_L": 0.28103661044837513, "CIDEr": 8.322186100045518e-11, "SPICE": {"All": {"pr": 0.12121212121212122, "re": 0.17391304347826086, "f": 0.14285714285714288, "fn": 19.0, "numImages": 1.0, "fp": 29.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 17.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.4444444444444444, "f": 0.4210526315789474, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a soccer player in blue and white uniform playing soccer on a field. The player is kicking the ball with his right foot and running towards the goal. The crowd is cheering in the background.\n\nThe image is taken from a high angle, looking down on the field. The sun"}, "120021": {"image_id": 120021, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.14686261420159358, "Bleu_3": 0.07606919312543149, "Bleu_4": 9.785799152874335e-06, "METEOR": 0.22402652810874268, "ROUGE_L": 0.3285457809694794, "CIDEr": 1.559496638794335e-10, "SPICE": {"All": {"pr": 0.23809523809523808, "re": 0.15151515151515152, "f": 0.18518518518518517, "fn": 28.0, "numImages": 1.0, "fp": 16.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.3125, "f": 0.35714285714285715, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a black and red motorcycle parked on a concrete floor. The motorcycle has a sleek, modern design with a black body and red seat. The tires are black and the wheels are red. The motorcycle is parked on a concrete floor with a red and black striped pattern"}, "534038": {"image_id": 534038, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.15070298814247132, "Bleu_3": 0.07587175464144183, "Bleu_4": 9.619833423045096e-06, "METEOR": 0.21952788334177445, "ROUGE_L": 0.20962199312714777, "CIDEr": 3.573212399639762e-11, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.2, "f": 0.1702127659574468, "fn": 16.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.3333333333333333, "f": 0.18181818181818182, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.375, "f": 0.33333333333333326, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a group of people walking along the beach. They are all wearing hats and carrying bags. The sky is cloudy and there are some trees in the background. The water is calm and there are some boats in the distance.\n\nThe image is in black and white and has a v"}, "146240": {"image_id": 146240, "Bleu_1": 0.2888888888824692, "Bleu_2": 0.21438189279304845, "Bleu_3": 0.16230151705609028, "Bleu_4": 0.13219344650626488, "METEOR": 0.2691757586675709, "ROUGE_L": 0.3917790622992935, "CIDEr": 1.8254830585746847e-07, "SPICE": {"All": {"pr": 0.125, "re": 0.12, "f": 0.12244897959183673, "fn": 22.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.08333333333333333, "re": 0.1, "f": 0.0909090909090909, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows a skateboarder performing a trick on a skateboard. The skateboarder is wearing a black shirt and black pants, and has a black hat on his head. The skateboard is black and has red wheels. The background is a concrete skate park with"}, "145325": {"image_id": 145325, "Bleu_1": 0.33333333332716053, "Bleu_2": 0.20982172726164103, "Bleu_3": 0.13643933968476482, "Bleu_4": 0.08400635913890077, "METEOR": 0.21115867325784682, "ROUGE_L": 0.2367487247726769, "CIDEr": 8.59433215351458e-08, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.23809523809523808, "f": 0.23255813953488372, "fn": 16.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4, "f": 0.380952380952381, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a desk with a computer, phone, and headset on it. There is also a chair in front of the desk. The walls are painted white and there are no windows in the room. The floor is made of hardwood. The room is well lit and there are no other objects in"}, "81251": {"image_id": 81251, "Bleu_1": 0.3749999999921875, "Bleu_2": 0.29625319819896073, "Bleu_3": 0.2372586752092234, "Bleu_4": 0.18562157151967446, "METEOR": 0.34759205102589996, "ROUGE_L": 0.43884892086330934, "CIDEr": 1.962941622711029e-08, "SPICE": {"All": {"pr": 0.2777777777777778, "re": 0.21739130434782608, "f": 0.24390243902439024, "fn": 18.0, "numImages": 1.0, "fp": 13.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.4, "re": 0.4, "f": 0.4000000000000001, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.375, "re": 0.3333333333333333, "f": 0.35294117647058826, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a pizza with tomato sauce, mozzarella cheese, and fresh basil leaves on top of a white plate. The pizza is served on a wooden table with a glass of wine next to it. The background is a white wall with a window in the background."}, "376531": {"image_id": 376531, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.21271781490162775, "Bleu_3": 0.15354390128889114, "Bleu_4": 0.09270974964697601, "METEOR": 0.25501772819704177, "ROUGE_L": 0.2688916876574307, "CIDEr": 3.558839698524973e-12, "SPICE": {"All": {"pr": 0.24, "re": 0.2, "f": 0.2181818181818182, "fn": 24.0, "numImages": 1.0, "fp": 19.0, "tp": 6.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.1, "f": 0.10526315789473685, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.3333333333333333, "f": 0.380952380952381, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a young boy playing baseball in a backyard. The boy is holding a bat and wearing a baseball cap. The background is a fenced in area with a grassy field and trees in the distance.\n\nThe image is well lit and the colors are vibrant. The boy is in"}, "360328": {"image_id": 360328, "Bleu_1": 0.27272727272107444, "Bleu_2": 0.23891934293536143, "Bleu_3": 0.1894121261728855, "Bleu_4": 0.13493274921266893, "METEOR": 0.30649557164037566, "ROUGE_L": 0.35835509138381205, "CIDEr": 2.8914437313794904e-07, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.21739130434782608, "f": 0.21739130434782608, "fn": 18.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.1, "f": 0.11764705882352941, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5714285714285714, "f": 0.4444444444444444, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The woman in the image is wearing a white shirt and has a phone in her hand. She is standing in a kitchen with a countertop and stove. There is a refrigerator in the background. The woman appears to be talking on the phone."}, "283785": {"image_id": 283785, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.11673403265989704, "Bleu_3": 0.06399271623835165, "Bleu_4": 8.466528301619355e-06, "METEOR": 0.21106725446264188, "ROUGE_L": 0.1783625730994152, "CIDEr": 2.9005547698056562e-11, "SPICE": {"All": {"pr": 0.2, "re": 0.22727272727272727, "f": 0.21276595744680854, "fn": 17.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.25, "re": 0.4, "f": 0.3076923076923077, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a group of people standing in front of a large bookshelf. They are all wearing yellow and orange costumes and holding books. There are several children in the group, including one who is sitting on the floor and reading a book. The adults are all standing and looking at the children."}, "181786": {"image_id": 181786, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.21852940772098994, "Bleu_3": 0.15846967859633185, "Bleu_4": 0.09592574224833643, "METEOR": 0.258843079769336, "ROUGE_L": 0.2776332899869961, "CIDEr": 6.119210517023463e-11, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.23809523809523808, "f": 0.20833333333333334, "fn": 16.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.2, "f": 0.15384615384615385, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4444444444444444, "f": 0.380952380952381, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a person skiing down a snowy mountain slope. The person is wearing a yellow jacket and black pants, and has a pair of skis on their feet. The sky is cloudy and there is snow on the ground.\n\nThe person is skiing down the mountain at a"}, "310622": {"image_id": 310622, "Bleu_1": 0.2916666666605903, "Bleu_2": 0.17614871215260033, "Bleu_3": 0.11049527392440639, "Bleu_4": 0.07399538747877654, "METEOR": 0.22256607133910158, "ROUGE_L": 0.20013123359580048, "CIDEr": 1.4381769306900848e-09, "SPICE": {"All": {"pr": 0.12, "re": 0.13043478260869565, "f": 0.12499999999999997, "fn": 20.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a street with several signs on the sidewalk. The signs are blue and white and have the words \"west 23rd street\" and \"korea way\" written on them. There are also several buildings in the background.\n\nThe image is taken from a high angle, looking down"}, "110231": {"image_id": 110231, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.15534712747310678, "Bleu_3": 0.11313211375448105, "Bleu_4": 0.07372935016556142, "METEOR": 0.21497034736166656, "ROUGE_L": 0.1920654911838791, "CIDEr": 5.401966383706314e-10, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.23809523809523808, "f": 0.20833333333333334, "fn": 16.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.1111111111111111, "f": 0.14285714285714285, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5714285714285714, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a person skiing down a snowy slope. The person is wearing a black and white ski suit, black ski boots, and a red ski helmet. The person is holding onto a pair of skis and appears to be in good physical condition. The background is a mountainous landscape with"}, "578849": {"image_id": 578849, "Bleu_1": 0.2941176470530566, "Bleu_2": 0.17149858513911254, "Bleu_3": 0.10628002947657487, "Bleu_4": 0.07071775095333474, "METEOR": 0.2080054515804839, "ROUGE_L": 0.19513755598208574, "CIDEr": 3.333579353264344e-11, "SPICE": {"All": {"pr": 0.125, "re": 0.15384615384615385, "f": 0.13793103448275862, "fn": 22.0, "numImages": 1.0, "fp": 28.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.3333333333333333, "f": 0.30769230769230765, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a person skiing down a snowy slope. The person is wearing a red and black suit with a black helmet and goggles. They are holding a pair of skis and have a pair of poles in their hands. The background is a snowy mountain with trees in the"}, "146887": {"image_id": 146887, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.1842190315423256, "Bleu_3": 0.12674769789184412, "Bleu_4": 0.08028899837277662, "METEOR": 0.2584367139385691, "ROUGE_L": 0.330722891566265, "CIDEr": 1.2463586449972308e-10, "SPICE": {"All": {"pr": 0.26666666666666666, "re": 0.4, "f": 0.32, "fn": 12.0, "numImages": 1.0, "fp": 22.0, "tp": 8.0}, "Relation": {"pr": 0.1, "re": 0.14285714285714285, "f": 0.11764705882352941, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 1.0, "f": 0.4, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.46153846153846156, "re": 0.75, "f": 0.5714285714285714, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows three green parrots perched on a tree branch. They are looking up at the sky and appear to be in a state of contemplation. The sky is blue and cloudless, with a few white clouds scattered across it. The trees are bare and the branches are thin and twisted."}, "215914": {"image_id": 215914, "Bleu_1": 0.36585365852766216, "Bleu_2": 0.16564728910817916, "Bleu_3": 8.894086581869481e-07, "Bleu_4": 2.0743408285785954e-09, "METEOR": 0.17436189113983705, "ROUGE_L": 0.21463757916959889, "CIDEr": 7.155356848949943e-07, "SPICE": {"All": {"pr": 0.2, "re": 0.18518518518518517, "f": 0.1923076923076923, "fn": 22.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.125, "f": 0.11764705882352941, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of people standing in front of a building with a sign that reads \"Outdoor Recreation\". They are all wearing winter clothing and some are holding skis and snowboards. There are also several snowballs on the ground."}, "544655": {"image_id": 544655, "Bleu_1": 0.28571428570845486, "Bleu_2": 0.18898223650071672, "Bleu_3": 0.09125318612878565, "Bleu_4": 1.1336958836408244e-05, "METEOR": 0.21459825166907928, "ROUGE_L": 0.27566171723692706, "CIDEr": 7.339997853907804e-10, "SPICE": {"All": {"pr": 0.32, "re": 0.27586206896551724, "f": 0.29629629629629634, "fn": 21.0, "numImages": 1.0, "fp": 17.0, "tp": 8.0}, "Relation": {"pr": 0.1, "re": 0.1, "f": 0.10000000000000002, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5833333333333334, "re": 0.5833333333333334, "f": 0.5833333333333334, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 7.0}}, "caption": "The image shows a young girl standing on the sidewalk, wearing a grey hoodie and a pair of sunglasses. She is smiling and looking down at her phone. There are people walking in the background, some of them are holding bags or walking dogs. The street is lined with"}, "34372": {"image_id": 34372, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.1295047816452144, "Bleu_3": 0.0685787273497866, "Bleu_4": 8.91763149698053e-06, "METEOR": 0.20834716703499245, "ROUGE_L": 0.24970760233918127, "CIDEr": 2.4139884109876047e-13, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.10344827586206896, "f": 0.12000000000000001, "fn": 26.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 12.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of people playing basketball in a gymnasium. They are wearing white shirts and black shorts, and one of them is holding a basketball. The floor is made of wood and there are hoops on either side of the court. The walls are painted blue and there are windows on"}, "447522": {"image_id": 447522, "Bleu_1": 0.26190476189852613, "Bleu_2": 0.17871654962201275, "Bleu_3": 0.0927733381723083, "Bleu_4": 1.1961929431463712e-05, "METEOR": 0.22027389211565512, "ROUGE_L": 0.2543786488740617, "CIDEr": 8.622854659498709e-07, "SPICE": {"All": {"pr": 0.25, "re": 0.22727272727272727, "f": 0.23809523809523808, "fn": 17.0, "numImages": 1.0, "fp": 15.0, "tp": 5.0}, "Relation": {"pr": 0.2, "re": 0.14285714285714285, "f": 0.16666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.4444444444444444, "f": 0.4210526315789474, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a plate of food with broccoli, carrots, and rice. The food is arranged in a pattern of alternating colors, with the broccoli and carrots on the bottom and the rice on top. The plate is on a white tablecloth."}, "451951": {"image_id": 451951, "Bleu_1": 0.31914893616342244, "Bleu_2": 0.20402970888419056, "Bleu_3": 9.743719400736514e-07, "Bleu_4": 2.1413142511476846e-09, "METEOR": 0.17288790060654322, "ROUGE_L": 0.23404527433175598, "CIDEr": 7.868250453054832e-09, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.13793103448275862, "f": 0.16, "fn": 25.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.3076923076923077, "f": 0.33333333333333337, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a man riding a skateboard on a bridge. He is wearing a black shirt and sunglasses, and has his arms outstretched as he rides. The sky is cloudy and there are some clouds in the background. The bridge is made of steel and has"}, "324634": {"image_id": 324634, "Bleu_1": 0.35555555554765433, "Bleu_2": 0.1556997888288053, "Bleu_3": 8.261061662208591e-07, "Bleu_4": 1.9140997885086772e-09, "METEOR": 0.19928073620514072, "ROUGE_L": 0.2053872053872054, "CIDEr": 2.3251608245229223e-08, "SPICE": {"All": {"pr": 0.2631578947368421, "re": 0.21739130434782608, "f": 0.23809523809523808, "fn": 18.0, "numImages": 1.0, "fp": 14.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.2, "f": 0.15384615384615385, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4, "f": 0.4210526315789474, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows two people standing in a room, one of them is holding a cell phone and the other is looking at it. The room is white and has a large window on one side. The people are dressed in black clothing and hats."}, "445658": {"image_id": 445658, "Bleu_1": 0.33333333332679743, "Bleu_2": 0.21602468994265056, "Bleu_3": 0.1418983411941997, "Bleu_4": 0.10445522730509256, "METEOR": 0.23806519989669542, "ROUGE_L": 0.26505276225946617, "CIDEr": 2.287291868963287e-09, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.17391304347826086, "f": 0.1904761904761905, "fn": 19.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.14285714285714285, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "This is a kitchen with wooden cabinets and a black stove. There is a refrigerator and a microwave on the counter. The floor is made of tile and there is a sink in the corner.\n\nThe kitchen has a lot of space and is well lit. There are no windows in"}, "381257": {"image_id": 381257, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.1748949264355777, "Bleu_3": 0.10767861130247747, "Bleu_4": 0.07141456170370668, "METEOR": 0.22006899920786793, "ROUGE_L": 0.29204069419509276, "CIDEr": 8.323032130646511e-11, "SPICE": {"All": {"pr": 0.2, "re": 0.18518518518518517, "f": 0.1923076923076923, "fn": 22.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.3333333333333333, "re": 0.09090909090909091, "f": 0.14285714285714288, "fn": 10.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4444444444444444, "f": 0.380952380952381, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a desk with a computer, mouse, and keyboard on it. There is also a pencil and paper on the desk. The background is a white wall with a window in the background. The lighting is from a lamp on the desk. The overall mood is calm and peace"}, "65394": {"image_id": 65394, "Bleu_1": 0.2553191489307379, "Bleu_2": 0.210720977237338, "Bleu_3": 0.18090535667325908, "Bleu_4": 0.14174433623978008, "METEOR": 0.30328232509354, "ROUGE_L": 0.31063017186505404, "CIDEr": 6.44584002228727e-08, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.16666666666666666, "f": 0.1111111111111111, "fn": 10.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.5, "f": 0.22222222222222224, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.09090909090909091, "re": 0.2, "f": 0.12500000000000003, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}}, "caption": "The image shows a man in a yellow life jacket and a dog on a kayak in the water. The man is standing on the dock and the dog is sitting on the kayak. The water is calm and there are trees and buildings in the background."}, "255135": {"image_id": 255135, "Bleu_1": 0.16071428571141586, "Bleu_2": 0.10811249552151897, "Bleu_3": 0.07564774850474067, "Bleu_4": 0.05345988714531164, "METEOR": 0.2094065972632385, "ROUGE_L": 0.20460644007155634, "CIDEr": 1.919188142567326e-12, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.20689655172413793, "f": 0.21052631578947367, "fn": 23.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.375, "re": 0.3, "f": 0.33333333333333326, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.3333333333333333, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.2727272727272727, "f": 0.2608695652173913, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a stop sign on the side of a road. The sign is made of metal and has a red and white background. The sign is in good condition and is easy to read. The road is paved and there are no other cars or pedestrians in sight. The sky is clear and there"}, "293315": {"image_id": 293315, "Bleu_1": 0.2075471698074048, "Bleu_2": 0.15475060698405138, "Bleu_3": 0.11209991078678341, "Bleu_4": 0.07285535003990262, "METEOR": 0.26798231851831134, "ROUGE_L": 0.25707405177603854, "CIDEr": 1.3956564597624623e-08, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.2, "f": 0.20833333333333331, "fn": 20.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.3333333333333333, "f": 0.32, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows three women sitting on a bench overlooking a valley. They are wearing casual clothing and are looking out at the view. The sky is clear and the sun is setting in the distance. The grass is green and there are trees in the background.\n\nThe image is taken from a"}, "491481": {"image_id": 491481, "Bleu_1": 0.29999999999400007, "Bleu_2": 0.23473823892604287, "Bleu_3": 0.1510135442554507, "Bleu_4": 0.09252039129896864, "METEOR": 0.21457236714443356, "ROUGE_L": 0.23252858958068615, "CIDEr": 9.031360063023383e-11, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.14814814814814814, "f": 0.14814814814814814, "fn": 23.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4444444444444444, "f": 0.380952380952381, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a giraffe standing on the grass in a clearing. The giraffe is brown with white spots on its back and legs. It has a long neck and long legs. The giraffe is looking up at the sky. The background is a green forest with trees and bushes."}, "50431": {"image_id": 50431, "Bleu_1": 0.33333333332679743, "Bleu_2": 0.19999999999603923, "Bleu_3": 9.345903743209011e-07, "Bleu_4": 2.0307462899251853e-09, "METEOR": 0.18844388286370245, "ROUGE_L": 0.2238532110091743, "CIDEr": 7.755465773943939e-10, "SPICE": {"All": {"pr": 0.2, "re": 0.19230769230769232, "f": 0.19607843137254902, "fn": 21.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.45454545454545453, "f": 0.45454545454545453, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a horse and carriage on a street in a city\n\nThe carriage is white and has a red and white striped canopy. The horses are black and have white manes and tails. The street is lined with buildings on either side, some of which have balconies and windows."}, "87889": {"image_id": 87889, "Bleu_1": 0.15999999999680004, "Bleu_2": 0.05714285714170266, "Bleu_3": 4.082199467684955e-07, "Bleu_4": 1.0968473790380015e-09, "METEOR": 0.13904874487095867, "ROUGE_L": 0.18944099378881987, "CIDEr": 1.8097646730527854e-09, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.25, "f": 0.22727272727272727, "fn": 15.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.5, "f": 0.25, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows two people skiing down a snowy slope. They are wearing ski gear and carrying poles. The trees in the background are bare and the sky is cloudy.\n\nThe image is taken from a low angle, looking down the slope. The lighting is soft and the colors are"}, "499716": {"image_id": 499716, "Bleu_1": 0.16981132075151303, "Bleu_2": 0.08081586876451534, "Bleu_3": 5.040508737561903e-07, "Bleu_4": 1.2650662740672959e-09, "METEOR": 0.12261174946976555, "ROUGE_L": 0.17722254503195814, "CIDEr": 3.5553253713410422e-12, "SPICE": {"All": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 24.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.25, "re": 0.14285714285714285, "f": 0.18181818181818182, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2, "f": 0.17391304347826086, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a group of zebras standing in a pen. They are all facing the same direction and appear to be grazing on the grass. The pen is surrounded by a fence and there are trees in the background. The zebras are all black and white with distinctive stripes on their back"}, "400887": {"image_id": 400887, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.15534712747310678, "Bleu_3": 0.07844142656278798, "Bleu_4": 9.962310935752864e-06, "METEOR": 0.23307889302035611, "ROUGE_L": 0.20701357466063353, "CIDEr": 7.274622436286998e-10, "SPICE": {"All": {"pr": 0.3, "re": 0.3103448275862069, "f": 0.3050847457627119, "fn": 20.0, "numImages": 1.0, "fp": 21.0, "tp": 9.0}, "Relation": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.1, "f": 0.11764705882352941, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.46153846153846156, "re": 0.6666666666666666, "f": 0.5454545454545455, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a group of people walking down a street with horses. The people are wearing hats and the horses are wearing saddles. There are cars parked on the side of the road.\n\nThe image is taken on a sunny day with a blue sky and green trees in the background"}, "26185": {"image_id": 26185, "Bleu_1": 0.23999999999520005, "Bleu_2": 0.1714285714251079, "Bleu_3": 8.491317075202567e-07, "Bleu_4": 1.8997953886425763e-09, "METEOR": 0.2569286244187607, "ROUGE_L": 0.25386444708680145, "CIDEr": 6.467448244970898e-10, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5714285714285714, "f": 0.4444444444444444, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a person standing on a cliff overlooking the ocean. The person is holding a surfboard and wearing a wetsuit. The sky is cloudy and there is a strong wind blowing. The waves are crashing against the rocks below. The person is standing on the edge of a"}, "385770": {"image_id": 385770, "Bleu_1": 0.23214285713871177, "Bleu_2": 0.18375590810914594, "Bleu_3": 0.1233303713651029, "Bleu_4": 0.07713181696889591, "METEOR": 0.2289656231181395, "ROUGE_L": 0.2426136363636364, "CIDEr": 1.0439666981717874e-11, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.23809523809523808, "f": 0.21276595744680848, "fn": 16.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.2, "re": 0.2222222222222222, "f": 0.2105263157894737, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.375, "f": 0.3157894736842105, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a living room with a couch, coffee table, and television. There is a window on the left side of the room and a door on the right side. The floor is made of hardwood and there are two lamps on the coffee table. The walls are painted white and there are two paint"}, "446260": {"image_id": 446260, "Bleu_1": 0.34210526314889206, "Bleu_2": 0.27197188373746883, "Bleu_3": 0.18335338163444456, "Bleu_4": 0.11519925806442244, "METEOR": 0.2352955449772646, "ROUGE_L": 0.36961696602467, "CIDEr": 5.734103406430854e-05, "SPICE": {"All": {"pr": 0.24, "re": 0.21428571428571427, "f": 0.2264150943396226, "fn": 22.0, "numImages": 1.0, "fp": 19.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.15384615384615385, "f": 0.1904761904761905, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.5, "f": 0.47058823529411764, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a man wearing a white shirt with black and yellow stripes, a black tie, and a pair of sunglasses. He is standing in front of a white wall with a blue sky in the background."}, "353409": {"image_id": 353409, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.21004201260012265, "Bleu_3": 0.1383315354235378, "Bleu_4": 0.08573178735427808, "METEOR": 0.22620425615548126, "ROUGE_L": 0.25722891566265055, "CIDEr": 1.5755695997469447e-11, "SPICE": {"All": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 20.0, "numImages": 1.0, "fp": 15.0, "tp": 5.0}, "Relation": {"pr": 0.125, "re": 0.1, "f": 0.11111111111111112, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.14285714285714285, "f": 0.18181818181818182, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.375, "re": 0.375, "f": 0.375, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a group of people riding motorcycles on a road. They are all wearing helmets and some are carrying bags or other items. There are cars parked along the side of the road and people walking on the sidewalk. The sky is clear and there are trees in the background"}, "536653": {"image_id": 536653, "Bleu_1": 0.30188679244713423, "Bleu_2": 0.17037481092075088, "Bleu_3": 0.08287309006228931, "Bleu_4": 1.0329222874229249e-05, "METEOR": 0.2650544595517552, "ROUGE_L": 0.27774615822424586, "CIDEr": 2.3409316814162256e-11, "SPICE": {"All": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 24.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.1, "f": 0.15384615384615383, "fn": 9.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.25, "f": 0.1904761904761905, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The woman in the image is wearing a white tennis dress and holding a tennis racket. She is standing on the court and looking at the camera. The crowd in the background is cheering.\n\nThe woman in the image is wearing a white tennis dress and holding a tennis racket. She is standing"}, "450217": {"image_id": 450217, "Bleu_1": 0.15999999999680004, "Bleu_2": 0.09897433185907904, "Bleu_3": 5.887550428177756e-07, "Bleu_4": 1.4435323317269659e-09, "METEOR": 0.12764381035387024, "ROUGE_L": 0.15501905972045746, "CIDEr": 8.213581778484738e-12, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.20833333333333334, "f": 0.21739130434782608, "fn": 19.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2857142857142857, "f": 0.30769230769230765, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3, "f": 0.3157894736842105, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a mother elephant and her baby swimming in a body of water. The mother elephant is on the left side of the image, while the baby elephant is on the right side. The mother elephant is swimming with her head above the water, while the baby ele"}, "548219": {"image_id": 548219, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.22562131409418867, "Bleu_3": 0.17202309201023538, "Bleu_4": 0.12006005586220404, "METEOR": 0.33930039020940755, "ROUGE_L": 0.3564273789649416, "CIDEr": 3.4807705222385367e-10, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.18518518518518517, "f": 0.19607843137254902, "fn": 22.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1, "re": 0.1, "f": 0.10000000000000002, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.4444444444444444, "re": 0.5, "f": 0.47058823529411764, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a black and white cat peeking out of a red suitcase. The cat is sitting on top of the suitcase and looking out of it. The suitcase is open and the cat is inside.\n\nThe image is taken from a low angle, looking up at the cat. The light"}, "266117": {"image_id": 266117, "Bleu_1": 0.2181818181778513, "Bleu_2": 0.17978662998689882, "Bleu_3": 0.1450116762655686, "Bleu_4": 0.12375603639842561, "METEOR": 0.29689502558733394, "ROUGE_L": 0.28126801152737757, "CIDEr": 9.271385534009492e-13, "SPICE": {"All": {"pr": 0.23809523809523808, "re": 0.22727272727272727, "f": 0.23255813953488372, "fn": 17.0, "numImages": 1.0, "fp": 16.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.25, "f": 0.18181818181818182, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.4444444444444444, "f": 0.4210526315789474, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a living room with a black leather couch, a coffee table, and a television. There is a window on the left side of the room with a view of the outdoors. The room is well lit and has a wooden floor. The walls are painted a light color and there are no"}, "67881": {"image_id": 67881, "Bleu_1": 0.29999999999250004, "Bleu_2": 0.12403473458606788, "Bleu_3": 7.397773249928141e-07, "Bleu_4": 1.8187597339048352e-09, "METEOR": 0.1367749918101757, "ROUGE_L": 0.24104320337197047, "CIDEr": 2.6516272102597407e-05, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.2, "f": 0.16, "fn": 16.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.2857142857142857, "f": 0.2857142857142857, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.4, "re": 0.5, "f": 0.4444444444444445, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Object": {"pr": 0.2, "re": 0.2857142857142857, "f": 0.23529411764705882, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a group of people playing baseball on a field. The players are wearing baseball uniforms and helmets, and one player is holding a bat. The field is made of dirt and there are trees in the background."}, "253177": {"image_id": 253177, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.16817499303341832, "Bleu_3": 0.10219406022809462, "Bleu_4": 1.1969290421672298e-05, "METEOR": 0.15805022361890947, "ROUGE_L": 0.20254565578306588, "CIDEr": 8.028155850599543e-12, "SPICE": {"All": {"pr": 0.16, "re": 0.13793103448275862, "f": 0.14814814814814817, "fn": 25.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.07692307692307693, "f": 0.09090909090909093, "fn": 12.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2727272727272727, "f": 0.2608695652173913, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image is a painting of a horse on a wall. The horse is painted in a realistic style with a brown coat and white mane. The painting is framed in a wooden frame with a white matte. The wall is painted a bright pink color with a white trim. There are two windows on"}, "196462": {"image_id": 196462, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.17131872291347974, "Bleu_3": 0.0826422446045352, "Bleu_4": 1.0256732621419825e-05, "METEOR": 0.1741510308737654, "ROUGE_L": 0.2738496071829405, "CIDEr": 7.391335799316263e-12, "SPICE": {"All": {"pr": 0.24, "re": 0.2608695652173913, "f": 0.24999999999999994, "fn": 17.0, "numImages": 1.0, "fp": 19.0, "tp": 6.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.5, "f": 0.22222222222222224, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a plate of food on a table with a glass of orange juice and a salad on the side. There are two glasses of orange juice on the table, one with a straw and one without. The plate of food has a piece of toast with a fried egg on top"}, "373424": {"image_id": 373424, "Bleu_1": 0.2909090909038017, "Bleu_2": 0.25425669046082566, "Bleu_3": 0.18270326340753062, "Bleu_4": 0.12375603639842561, "METEOR": 0.2748646267551412, "ROUGE_L": 0.3685800604229607, "CIDEr": 4.140743198262479e-09, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.18518518518518517, "f": 0.16129032258064516, "fn": 22.0, "numImages": 1.0, "fp": 30.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1, "re": 0.2, "f": 0.13333333333333333, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5, "f": 0.36363636363636365, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a woman sitting on a couch in front of a window with a view of the snow outside. The woman is wearing a black sweater and black pants, and has a cat on her lap. The cat is also black and is looking out the window. The room is decorated with a"}, "183217": {"image_id": 183217, "Bleu_1": 0.23913043477741028, "Bleu_2": 0.1457945082861785, "Bleu_3": 0.09885983548206408, "Bleu_4": 0.06884902567176299, "METEOR": 0.18682896779187994, "ROUGE_L": 0.20819112627986344, "CIDEr": 1.7581198227891585e-08, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.24, "f": 0.23076923076923075, "fn": 19.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.07692307692307693, "f": 0.08333333333333334, "fn": 12.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.45454545454545453, "f": 0.4, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows a box filled with various types of donuts, including chocolate frosted, glazed, and sprinkled with hearts. The box is sitting on a table with a white tablecloth and a red and white checkered tablecloth. There are two chairs in front of the table"}, "464622": {"image_id": 464622, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.19603921176003908, "Bleu_3": 0.0922210236156557, "Bleu_4": 1.1306082351374465e-05, "METEOR": 0.25745341556809315, "ROUGE_L": 0.22889305816135083, "CIDEr": 9.583371097461573e-12, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.12121212121212122, "f": 0.13559322033898305, "fn": 29.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.06666666666666667, "f": 0.09090909090909091, "fn": 14.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.2727272727272727, "f": 0.2727272727272727, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a table with several mason jars filled with oranges on it. The jars are labeled with the date and time. There are also several other objects on the table, including a knife and a cutting board. The overall appearance of the image is clean and organized.\n\nThe image"}, "175737": {"image_id": 175737, "Bleu_1": 0.3399999999932, "Bleu_2": 0.18626292585916956, "Bleu_3": 0.08974367873592762, "Bleu_4": 1.1135979851734813e-05, "METEOR": 0.2103869984444653, "ROUGE_L": 0.1937738246505718, "CIDEr": 9.154711044501588e-10, "SPICE": {"All": {"pr": 0.23809523809523808, "re": 0.35714285714285715, "f": 0.2857142857142857, "fn": 9.0, "numImages": 1.0, "fp": 16.0, "tp": 5.0}, "Relation": {"pr": 0.2222222222222222, "re": 0.2857142857142857, "f": 0.25, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 1.0, "f": 0.4615384615384615, "fn": 0.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows two motorcycles parked on the side of a dirt road in the middle of a desert. The motorcycles are black and white, and they have red and blue stripes on them. The road is sandy and rocky, and there are some rocks and bushes on either side"}, "372756": {"image_id": 372756, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.11155016321612385, "Bleu_3": 6.290089214849726e-07, "Bleu_4": 1.5012173807660588e-09, "METEOR": 0.2054925228488086, "ROUGE_L": 0.24653579676674361, "CIDEr": 3.9320304301283103e-11, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.16666666666666666, "f": 0.15686274509803924, "fn": 20.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4444444444444444, "f": 0.39999999999999997, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a small bird perched on a mirror. The bird has bright yellow feathers and a long, curly tail. It is looking directly at the camera with its beak open. The mirror is a round, silver frame with a white background. The bird's reflection is visible in the mirror."}, "308316": {"image_id": 308316, "Bleu_1": 0.2765957446749661, "Bleu_2": 0.18994132206127556, "Bleu_3": 0.09289852098623474, "Bleu_4": 1.1618322588062582e-05, "METEOR": 0.24268702806555564, "ROUGE_L": 0.2501708817498291, "CIDEr": 1.20515877035058e-09, "SPICE": {"All": {"pr": 0.06666666666666667, "re": 0.07142857142857142, "f": 0.0689655172413793, "fn": 26.0, "numImages": 1.0, "fp": 28.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.18181818181818182, "f": 0.1904761904761905, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image is of a military aircraft flying in the sky. The plane is painted in a camouflage pattern and has a large propeller on the front. The plane is flying over a field with trees and buildings in the background. The sky is clear and blue."}, "168416": {"image_id": 168416, "Bleu_1": 0.2941176470530566, "Bleu_2": 0.17149858513911254, "Bleu_3": 0.08435451529716312, "Bleu_4": 1.057477026987749e-05, "METEOR": 0.2575061336323944, "ROUGE_L": 0.2893689114781872, "CIDEr": 3.2376030384210065e-09, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.3333333333333333, "f": 0.2608695652173913, "fn": 12.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.18181818181818182, "re": 0.2, "f": 0.1904761904761905, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5714285714285714, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a bird feeder hanging from a tree branch in a forest. The feeder is made of metal and has a small bird perched on top of it. The bird is a cardinal, which is known for its bright red plumage. The feeder is surrounded by trees and foliage"}, "348496": {"image_id": 348496, "Bleu_1": 0.2142857142818878, "Bleu_2": 0.1651445647659783, "Bleu_3": 0.11485555942835955, "Bleu_4": 0.08695659942157265, "METEOR": 0.1847690990904969, "ROUGE_L": 0.20378619153674835, "CIDEr": 3.603907591865525e-12, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.23809523809523808, "f": 0.19999999999999998, "fn": 16.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a river with boats and people on the water. There are buildings on the shore and a bridge in the background. The sky is blue and there are trees on the shore.\n\nThe image is taken from a high vantage point, looking down on the river and the people on it. The boats"}, "333677": {"image_id": 333677, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.13719886811129003, "Bleu_3": 0.09158935294477763, "Bleu_4": 1.1247951467485058e-05, "METEOR": 0.17996973137418856, "ROUGE_L": 0.25553560742070613, "CIDEr": 6.889827933043101e-10, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.16, "f": 0.15094339622641512, "fn": 21.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4444444444444444, "f": 0.380952380952381, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a small bedroom with a bed, a dresser, and a window. The walls are painted white and there is a wooden floor. The room is well lit and has a comfortable atmosphere.\n\nThe bed is made up with a blue and white striped blanket and a pillow. The"}, "224364": {"image_id": 224364, "Bleu_1": 0.29268292682212976, "Bleu_2": 0.17107978454943554, "Bleu_3": 0.11449528240744467, "Bleu_4": 0.07927655972962615, "METEOR": 0.19987324242659504, "ROUGE_L": 0.2713120830244626, "CIDEr": 1.7283479989059651e-06, "SPICE": {"All": {"pr": 0.10344827586206896, "re": 0.13043478260869565, "f": 0.11538461538461538, "fn": 20.0, "numImages": 1.0, "fp": 26.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.3, "f": 0.25, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a group of giraffes standing in a grassy area. They are all facing the same direction and appear to be looking at something in the distance. The trees in the background are bare and the sky is cloudy."}, "551820": {"image_id": 551820, "Bleu_1": 0.28571428570918367, "Bleu_2": 0.2038588765713768, "Bleu_3": 0.1454705770342468, "Bleu_4": 0.08729963014185808, "METEOR": 0.2903471522255567, "ROUGE_L": 0.2830626450116009, "CIDEr": 5.313338849076975e-14, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.22727272727272727, "f": 0.20833333333333331, "fn": 17.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.125, "re": 0.125, "f": 0.125, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.25, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.3333333333333333, "f": 0.2222222222222222, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a group of people playing tennis on a court. They are wearing tennis shoes and holding rackets. The court is made of concrete and has a net in the middle. There are spectators watching the game from the stands. The image is in color and has a clear view of the players and"}, "152962": {"image_id": 152962, "Bleu_1": 0.3399999999932, "Bleu_2": 0.2634155559173319, "Bleu_3": 0.16307508670756585, "Bleu_4": 0.09800906568206562, "METEOR": 0.24249019789588294, "ROUGE_L": 0.30310559006211185, "CIDEr": 2.914944821064507e-10, "SPICE": {"All": {"pr": 0.2, "re": 0.17857142857142858, "f": 0.1886792452830189, "fn": 23.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.08333333333333333, "f": 0.11764705882352941, "fn": 11.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4444444444444444, "f": 0.3636363636363637, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a group of people standing on a tennis court, looking at a man who is holding a tennis racket. The man is wearing a white shirt and black pants, and the people are wearing various colors of shirts and pants. The image is in black and white."}, "32779": {"image_id": 32779, "Bleu_1": 0.2941176470530566, "Bleu_2": 0.2543735095295466, "Bleu_3": 0.13822689730834659, "Bleu_4": 1.5315603357857855e-05, "METEOR": 0.21700056374890583, "ROUGE_L": 0.30771078549123226, "CIDEr": 8.619024499358165e-09, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.2, "f": 0.1923076923076923, "fn": 20.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.5, "f": 0.41666666666666663, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows a bedroom with a large bed, a bathroom with a bathtub, and a living room with a couch and a coffee table. The walls are painted yellow and the floor is made of hardwood. There is a large window in the living room that lets in natural light."}, "97767": {"image_id": 97767, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.14002800840008178, "Bleu_3": 0.09222102361569189, "Bleu_4": 1.1247951467489566e-05, "METEOR": 0.21762076225596774, "ROUGE_L": 0.2544696066746126, "CIDEr": 2.253753008543987e-09, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a fire hydrant on the sidewalk in front of a building. The hydrant is yellow and has a hose attached to it. There are no people in the image. The building is made of brick and has a sign on the front that reads, \"Fire Hydrant\". The image is"}, "349485": {"image_id": 349485, "Bleu_1": 0.2666666666607408, "Bleu_2": 0.1740776559517855, "Bleu_3": 0.12834519665251867, "Bleu_4": 0.08423108446301197, "METEOR": 0.26307985600111483, "ROUGE_L": 0.30219391365888176, "CIDEr": 2.0121806192649777e-07, "SPICE": {"All": {"pr": 0.27586206896551724, "re": 0.32, "f": 0.29629629629629634, "fn": 17.0, "numImages": 1.0, "fp": 21.0, "tp": 8.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.07692307692307693, "f": 0.08333333333333334, "fn": 12.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.5, "f": 0.22222222222222224, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5454545454545454, "re": 0.6, "f": 0.5714285714285713, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 6.0}}, "caption": "The image shows a grocery store with various fruits and vegetables on display. There are several shelves with different types of produce, including apples, bananas, and oranges. The store also has a large refrigerator in the back with various types of meat and dairy products."}, "53635": {"image_id": 53635, "Bleu_1": 0.3829787233961069, "Bleu_2": 0.20402970888419056, "Bleu_3": 0.0974371940073651, "Bleu_4": 1.2041494936393692e-05, "METEOR": 0.2303430941615878, "ROUGE_L": 0.28259430840502975, "CIDEr": 1.0735448083157129e-08, "SPICE": {"All": {"pr": 0.1875, "re": 0.14285714285714285, "f": 0.16216216216216214, "fn": 18.0, "numImages": 1.0, "fp": 13.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.6, "re": 0.375, "f": 0.4615384615384615, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 3.0}}, "caption": "The image shows a young man riding a skateboard down a concrete skate park. He is wearing a green t-shirt and blue jeans, and has his hands on the handlebars of the skateboard. The skate park is surrounded by trees and there are other skaters in the"}, "87409": {"image_id": 87409, "Bleu_1": 0.2264150943353507, "Bleu_2": 0.174582229926461, "Bleu_3": 0.13370981749106947, "Bleu_4": 0.09888660568274774, "METEOR": 0.20898557450214789, "ROUGE_L": 0.25894481503941785, "CIDEr": 2.790919958812214e-11, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.2, "f": 0.23529411764705882, "fn": 24.0, "numImages": 1.0, "fp": 15.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.46153846153846156, "re": 0.5, "f": 0.48000000000000004, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows three people on a surfboard in the middle of a body of water. They are wearing black and white wetsuits and are standing on the board with their arms outstretched. The water is calm and there are no waves in sight. The sky is clear and there are no clouds"}, "572095": {"image_id": 572095, "Bleu_1": 0.2941176470530566, "Bleu_2": 0.1878672873218244, "Bleu_3": 0.08964004852977604, "Bleu_4": 1.106792615545272e-05, "METEOR": 0.20057086299476165, "ROUGE_L": 0.22889305816135083, "CIDEr": 1.2143407396313747e-10, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.23809523809523808, "f": 0.21276595744680848, "fn": 16.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.1, "f": 0.11111111111111112, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.16666666666666666, "re": 1.0, "f": 0.2857142857142857, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5714285714285714, "f": 0.4444444444444444, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a line of motorcycles parked on the side of a street. The bikes are all different colors and styles, with some having chrome and others having black and silver. There are people standing next to the bikes, looking at them. The street is lined with buildings and trees."}, "251801": {"image_id": 251801, "Bleu_1": 0.2826086956460303, "Bleu_2": 0.17720317768650332, "Bleu_3": 0.11259180377051929, "Bleu_4": 0.0759037038740657, "METEOR": 0.20959717936931294, "ROUGE_L": 0.28968792401628224, "CIDEr": 1.85498786244395e-08, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.12121212121212122, "f": 0.14285714285714288, "fn": 29.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.4, "f": 0.4000000000000001, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image is a table with a variety of desserts on it. There are cakes, cupcakes, and other sweet treats. The table is set up with a white tablecloth and there are several plates and cups on it. The desserts are arranged in a creative way"}, "432017": {"image_id": 432017, "Bleu_1": 0.26315789473222534, "Bleu_2": 0.0969458417894692, "Bleu_3": 5.549219378020359e-07, "Bleu_4": 1.3337532192271047e-09, "METEOR": 0.18897683772923418, "ROUGE_L": 0.23682750970604544, "CIDEr": 3.180980897052493e-13, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.09523809523809523, "f": 0.08888888888888889, "fn": 19.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The clock tower is made of metal and has a large clock face on it. The clock face has Roman numerals and hands. The clock tower is tall and has a large clock face on it. The clock face has Roman numerals and hands. The clock tower is tall and has a large clock face on it."}, "139605": {"image_id": 139605, "Bleu_1": 0.2941176470530566, "Bleu_2": 0.1878672873218244, "Bleu_3": 0.08964004852977604, "Bleu_4": 1.106792615545272e-05, "METEOR": 0.2682762324782883, "ROUGE_L": 0.22889305816135083, "CIDEr": 8.081947717502864e-11, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.125, "f": 0.1276595744680851, "fn": 21.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a small boat sailing in the ocean near a lighthouse. The lighthouse is a tall, white tower with a black and white striped roof. The boat is a small, wooden vessel with a white hull and a black and white striped sail. The water is calm and clear"}, "137395": {"image_id": 137395, "Bleu_1": 0.1666666666631945, "Bleu_2": 0.08421519210487878, "Bleu_3": 5.362175432481669e-07, "Bleu_4": 1.3605128106221755e-09, "METEOR": 0.13095535373785405, "ROUGE_L": 0.19551282051282048, "CIDEr": 7.07821117635917e-10, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.2, "f": 0.18604651162790697, "fn": 16.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a group of people playing baseball in a cage. The players are wearing baseball gear, including helmets, bats, and gloves. The cage is made of chain link fencing and has a gate in the front. The players are hitting the ball with their bats and"}, "309279": {"image_id": 309279, "Bleu_1": 0.31481481480898493, "Bleu_2": 0.21798903029455755, "Bleu_3": 0.13995735618512756, "Bleu_4": 0.08562572337386101, "METEOR": 0.29220271452826474, "ROUGE_L": 0.4041794087665647, "CIDEr": 1.6414273976294676e-09, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.20833333333333334, "f": 0.2, "fn": 19.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a slice of orange cake on a plate with a fork next to it. The cake has a round shape and is topped with a slice of orange. There are two forks on the plate, one with a bite taken out of it. The background is white and the lighting is"}, "33221": {"image_id": 33221, "Bleu_1": 0.3399999999932, "Bleu_2": 0.276272565791757, "Bleu_3": 0.1995881547631518, "Bleu_4": 0.1500917846172271, "METEOR": 0.2494768111325597, "ROUGE_L": 0.3317220543806646, "CIDEr": 1.5719972022266608e-05, "SPICE": {"All": {"pr": 0.15625, "re": 0.29411764705882354, "f": 0.20408163265306123, "fn": 12.0, "numImages": 1.0, "fp": 27.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.7142857142857143, "f": 0.4761904761904762, "fn": 2.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows a group of people standing on a beach, holding tennis rackets. They are wearing orange vests and hats, and one person is holding a tennis ball. The sky is cloudy and there are palm trees in the background.\n\nThe image is taken from a low angle, looking"}, "272925": {"image_id": 272925, "Bleu_1": 0.3061224489733445, "Bleu_2": 0.11293848786082757, "Bleu_3": 6.47433787030964e-07, "Bleu_4": 1.558500916173644e-09, "METEOR": 0.2019108635967674, "ROUGE_L": 0.2576946288473144, "CIDEr": 1.5935900442507983e-09, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.20689655172413793, "f": 0.21052631578947367, "fn": 23.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2222222222222222, "re": 0.2222222222222222, "f": 0.2222222222222222, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.36363636363636365, "re": 0.36363636363636365, "f": 0.36363636363636365, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a sidewalk covered in confetti. There are several people walking down the street, some of them holding umbrellas. The sky is overcast and there is a fire hydrant on the corner.\n\nThe image is taken from a low angle, looking down at the scene. The conf"}, "231471": {"image_id": 231471, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.11895773785545558, "Bleu_3": 6.522364605376366e-07, "Bleu_4": 1.5348336186916813e-09, "METEOR": 0.165062101934485, "ROUGE_L": 0.18100890207715134, "CIDEr": 7.91344954132257e-12, "SPICE": {"All": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a group of ducks swimming in a river next to a park bench. The park bench is made of wood and has a metal frame. The ducks are swimming in the water and seem to be enjoying themselves. The sky is clear and blue, with some clouds in the distance"}, "152795": {"image_id": 152795, "Bleu_1": 0.1702127659538253, "Bleu_2": 0.1216598129318781, "Bleu_3": 0.06902831277935673, "Bleu_4": 9.298379656008716e-06, "METEOR": 0.17705470582012495, "ROUGE_L": 0.20847573479152426, "CIDEr": 2.555449417698561e-09, "SPICE": {"All": {"pr": 0.16, "re": 0.14814814814814814, "f": 0.15384615384615383, "fn": 23.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4, "f": 0.34782608695652173, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a bunch of bananas hanging from a metal railing. The bananas are green and ripe, and they are hanging from the railing in a row. There are some other fruits hanging from the railing as well, including apples and pears. The image is taken"}, "489046": {"image_id": 489046, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.18504536999744126, "Bleu_3": 0.13810339807225144, "Bleu_4": 0.08477362013223914, "METEOR": 0.2825382557282901, "ROUGE_L": 0.2445589919816724, "CIDEr": 2.6334237915417625e-12, "SPICE": {"All": {"pr": 0.05263157894736842, "re": 0.041666666666666664, "f": 0.04651162790697675, "fn": 23.0, "numImages": 1.0, "fp": 18.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}}, "caption": "The image shows a bird perched on a branch of a tree growing in a body of water. The bird has a greenish plumage and a long, curved beak. The tree has green leaves and the water is clear and calm. The sky is blue and there are no other objects in the image"}, "215808": {"image_id": 215808, "Bleu_1": 0.27272727272107444, "Bleu_2": 0.22525531672830446, "Bleu_3": 0.1935310651946666, "Bleu_4": 0.1630729955075475, "METEOR": 0.35244803982813105, "ROUGE_L": 0.37602739726027395, "CIDEr": 7.04403120221399e-08, "SPICE": {"All": {"pr": 0.12, "re": 0.1111111111111111, "f": 0.11538461538461538, "fn": 24.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a slice of carrot cake on a plate with a fork on the side. The cake has a creamy frosting on top and is decorated with chopped nuts. There is a glass of milk on the table next to the plate."}, "197716": {"image_id": 197716, "Bleu_1": 0.27586206896076104, "Bleu_2": 0.18405922400671818, "Bleu_3": 0.13425456010359604, "Bleu_4": 0.10718559564745299, "METEOR": 0.2513173020122495, "ROUGE_L": 0.2973997833152763, "CIDEr": 1.532678032629822e-14, "SPICE": {"All": {"pr": 0.2631578947368421, "re": 0.25, "f": 0.25641025641025644, "fn": 15.0, "numImages": 1.0, "fp": 14.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.42857142857142855, "f": 0.4615384615384615, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.75, "f": 0.8571428571428571, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 3.0}, "Object": {"pr": 0.2857142857142857, "re": 0.3333333333333333, "f": 0.30769230769230765, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The image shows a blue and white bus driving down a street with houses on either side. The bus has a large advertisement on the side for a local restaurant. The sky is clear and blue with some clouds in the distance. The grass is green and there are some trees on the side of the road. The"}, "330204": {"image_id": 330204, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.18070977621815823, "Bleu_3": 0.14738536958247936, "Bleu_4": 0.12650662740672958, "METEOR": 0.2883988758075374, "ROUGE_L": 0.3190005810575247, "CIDEr": 5.726769970715824e-10, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.24, "f": 0.2264150943396226, "fn": 19.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.1, "re": 0.14285714285714285, "f": 0.11764705882352941, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3125, "re": 0.45454545454545453, "f": 0.3703703703703703, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 5.0}}, "caption": "The image shows a green and white school bus parked in front of a building. The bus has a large sign on the side that reads, \"cleveland public transportation\". There are people standing in front of the bus, looking at it. The building behind the bus appears to be a large, modern structure"}, "150703": {"image_id": 150703, "Bleu_1": 0.27999999999440006, "Bleu_2": 0.16903085094228823, "Bleu_3": 0.10598398329266914, "Bleu_4": 0.07094237342460057, "METEOR": 0.19921239576062594, "ROUGE_L": 0.2852466682253917, "CIDEr": 5.370071016112243e-09, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.23529411764705882, "f": 0.1818181818181818, "fn": 13.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a hot dog and a drink on a table. The hot dog is wrapped in a bun and has ketchup and mustard on it. The drink is a beer.\n\nThe table is made of wood and has a white tablecloth on it. There is a napkin on"}, "497928": {"image_id": 497928, "Bleu_1": 0.37999999999240003, "Bleu_2": 0.19691498216873898, "Bleu_3": 0.09313339354637472, "Bleu_4": 1.1449976791751503e-05, "METEOR": 0.1899054056763861, "ROUGE_L": 0.23664375969851476, "CIDEr": 4.332934139536633e-08, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.19047619047619047, "f": 0.17777777777777778, "fn": 17.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.2, "f": 0.14285714285714285, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a living room with a white couch, a white coffee table, and a white television on the wall. There is a window on the left side of the room with curtains open to let in natural light. The room is well lit and has a comfortable atmosphere."}, "91334": {"image_id": 91334, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.18216065373578985, "Bleu_3": 0.15850031615011761, "Bleu_4": 0.14197637411492256, "METEOR": 0.3146564279850917, "ROUGE_L": 0.30049261083743845, "CIDEr": 1.0008059313772487e-11, "SPICE": {"All": {"pr": 0.12, "re": 0.15789473684210525, "f": 0.13636363636363635, "fn": 16.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.3333333333333333, "f": 0.2608695652173913, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a cat drinking from a bowl of water. The cat is sitting on the edge of the bowl and has its head tilted back to drink from the water. The bowl is made of white ceramic and has a small handle on the side. The cat's fur is"}, "191053": {"image_id": 191053, "Bleu_1": 0.33333333332679743, "Bleu_2": 0.18257418583143972, "Bleu_3": 0.11080794149542911, "Bleu_4": 1.2975313384213989e-05, "METEOR": 0.26742673001047645, "ROUGE_L": 0.26704190118824267, "CIDEr": 3.703501802346142e-10, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.15, "f": 0.12765957446808512, "fn": 17.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.09090909090909091, "re": 0.16666666666666666, "f": 0.11764705882352942, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.14285714285714285, "re": 1.0, "f": 0.25, "fn": 0.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Object": {"pr": 0.2, "re": 0.2857142857142857, "f": 0.23529411764705882, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a young boy sitting on the floor in a living room. He is wearing a white shirt and tie, and has a smile on his face. The room is decorated with a fireplace, a couch, and a coffee table. There is a window in the background with curtains"}, "300323": {"image_id": 300323, "Bleu_1": 0.33333333332716053, "Bleu_2": 0.2507849312830716, "Bleu_3": 0.18218943070472146, "Bleu_4": 0.12409587310674511, "METEOR": 0.24049272315337286, "ROUGE_L": 0.2694941462337088, "CIDEr": 4.107133207554834e-09, "SPICE": {"All": {"pr": 0.15625, "re": 0.20833333333333334, "f": 0.17857142857142858, "fn": 19.0, "numImages": 1.0, "fp": 27.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.09090909090909091, "re": 0.25, "f": 0.13333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.3333333333333333, "f": 0.32, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The woman is holding a remote control in her hand and is standing in front of a television. She is wearing a white shirt and black pants. The room is dimly lit and there are several pieces of furniture in the background, including a couch and a coffee table. The woman is looking at"}, "66423": {"image_id": 66423, "Bleu_1": 0.19999999999600004, "Bleu_2": 0.16903085094228823, "Bleu_3": 0.1335314515021404, "Bleu_4": 0.1110308147206433, "METEOR": 0.2746557472656916, "ROUGE_L": 0.31955762514551805, "CIDEr": 1.8542502590565023e-09, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.15, "f": 0.13043478260869565, "fn": 17.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a group of people standing on surfboards in the ocean, with a volcano in the background\n\nThe people are wearing wetsuits and life jackets, and they are holding their boards with their hands. The water is calm and clear, with a few waves breaking on the shore"}, "223122": {"image_id": 223122, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.1234482786556696, "Bleu_3": 0.06685490874553476, "Bleu_4": 8.792400740713413e-06, "METEOR": 0.25522994114146974, "ROUGE_L": 0.26472411655300687, "CIDEr": 1.0864183838616649e-12, "SPICE": {"All": {"pr": 0.24, "re": 0.17142857142857143, "f": 0.19999999999999998, "fn": 29.0, "numImages": 1.0, "fp": 19.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.1111111111111111, "f": 0.14285714285714285, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.38461538461538464, "f": 0.41666666666666663, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a small bathroom with a toilet, sink, and shower. The walls are painted white and there is a small window on the side of the room. The floor is made of tile and there is a small rug in the corner. The toilet is a standard size and has a"}, "390718": {"image_id": 390718, "Bleu_1": 0.34693877550312374, "Bleu_2": 0.20824828195446654, "Bleu_3": 0.1404091521942512, "Bleu_4": 0.08807590307437163, "METEOR": 0.26283571693885466, "ROUGE_L": 0.2982103884766477, "CIDEr": 9.37233676926004e-09, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.18181818181818182, "f": 0.17391304347826086, "fn": 18.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.2, "f": 0.14285714285714285, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a group of men playing baseball in a park. They are wearing baseball uniforms and gloves, and one of them is holding a bat. The field is grassy and there are trees in the background. The sky is blue and there are clouds in the sky."}, "46508": {"image_id": 46508, "Bleu_1": 0.23913043477741028, "Bleu_2": 0.1457945082861785, "Bleu_3": 7.846510342081586e-07, "Bleu_4": 1.8307974536939859e-09, "METEOR": 0.21793558433518068, "ROUGE_L": 0.16968011126564672, "CIDEr": 2.0378485810843922e-09, "SPICE": {"All": {"pr": 0.125, "re": 0.15789473684210525, "f": 0.13953488372093023, "fn": 16.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows an elephant riding on the back of a man on a bicycle. The elephant is wearing a saddle and the man is wearing a helmet. The image is taken in a rural area with trees and grass in the background.\n\nThe elephant is"}, "526576": {"image_id": 526576, "Bleu_1": 0.3584905660309719, "Bleu_2": 0.2753807233863433, "Bleu_3": 0.21833916525099153, "Bleu_4": 0.18799402472613977, "METEOR": 0.3619226158155062, "ROUGE_L": 0.33292904790782296, "CIDEr": 1.7322447727564707e-12, "SPICE": {"All": {"pr": 0.08, "re": 0.07407407407407407, "f": 0.07692307692307691, "fn": 25.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2, "f": 0.1904761904761905, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a man standing in front of a white wall, wearing a red shirt and black pants. He is holding a guitar in his left hand and has his right hand on his chin. The man is standing in a crouched position, with his legs bent and his back straight. The"}, "581717": {"image_id": 581717, "Bleu_1": 0.255813953482423, "Bleu_2": 0.17451086522195614, "Bleu_3": 0.1141029906527568, "Bleu_4": 0.07806525322093243, "METEOR": 0.26941188751761685, "ROUGE_L": 0.2616154395997141, "CIDEr": 1.2429031737362093e-07, "SPICE": {"All": {"pr": 0.1, "re": 0.11764705882352941, "f": 0.1081081081081081, "fn": 15.0, "numImages": 1.0, "fp": 18.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2222222222222222, "f": 0.19999999999999998, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a cat peeking out of a suitcase. The cat has a collar on its neck and is looking up at the camera. The suitcase is open and has a pile of clothes inside. The background is a dark brown color."}, "307683": {"image_id": 307683, "Bleu_1": 0.29999999999400007, "Bleu_2": 0.20701966779852365, "Bleu_3": 0.12132137515765275, "Bleu_4": 1.3961385815466905e-05, "METEOR": 0.2233676265554817, "ROUGE_L": 0.21759809750297268, "CIDEr": 5.38144716963401e-09, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.2727272727272727, "f": 0.23529411764705882, "fn": 16.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.1, "re": 0.14285714285714285, "f": 0.11764705882352941, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.2, "f": 0.23529411764705882, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.6, "f": 0.35294117647058826, "fn": 2.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a person sitting on a blanket with a plate of food in front of them. The plate has a sandwich, an apple, and a glass of milk. The person is wearing a blue shirt and jeans. The background is a green field with trees in the distance."}, "580757": {"image_id": 580757, "Bleu_1": 0.31372549018992696, "Bleu_2": 0.22404481344004462, "Bleu_3": 0.1831787058895552, "Bleu_4": 0.15043916989451903, "METEOR": 0.26193473925354704, "ROUGE_L": 0.3285457809694794, "CIDEr": 2.286061048880361e-10, "SPICE": {"All": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 18.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.5555555555555556, "re": 0.5, "f": 0.5263157894736842, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 5.0}}, "caption": "The image shows a red, white, and blue fire hydrant with a flag on top. The hydrant is painted with a patriotic design, including a red, white, and blue striped pattern on the body and a blue star on top. The flag on top is a red, white, and blue strip"}, "230995": {"image_id": 230995, "Bleu_1": 0.2916666666605903, "Bleu_2": 0.2084219669570065, "Bleu_3": 0.1557386407653264, "Bleu_4": 0.1138286684235099, "METEOR": 0.2576273668036634, "ROUGE_L": 0.3057644110275689, "CIDEr": 2.8623144701715896e-09, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.17647058823529413, "f": 0.15789473684210528, "fn": 14.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3, "f": 0.3, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a woman sitting on the sidewalk with an umbrella over her head. She is wearing a purple and white striped shirt and black pants. The umbrella has a purple and white striped pattern. The woman is looking down at her phone. There is a white"}, "242779": {"image_id": 242779, "Bleu_1": 0.21999999999560005, "Bleu_2": 0.14982983544985165, "Bleu_3": 0.07762205255475667, "Bleu_4": 9.987674437131075e-06, "METEOR": 0.20109072105843567, "ROUGE_L": 0.21759809750297268, "CIDEr": 3.7022380410973485e-08, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.20833333333333334, "f": 0.1851851851851852, "fn": 19.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.14285714285714285, "f": 0.10526315789473685, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.4444444444444444, "f": 0.34782608695652173, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a baseball game in progress. The players are wearing their team's uniforms and are standing on the field. The crowd is watching from the stands. The umpire is standing behind home plate, ready to call the next pitch. The scoreboard shows the score of the game."}, "70815": {"image_id": 70815, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.17842851423634928, "Bleu_3": 0.08720922498013364, "Bleu_4": 1.0899268608532945e-05, "METEOR": 0.23186717107477783, "ROUGE_L": 0.22333414693678305, "CIDEr": 3.0981250832710533e-10, "SPICE": {"All": {"pr": 0.10526315789473684, "re": 0.14285714285714285, "f": 0.12121212121212122, "fn": 12.0, "numImages": 1.0, "fp": 17.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows a dog sitting on the sidewalk next to a bicycle. The dog is wearing a collar and leash. There are people walking on the sidewalk in the background. The image is in black and white.\n\nThe image shows a dog sitting on the sidewalk next to a"}, "477688": {"image_id": 477688, "Bleu_1": 0.2653061224435652, "Bleu_2": 0.16624095289858323, "Bleu_3": 0.12082769166967569, "Bleu_4": 0.07869287296538219, "METEOR": 0.23748723961986373, "ROUGE_L": 0.2945081472540736, "CIDEr": 1.650831549150367e-09, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.2608695652173913, "f": 0.23076923076923075, "fn": 17.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.1, "re": 0.125, "f": 0.11111111111111112, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5, "f": 0.380952380952381, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a group of police officers standing in front of a line of motorcycles. They are all wearing helmets and have their arms crossed over their chests. The motorcycles are all black and white, with the words \"Police\" written on the side. There are people standing on"}, "411841": {"image_id": 411841, "Bleu_1": 0.3773584905589178, "Bleu_2": 0.2555622163811263, "Bleu_3": 0.1973292455171, "Bleu_4": 0.14653163791238963, "METEOR": 0.28374310351353627, "ROUGE_L": 0.2959369314736203, "CIDEr": 3.543280287835446e-12, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.15384615384615385, "f": 0.14285714285714288, "fn": 22.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4444444444444444, "f": 0.3636363636363637, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a large airport terminal building with a red and white plane parked in front of it. The terminal building has a large sign that reads 'makati airport' in white letters on a red background. The plane has a red and white tail and wings. The airport is surrounded by a"}, "153150": {"image_id": 153150, "Bleu_1": 0.23214285713871177, "Bleu_2": 0.14527180078437643, "Bleu_3": 0.07311216202480127, "Bleu_4": 9.26666051029079e-06, "METEOR": 0.23144155898793697, "ROUGE_L": 0.2717149220489977, "CIDEr": 4.848746134218871e-13, "SPICE": {"All": {"pr": 0.3125, "re": 0.2, "f": 0.24390243902439027, "fn": 20.0, "numImages": 1.0, "fp": 11.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.625, "re": 0.4166666666666667, "f": 0.5, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 5.0}}, "caption": "The image shows a dog lying on a bed with its tongue hanging out of its mouth. The dog is wearing a blue collar with a white bow tie. The bed is covered in a white sheet and there is a white blanket on top of it. The room is dimly lit and there is a"}, "284860": {"image_id": 284860, "Bleu_1": 0.24137931034066593, "Bleu_2": 0.17217163877715416, "Bleu_3": 0.08089314736598265, "Bleu_4": 9.904741088158614e-06, "METEOR": 0.16689570401743603, "ROUGE_L": 0.23591160220994478, "CIDEr": 4.72646072803923e-14, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.2857142857142857, "f": 0.2105263157894737, "fn": 15.0, "numImages": 1.0, "fp": 30.0, "tp": 6.0}, "Relation": {"pr": 0.058823529411764705, "re": 0.1111111111111111, "f": 0.07692307692307691, "fn": 8.0, "numImages": 1.0, "fp": 16.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3125, "re": 0.45454545454545453, "f": 0.3703703703703703, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 5.0}}, "caption": "The image shows a street with a red light at the corner. There is a brick building on the left side of the street and a white building on the right side. The street is empty and there are no cars or people in the image.\n\nThe image is taken from a low angle, looking down the street"}, "145061": {"image_id": 145061, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.22562131409418867, "Bleu_3": 0.14508989498764088, "Bleu_4": 0.08885438285000041, "METEOR": 0.22558281589131782, "ROUGE_L": 0.24151583710407235, "CIDEr": 1.6547433674348267e-10, "SPICE": {"All": {"pr": 0.10344827586206896, "re": 0.13636363636363635, "f": 0.11764705882352941, "fn": 19.0, "numImages": 1.0, "fp": 26.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3333333333333333, "f": 0.27272727272727276, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a city street with a bus stop on the corner. There are several cars parked on the side of the road, and a few pedestrians walking in the area. The sky is cloudy and there are some trees in the background.\n\nThe image is taken from a bird's"}, "535591": {"image_id": 535591, "Bleu_1": 0.24999999999553577, "Bleu_2": 0.19069251784568234, "Bleu_3": 0.13913760149235183, "Bleu_4": 0.08443342702964665, "METEOR": 0.23599307627979527, "ROUGE_L": 0.2663755458515284, "CIDEr": 3.4826925494512883e-12, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.2631578947368421, "f": 0.20833333333333334, "fn": 14.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.07142857142857142, "re": 0.125, "f": 0.09090909090909091, "fn": 7.0, "numImages": 1.0, "fp": 13.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5, "f": 0.36363636363636365, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a train station with a train on the tracks. There are mountains in the background. The sky is clear and blue.\n\nThe train station has a small platform with a few benches and a sign that says \"train station\". There is a small building with a door and windows on the left side"}, "443537": {"image_id": 443537, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.1922322627296484, "Bleu_3": 0.130397100762736, "Bleu_4": 0.08201663209189095, "METEOR": 0.2655816584994607, "ROUGE_L": 0.319823568575233, "CIDEr": 4.03097217351577e-10, "SPICE": {"All": {"pr": 0.06666666666666667, "re": 0.1111111111111111, "f": 0.08333333333333334, "fn": 16.0, "numImages": 1.0, "fp": 28.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.14285714285714285, "re": 0.25, "f": 0.18181818181818182, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 2.0}}, "caption": "The image shows a black and white cat and a brown dog lying on a bed. The cat is looking up at the dog, while the dog is looking down at the cat. The bed is covered in a patterned blanket. The room is dimly lit by a lamp on the nightstand."}, "306426": {"image_id": 306426, "Bleu_1": 0.17647058823183395, "Bleu_2": 0.10289915108346752, "Bleu_3": 6.000800213279683e-07, "Bleu_4": 1.456620999046851e-09, "METEOR": 0.17725543372801036, "ROUGE_L": 0.22889305816135083, "CIDEr": 5.0214424020145585e-11, "SPICE": {"All": {"pr": 0.34615384615384615, "re": 0.2727272727272727, "f": 0.3050847457627119, "fn": 24.0, "numImages": 1.0, "fp": 17.0, "tp": 9.0}, "Relation": {"pr": 0.3333333333333333, "re": 0.13333333333333333, "f": 0.19047619047619044, "fn": 13.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.6363636363636364, "re": 0.4666666666666667, "f": 0.5384615384615385, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 7.0}}, "caption": "The image shows a woman wearing a blue shirt and white pants kneeling on a table, cutting a long, thin piece of dough with a knife. The woman is surrounded by other women in white aprons and hats, all of whom are working on various tasks in the kitchen. The room"}, "346972": {"image_id": 346972, "Bleu_1": 0.2765957446749661, "Bleu_2": 0.17339191115885713, "Bleu_3": 0.08742085527780723, "Bleu_4": 1.1100642574269578e-05, "METEOR": 0.23125417516668859, "ROUGE_L": 0.30367143746110764, "CIDEr": 1.1921693513881733e-08, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.21739130434782608, "f": 0.2127659574468085, "fn": 18.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5714285714285714, "f": 0.4210526315789474, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The woman is standing in front of the sink, holding a sponge and cleaning the countertop\n\nThe woman is wearing a black shirt and jeans, and her hair is tied back in a ponytail. She is standing in front of the sink, holding a sponge and clean"}, "359659": {"image_id": 359659, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.161880977056518, "Bleu_3": 7.95785637725362e-07, "Bleu_4": 1.772982824869927e-09, "METEOR": 0.21456824206309472, "ROUGE_L": 0.1783625730994152, "CIDEr": 2.0959832714475137e-12, "SPICE": {"All": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 15.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.3333333333333333, "f": 0.2222222222222222, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4444444444444444, "f": 0.39999999999999997, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a bathroom with a toilet, sink, and mirror. The walls are painted a light brown color and the floor is made of wood. There is a window on the left side of the room and a door on the right side. The room is well lit and has a clean appearance."}, "332407": {"image_id": 332407, "Bleu_1": 0.13207547169562125, "Bleu_2": 0.05039754872188373, "Bleu_3": 3.679167415154084e-07, "Bleu_4": 9.990095999341766e-10, "METEOR": 0.14245269131533844, "ROUGE_L": 0.13887307911212293, "CIDEr": 4.421716199401585e-10, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.2857142857142857, "f": 0.2727272727272727, "fn": 15.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.125, "re": 0.125, "f": 0.125, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.14285714285714285, "f": 0.18181818181818182, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.6666666666666666, "f": 0.4705882352941177, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a group of people playing baseball on a field. There are two adults and two children playing the game. The adults are wearing baseball caps and the children are wearing baseball gloves. The adults are holding bats and the children are holding balls. The field is made of grass and"}, "310442": {"image_id": 310442, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.22976150005156148, "Bleu_3": 0.1616397490946152, "Bleu_4": 0.09635230436528824, "METEOR": 0.30343656060631213, "ROUGE_L": 0.330722891566265, "CIDEr": 2.8289516132292456e-10, "SPICE": {"All": {"pr": 0.25, "re": 0.22727272727272727, "f": 0.23809523809523808, "fn": 17.0, "numImages": 1.0, "fp": 15.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.5714285714285714, "re": 0.4, "f": 0.47058823529411764, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 4.0}}, "caption": "The image shows a woman riding a bicycle on a sidewalk. She is wearing a white shirt with a black cat on it and has a black cat on her head. The woman is smiling and appears to be enjoying the ride. The background is a residential area with trees and houses"}, "339344": {"image_id": 339344, "Bleu_1": 0.32608695651465036, "Bleu_2": 0.24077170616624616, "Bleu_3": 0.17402276782115267, "Bleu_4": 0.1384738457070088, "METEOR": 0.27354776800733605, "ROUGE_L": 0.3084702907711757, "CIDEr": 6.425435716472079e-08, "SPICE": {"All": {"pr": 0.037037037037037035, "re": 0.05, "f": 0.0425531914893617, "fn": 19.0, "numImages": 1.0, "fp": 26.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.09090909090909091, "re": 0.125, "f": 0.10526315789473685, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}}, "caption": "The image shows a yellow school bus driving down a street in a residential area. The bus is parked on the side of the road and there are trees and houses visible in the background. The sky is blue and there are clouds in the distance."}, "395997": {"image_id": 395997, "Bleu_1": 0.2142857142818878, "Bleu_2": 0.17654696589776867, "Bleu_3": 0.132168790590052, "Bleu_4": 1.4447001557342331e-05, "METEOR": 0.23624628457880212, "ROUGE_L": 0.2426136363636364, "CIDEr": 7.195467260089885e-13, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.6, "f": 0.3076923076923077, "fn": 4.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.25, "f": 0.13333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.4, "re": 1.0, "f": 0.5714285714285715, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.75, "f": 0.3529411764705882, "fn": 1.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a golden retriever sitting in the back seat of a car. The dog is looking out the window with its tongue hanging out of its mouth. The car is parked on the side of the road with other cars passing by. The sky is cloudy and there are trees and buildings in the"}, "288633": {"image_id": 288633, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.11673403265989704, "Bleu_3": 6.399271623835167e-07, "Bleu_4": 1.5055852953281506e-09, "METEOR": 0.21949993875455684, "ROUGE_L": 0.1821983273596177, "CIDEr": 4.179159395255869e-12, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.2, "f": 0.1702127659574468, "fn": 16.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.6666666666666666, "f": 0.4444444444444444, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a street with horse-drawn carriages and people walking on the sidewalk. There are buildings on either side of the street, with one of them being a large red brick building with a clock tower. The street is lined with trees and there are cars parked on the side of the road"}, "248334": {"image_id": 248334, "Bleu_1": 0.36538461537758876, "Bleu_2": 0.2073316795323305, "Bleu_3": 0.11980191889445956, "Bleu_4": 1.3686701975870404e-05, "METEOR": 0.24862538081301258, "ROUGE_L": 0.26293103448275856, "CIDEr": 3.0007903420258766e-10, "SPICE": {"All": {"pr": 0.23333333333333334, "re": 0.21212121212121213, "f": 0.22222222222222224, "fn": 26.0, "numImages": 1.0, "fp": 23.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 7.0}}, "caption": "The image shows a small wooden boat floating on a river surrounded by lush green vegetation. The boat is filled with people, including children, who are waving and smiling at the camera. The boat is moving slowly down the river, with the sun shining brightly overhead. The water is calm and clear"}, "228541": {"image_id": 228541, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.2047650389408227, "Bleu_3": 0.13423832028711077, "Bleu_4": 0.08298791202772011, "METEOR": 0.2628092091733403, "ROUGE_L": 0.2445589919816724, "CIDEr": 2.769877711086791e-12, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.25, "f": 0.23529411764705882, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.5, "f": 0.4347826086956522, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a group of people standing in front of a building with a large window on the side. There are several cars parked in front of the building, and a fire hydrant is visible in the foreground. The sky is clear and blue, with a few clouds visible in the distance.\n\nThe"}, "351667": {"image_id": 351667, "Bleu_1": 0.17777777777382722, "Bleu_2": 0.06356417261494428, "Bleu_3": 4.546237434540883e-07, "Bleu_4": 1.2230008607068681e-09, "METEOR": 0.09273743170546712, "ROUGE_L": 0.17268223637650387, "CIDEr": 8.117718222935607e-07, "SPICE": {"All": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 14.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2222222222222222, "re": 0.3333333333333333, "f": 0.26666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.375, "f": 0.3157894736842105, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows two pizzas in a baking dish. One pizza has tomato slices on top and the other has cheese slices on top. The pizzas are in a baking dish with a lid on top. The dish is on a countertop in a kitchen."}, "243950": {"image_id": 243950, "Bleu_1": 0.3636363636297521, "Bleu_2": 0.23210354127000468, "Bleu_3": 0.12667936754426512, "Bleu_4": 1.4061400527314794e-05, "METEOR": 0.1900636890683908, "ROUGE_L": 0.2597126130920702, "CIDEr": 8.03032575153984e-10, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.2222222222222222, "f": 0.22641509433962265, "fn": 21.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.125, "f": 0.125, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.4166666666666667, "f": 0.4166666666666667, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a woman in a white tennis outfit playing a tennis match on a grass court. She is holding a tennis racket in her right hand and is about to hit the ball with her left hand. The crowd is watching her from the stands. The sky is clear and sunny.\n\nThe image"}, "120070": {"image_id": 120070, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.1978218215826409, "Bleu_3": 0.1311863860222524, "Bleu_4": 1.4505210378846464e-05, "METEOR": 0.24329100403225648, "ROUGE_L": 0.21403508771929822, "CIDEr": 3.5030116753406694e-12, "SPICE": {"All": {"pr": 0.16, "re": 0.14285714285714285, "f": 0.15094339622641512, "fn": 24.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.3333333333333333, "f": 0.32, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a man sitting at a desk in front of a computer. He is wearing a black shirt and jeans and has a laptop on his lap. There are papers and pens on the desk next to him. The room is dimly lit and there are windows on the wall behind him."}, "539196": {"image_id": 539196, "Bleu_1": 0.24561403508341031, "Bleu_2": 0.1480872194371519, "Bleu_3": 0.07360220454770079, "Bleu_4": 9.269785333031079e-06, "METEOR": 0.21389388842852428, "ROUGE_L": 0.25616797900262467, "CIDEr": 1.3737010542281139e-11, "SPICE": {"All": {"pr": 0.125, "re": 0.15, "f": 0.13636363636363635, "fn": 17.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.375, "f": 0.35294117647058826, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a white horse standing in a stable with its head sticking out of the door. The horse is wearing a halter and has a tag on its neck. The stable is made of wood and has a wooden roof. The horse is looking out of the door with its head sticking out of the"}, "163451": {"image_id": 163451, "Bleu_1": 0.14545454545190087, "Bleu_2": 0.07339758434041055, "Bleu_3": 4.6669078026091577e-07, "Bleu_4": 1.1824181296865702e-09, "METEOR": 0.1578171907462666, "ROUGE_L": 0.14063400576368879, "CIDEr": 9.096722468122257e-14, "SPICE": {"All": {"pr": 0.045454545454545456, "re": 0.04, "f": 0.0425531914893617, "fn": 24.0, "numImages": 1.0, "fp": 21.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}}, "caption": "The image shows a group of people sitting on the ground, one of them is holding a baby in her arms. The people are wearing casual clothes and are looking at the baby. The baby is wearing a pink dress and is holding a toy in its hand. The background is a concrete wall with"}, "401808": {"image_id": 401808, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.20550493309104514, "Bleu_3": 0.13633224439757732, "Bleu_4": 0.0848007939389795, "METEOR": 0.25769929982078676, "ROUGE_L": 0.2544696066746126, "CIDEr": 1.0199973296422846e-10, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.21428571428571427, "f": 0.21428571428571427, "fn": 22.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.14285714285714285, "f": 0.18181818181818182, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.4166666666666667, "f": 0.3846153846153846, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows a man standing on a beach with his dog. The man is wearing a black shirt and jeans, and the dog is wearing a red collar. The beach is covered in rocks and there are mountains in the background. The sky is clear and there are no clouds.\n\nThe"}, "325357": {"image_id": 325357, "Bleu_1": 0.2105263157857803, "Bleu_2": 0.18394180184223394, "Bleu_3": 0.15454316820172162, "Bleu_4": 0.12858902882231668, "METEOR": 0.26358188763595425, "ROUGE_L": 0.27901658090337333, "CIDEr": 1.193802734969816e-12, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.2, "f": 0.1851851851851852, "fn": 20.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.1, "re": 0.125, "f": 0.11111111111111112, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.36363636363636365, "f": 0.36363636363636365, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a metal pot sitting on a brick hearth in front of a brick wall. The pot is made of stainless steel and has a small handle on the side. The hearth is made of brick and has a small opening in the center. The wall behind the pot is made of brick and has"}, "104980": {"image_id": 104980, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.1814970425910117, "Bleu_3": 8.760206462900567e-07, "Bleu_4": 1.9345299022435176e-09, "METEOR": 0.17604655785421175, "ROUGE_L": 0.22889305816135083, "CIDEr": 2.4296512496608005e-10, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.22727272727272727, "f": 0.2, "fn": 17.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5, "f": 0.4210526315789474, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a man standing in front of a bar with a group of people behind him. The man is wearing a white shirt and black pants, and has a tattoo on his left arm. The people behind him are wearing various clothing and are drinking from glasses. There is"}, "467593": {"image_id": 467593, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.12366938847771936, "Bleu_3": 6.783327450171972e-07, "Bleu_4": 1.5968781455284126e-09, "METEOR": 0.18455783389748484, "ROUGE_L": 0.1561100447856686, "CIDEr": 4.0484114603932917e-11, "SPICE": {"All": {"pr": 0.09375, "re": 0.125, "f": 0.10714285714285714, "fn": 21.0, "numImages": 1.0, "fp": 29.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.375, "f": 0.3157894736842105, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The bird is a peacock with bright green feathers and a long, slender neck. It is standing in front of a cage with a mesh door. The bird's eyes are bright and alert, and it appears to be looking out of the cage. The cage is made of metal and"}, "301236": {"image_id": 301236, "Bleu_1": 0.34615384614718936, "Bleu_2": 0.24715576636669082, "Bleu_3": 0.16969817063172157, "Bleu_4": 0.09993298280167971, "METEOR": 0.1617642317738213, "ROUGE_L": 0.35427666314677936, "CIDEr": 0.0013663943737981532, "SPICE": {"All": {"pr": 0.30434782608695654, "re": 0.25925925925925924, "f": 0.27999999999999997, "fn": 20.0, "numImages": 1.0, "fp": 16.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2222222222222222, "re": 0.5, "f": 0.30769230769230765, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Size": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5555555555555556, "re": 0.5555555555555556, "f": 0.5555555555555556, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 5.0}}, "caption": "The image shows a zebra standing in front of a car window. The zebra is wearing a halter and has a tag on its neck. The car window is open and there is a reflection of the zebra in the glass. The background is a green field with trees in the distance."}, "313783": {"image_id": 313783, "Bleu_1": 0.29787234041919425, "Bleu_2": 0.21290467263078663, "Bleu_3": 0.14457494718760727, "Bleu_4": 0.09103462606239374, "METEOR": 0.24225818280081382, "ROUGE_L": 0.30367143746110764, "CIDEr": 1.555635164889086e-08, "SPICE": {"All": {"pr": 0.2, "re": 0.16, "f": 0.17777777777777778, "fn": 21.0, "numImages": 1.0, "fp": 16.0, "tp": 4.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.08333333333333333, "f": 0.10526315789473685, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3, "f": 0.3157894736842105, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a plate with two grilled chicken breasts, a side of mashed potatoes, and a side of carrots. The plate is on a white plate with a fork and knife on the side. The background is a white wall with a window in the background."}, "437347": {"image_id": 437347, "Bleu_1": 0.20689655172057078, "Bleu_2": 0.12049504662365988, "Bleu_3": 6.376515999187724e-07, "Bleu_4": 1.4734890672858558e-09, "METEOR": 0.14796231280031258, "ROUGE_L": 0.1908237747653806, "CIDEr": 5.177623584071517e-14, "SPICE": {"All": {"pr": 0.2, "re": 0.18518518518518517, "f": 0.1923076923076923, "fn": 22.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.1, "re": 0.08333333333333333, "f": 0.0909090909090909, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.4, "f": 0.4000000000000001, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a person in a black and white striped shirt and black pants standing in the middle of a street in front of a large brick building with windows on the upper floors. The person is holding a baseball bat and appears to be about to hit a ball that is lying on the ground. The"}, "210990": {"image_id": 210990, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.18171094607444632, "Bleu_3": 0.10899519002406471, "Bleu_4": 1.2685657992627591e-05, "METEOR": 0.18183380272903024, "ROUGE_L": 0.24811156304474144, "CIDEr": 1.112470391309354e-11, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.12903225806451613, "f": 0.13793103448275862, "fn": 27.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.09090909090909091, "f": 0.1111111111111111, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.2727272727272727, "f": 0.2727272727272727, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a person skiing down a snowy mountain slope. The person is wearing a black and white ski suit and has a backpack on their back. The mountain is covered in snow and there are trees and rocks in the background.\n\nThe person is skiing down the mountain at a steep"}, "244468": {"image_id": 244468, "Bleu_1": 0.2452830188632966, "Bleu_2": 0.15357377920556234, "Bleu_3": 0.0974311856710336, "Bleu_4": 1.1662213550197087e-05, "METEOR": 0.2049130187018614, "ROUGE_L": 0.25894481503941785, "CIDEr": 9.69468205514196e-11, "SPICE": {"All": {"pr": 0.2, "re": 0.16, "f": 0.17777777777777778, "fn": 21.0, "numImages": 1.0, "fp": 16.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.25, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 2.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.3333333333333333, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.2222222222222222, "re": 0.25, "f": 0.23529411764705882, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a small kitten sitting on the windowsill of a red barn. The kitten is looking out of the window, with its ears perked up and its tail curled around its body. The barn has a wooden door with a large window on the side. The sky is blue and there"}, "379161": {"image_id": 379161, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.20597146021399534, "Bleu_3": 0.17761463815099374, "Bleu_4": 0.15235290224324813, "METEOR": 0.2927488951874108, "ROUGE_L": 0.28636096906697955, "CIDEr": 4.788433634141297e-13, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.21739130434782608, "f": 0.2127659574468085, "fn": 18.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4444444444444444, "f": 0.4444444444444444, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a vase with red berries and green leaves in it. The vase is sitting on a table in front of a window with a view of the city. The table has a white tablecloth on it and there are two chairs in front of it. The room has a wooden floor and"}, "446136": {"image_id": 446136, "Bleu_1": 0.4038461538383876, "Bleu_2": 0.3208444739536434, "Bleu_3": 0.2646187076724688, "Bleu_4": 0.22682498879249188, "METEOR": 0.3133993017000862, "ROUGE_L": 0.3588235294117647, "CIDEr": 6.523200909762392e-10, "SPICE": {"All": {"pr": 0.16, "re": 0.23529411764705882, "f": 0.19047619047619052, "fn": 13.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.8, "f": 0.47058823529411764, "fn": 1.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of people sitting around a table with plates of food in front of them. They are all smiling and appear to be enjoying their meal. The table is covered in a red and white checkered tablecloth and there are candles on the table. The room is dimly"}, "193021": {"image_id": 193021, "Bleu_1": 0.39583333332508686, "Bleu_2": 0.2595686547370185, "Bleu_3": 0.1637907249802062, "Bleu_4": 1.7677215259806792e-05, "METEOR": 0.30377401476561794, "ROUGE_L": 0.33639705882352944, "CIDEr": 9.790299343275002e-07, "SPICE": {"All": {"pr": 0.09523809523809523, "re": 0.08333333333333333, "f": 0.08888888888888889, "fn": 22.0, "numImages": 1.0, "fp": 19.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2857142857142857, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a kitchen with wooden floors, white cabinets, and a black stove. There is a dining table with chairs in the center of the room, and a window with blinds on the left side. The room is well lit with overhead lighting and a chandelier hanging"}, "350099": {"image_id": 350099, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.1313579154832357, "Bleu_3": 0.07061638347858736, "Bleu_4": 9.254837436221288e-06, "METEOR": 0.1581059547380037, "ROUGE_L": 0.21903052064631956, "CIDEr": 7.654709089011189e-11, "SPICE": {"All": {"pr": 0.5714285714285714, "re": 0.26666666666666666, "f": 0.36363636363636365, "fn": 11.0, "numImages": 1.0, "fp": 3.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.25, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.75, "re": 0.6, "f": 0.6666666666666665, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 3.0}}, "caption": "The image shows a person standing on a skateboard in the middle of a field. The person is wearing a black suit and holding onto the handlebars of the skateboard. The sky is cloudy and there are trees in the background.\n\nThe image is taken from a low angle, looking up"}, "285433": {"image_id": 285433, "Bleu_1": 0.31249999999348965, "Bleu_2": 0.23063280200236536, "Bleu_3": 0.10496103543446568, "Bleu_4": 1.2660998324356513e-05, "METEOR": 0.2646537037140014, "ROUGE_L": 0.3057644110275689, "CIDEr": 2.2798959640423458e-09, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.17857142857142858, "f": 0.17857142857142858, "fn": 23.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.4, "f": 0.3636363636363636, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a bathroom with a toilet, sink, and mirror. The walls are white and the floor is made of tile. There is a window on the left side of the room.\n\nThe toilet is a white porcelain bowl with a seat and lid. The sink is"}, "471842": {"image_id": 471842, "Bleu_1": 0.35897435896515456, "Bleu_2": 0.27490643727245323, "Bleu_3": 0.20140768086634514, "Bleu_4": 0.14596169407878418, "METEOR": 0.3542266839040766, "ROUGE_L": 0.3468372423596305, "CIDEr": 3.131996841530866e-05, "SPICE": {"All": {"pr": 0.12, "re": 0.11538461538461539, "f": 0.11764705882352941, "fn": 23.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.375, "f": 0.33333333333333326, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a plate of food with a piece of cake on it. The cake has blue and white frosting on it and is topped with a fork. There are also two glasses of wine on the table."}, "24734": {"image_id": 24734, "Bleu_1": 0.27906976743537054, "Bleu_2": 0.23055616707626844, "Bleu_3": 0.17309181846231234, "Bleu_4": 0.10670687651358146, "METEOR": 0.19091250132947155, "ROUGE_L": 0.35580038885288406, "CIDEr": 8.855161944747905e-07, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.1724137931034483, "f": 0.18867924528301888, "fn": 24.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.1, "f": 0.11764705882352941, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.4444444444444444, "f": 0.4210526315789474, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image appears to be a pizza with various toppings such as olives, mushrooms, and cheese on top of a crust. There are two slices of pizza on the plate, and a glass of wine is on the table next to the plate."}, "270222": {"image_id": 270222, "Bleu_1": 0.2142857142818878, "Bleu_2": 0.10811249552151898, "Bleu_3": 6.00416577777256e-07, "Bleu_4": 1.4215774922802648e-09, "METEOR": 0.13632286995515694, "ROUGE_L": 0.1769141531322506, "CIDEr": 3.2140157447914634e-13, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.16216216216216217, "f": 0.18181818181818182, "fn": 31.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.4, "re": 0.2, "f": 0.26666666666666666, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.2857142857142857, "re": 0.26666666666666666, "f": 0.2758620689655172, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image is a clock tower with a large clock face on the front. The clock face has numbers and hands, and there are several other clocks on the sides of the tower. The tower is made of stone and has a steeple on top. There are several windows on the sides of the tower, and"}, "330265": {"image_id": 330265, "Bleu_1": 0.23255813952947546, "Bleu_2": 0.10523401123300179, "Bleu_3": 6.464120807060151e-07, "Bleu_4": 1.6120076571346083e-09, "METEOR": 0.18480519628039963, "ROUGE_L": 0.20734194425560842, "CIDEr": 3.284831314182693e-07, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.125, "f": 0.10526315789473685, "fn": 21.0, "numImages": 1.0, "fp": 30.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a bathroom with a toilet, sink, and mirror. The toilet is a white porcelain bowl with a seat and handle. The sink is a white porcelain basin with a faucet and drain. The mirror is a large, rectangular piece of"}, "498511": {"image_id": 498511, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.21969401454816964, "Bleu_3": 0.16899693052245884, "Bleu_4": 0.13111138531496772, "METEOR": 0.23661050797889774, "ROUGE_L": 0.28773584905660377, "CIDEr": 6.881659597621971e-11, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.375, "f": 0.27906976744186046, "fn": 10.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.4, "f": 0.3636363636363636, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5, "f": 0.4210526315789474, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a herd of horses grazing in a green field surrounded by mountains in the background. The horses are brown and black with long manes and tails. They are standing in a line, looking at each other. The sky is blue and cloudy.\n\nThe image is taken from a distance"}, "163296": {"image_id": 163296, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.15639188177115004, "Bleu_3": 0.09798321550063374, "Bleu_4": 1.1653898241136167e-05, "METEOR": 0.22730053340052148, "ROUGE_L": 0.28367380833748546, "CIDEr": 5.255904550840993e-12, "SPICE": {"All": {"pr": 0.16129032258064516, "re": 0.22727272727272727, "f": 0.18867924528301885, "fn": 17.0, "numImages": 1.0, "fp": 26.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3125, "re": 0.5, "f": 0.38461538461538464, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 5.0}}, "caption": "The image shows a close up of a green, leafy plant with small, white flowers on the ends of the leaves. The leaves are curled and the flowers are clustered together. The plant is growing in a garden with other plants around it.\n\nThe image is taken from a garden in the spring season"}, "251856": {"image_id": 251856, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.1258455564244646, "Bleu_3": 0.06816612219878869, "Bleu_4": 8.966592262802168e-06, "METEOR": 0.18012909196132382, "ROUGE_L": 0.21131639722863746, "CIDEr": 8.403885594640645e-11, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.29411764705882354, "f": 0.2173913043478261, "fn": 12.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.5, "f": 0.25, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5714285714285714, "f": 0.4210526315789474, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a person surfing on a wave in the ocean. The person is lying on the board and paddling with their hands to stay on the wave. The wave is large and white, and the person is wearing a wetsuit and a helmet to protect themselves from the water. The"}, "245173": {"image_id": 245173, "Bleu_1": 0.19230769230399414, "Bleu_2": 0.15041420939612582, "Bleu_3": 0.0767719506444456, "Bleu_4": 9.802862511748142e-06, "METEOR": 0.2746134663163581, "ROUGE_L": 0.22536945812807885, "CIDEr": 2.604856952069668e-12, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.17857142857142858, "f": 0.17857142857142858, "fn": 23.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.1, "f": 0.10526315789473685, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a tall, stone church with a clock tower on top. The church has a large, stone facade with a pointed roof and a steeple on top. The clock tower has a large clock face on it, with hands pointing to the time. The church is surrounded by a cobblestone"}, "469300": {"image_id": 469300, "Bleu_1": 0.2727272727223141, "Bleu_2": 0.15891043153801604, "Bleu_3": 0.07810454645155356, "Bleu_4": 9.783773730343155e-06, "METEOR": 0.256651348517174, "ROUGE_L": 0.2987174504469491, "CIDEr": 4.965464006968293e-11, "SPICE": {"All": {"pr": 0.1, "re": 0.15, "f": 0.12, "fn": 17.0, "numImages": 1.0, "fp": 27.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.08333333333333333, "re": 0.16666666666666666, "f": 0.1111111111111111, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.125, "re": 0.2, "f": 0.15384615384615385, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2857142857142857, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a man standing at a bar with a tray of wine glasses in front of him. There are several people sitting at tables nearby, sipping wine from their glasses. The bar is made of wood and has a white countertop. The walls are painted a light color and there are several windows"}, "547293": {"image_id": 547293, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.15339299776643633, "Bleu_3": 0.0986616396109014, "Bleu_4": 1.1893260654292823e-05, "METEOR": 0.16676195137245312, "ROUGE_L": 0.24497991967871488, "CIDEr": 3.5252959486185373e-10, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.07692307692307693, "f": 0.08333333333333334, "fn": 24.0, "numImages": 1.0, "fp": 20.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2222222222222222, "f": 0.1904761904761905, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a large airplane parked on the tarmac at an airport. The plane has a white body with red and blue stripes on the wings and tail. The plane has a large bumper on the front and a small bumper on the back. The plane has a large t"}, "274593": {"image_id": 274593, "Bleu_1": 0.2448979591786756, "Bleu_2": 0.15971914124669148, "Bleu_3": 0.11764652669092619, "Bleu_4": 0.09172809483333236, "METEOR": 0.1918064373538805, "ROUGE_L": 0.2894768062640882, "CIDEr": 1.0596052214888711e-08, "SPICE": {"All": {"pr": 0.23809523809523808, "re": 0.22727272727272727, "f": 0.23255813953488372, "fn": 17.0, "numImages": 1.0, "fp": 16.0, "tp": 5.0}, "Relation": {"pr": 0.2857142857142857, "re": 0.25, "f": 0.26666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a red fire hydrant on the sidewalk in front of a building. The hydrant has a sign on it that reads, \"Fire Hydrant.\" There are no other objects in the image.\n\nThe image is taken from a bird's eye view, looking down on the hydrant"}, "231879": {"image_id": 231879, "Bleu_1": 0.1960784313687044, "Bleu_2": 0.10846522890718008, "Bleu_3": 0.06215293828145826, "Bleu_4": 8.409805259069613e-06, "METEOR": 0.1589805171213393, "ROUGE_L": 0.21997836278398844, "CIDEr": 1.4045963597144365e-08, "SPICE": {"All": {"pr": 0.25, "re": 0.2631578947368421, "f": 0.25641025641025644, "fn": 14.0, "numImages": 1.0, "fp": 15.0, "tp": 5.0}, "Relation": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5714285714285714, "f": 0.5333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 4.0}}, "caption": "The image shows a group of people standing around a table with a cake on it. The people are all wearing casual clothing and are smiling and looking at the cake. The cake is a white cake with blue frosting and has a number on it. The people are all holding"}, "291257": {"image_id": 291257, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.17733172552966228, "Bleu_3": 0.08456484271613046, "Bleu_4": 1.0435177484324181e-05, "METEOR": 0.18053656517379232, "ROUGE_L": 0.19934640522875818, "CIDEr": 5.975325565447052e-10, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.10714285714285714, "f": 0.11764705882352941, "fn": 25.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.2222222222222222, "re": 0.2, "f": 0.2105263157894737, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a woman lying in bed with her cat on her lap. The woman is wearing a white nightgown and has her hair tied back in a ponytail. The cat is sitting on her lap and looks like it is sleeping. The room is dimly lit and there are some books and"}, "326854": {"image_id": 326854, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.11215443081631235, "Bleu_3": 6.230790884875896e-07, "Bleu_4": 1.475756952410161e-09, "METEOR": 0.2292190152123574, "ROUGE_L": 0.19588953114964675, "CIDEr": 1.0070042679247965e-10, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.21052631578947367, "f": 0.17777777777777778, "fn": 15.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.16666666666666666, "f": 0.14285714285714288, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.375, "f": 0.3157894736842105, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The room is well lit by the windows and the overhead lighting. There is a desk with a computer on it, a chair, and a bookshelf. The floor is made of hardwood and there are two windows on either side of the room. The walls are painted white and there are two doors leading"}, "394449": {"image_id": 394449, "Bleu_1": 0.19672131147218494, "Bleu_2": 0.11451966686088065, "Bleu_3": 0.076321350281743, "Bleu_4": 9.356804071908538e-06, "METEOR": 0.14090380059280624, "ROUGE_L": 0.16495402920497565, "CIDEr": 2.3374469257340997e-15, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.13793103448275862, "f": 0.16666666666666666, "fn": 25.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3076923076923077, "f": 0.32, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a train station with a platform and a few people standing on it. There are trees and plants growing on the platform and the walls of the station are made of brick. The roof is made of metal and there are windows on the sides of the station. The train station is surrounded by buildings and there are"}, "113294": {"image_id": 113294, "Bleu_1": 0.23214285713871177, "Bleu_2": 0.18375590810914594, "Bleu_3": 0.0855125034558448, "Bleu_4": 1.0422052302891999e-05, "METEOR": 0.16738663630188186, "ROUGE_L": 0.2330786026200873, "CIDEr": 4.4838326085402687e-13, "SPICE": {"All": {"pr": 0.2, "re": 0.1724137931034483, "f": 0.1851851851851852, "fn": 24.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5555555555555556, "re": 0.5, "f": 0.5263157894736842, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 5.0}}, "caption": "The image shows a man standing on the edge of a body of water, looking out at a boat in the distance. The man is wearing a hat and has a fishing rod in his hand. There are several other boats in the distance, and the water is calm and peaceful. The sky is a light"}, "8771": {"image_id": 8771, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.1650868521781302, "Bleu_3": 0.1162824824679724, "Bleu_4": 1.3250832891618527e-05, "METEOR": 0.22445962243177445, "ROUGE_L": 0.28367380833748546, "CIDEr": 1.3936493627106433e-11, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.11538461538461539, "f": 0.11320754716981132, "fn": 23.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a man sitting on a snowboard in the snow. He is wearing a black and white jacket and black pants. The snowboard has a red and black design on it. The man is holding the snowboard with both hands and looking down at it. There is a mountain in the background"}, "252388": {"image_id": 252388, "Bleu_1": 0.2444444444390124, "Bleu_2": 0.12909944487067912, "Bleu_3": 0.07291106321801014, "Bleu_4": 9.801278978426892e-06, "METEOR": 0.24337475333474848, "ROUGE_L": 0.24063116370808676, "CIDEr": 3.3181179521527224e-08, "SPICE": {"All": {"pr": 0.2727272727272727, "re": 0.3157894736842105, "f": 0.2926829268292683, "fn": 13.0, "numImages": 1.0, "fp": 16.0, "tp": 6.0}, "Relation": {"pr": 0.2, "re": 0.14285714285714285, "f": 0.16666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5714285714285714, "f": 0.4210526315789474, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a young child sitting in a bathtub, using a toothbrush to brush their teeth. The child is wearing a pink shirt and blue pants, and has a towel wrapped around their head. The bathtub is made of white porcelain and has a"}, "325114": {"image_id": 325114, "Bleu_1": 0.2830188679191884, "Bleu_2": 0.19518886688953246, "Bleu_3": 9.073607102736224e-07, "Bleu_4": 1.966040574165469e-09, "METEOR": 0.2082740625776956, "ROUGE_L": 0.23894795747062114, "CIDEr": 1.1275734487137589e-10, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.14285714285714285, "f": 0.17142857142857143, "fn": 18.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image is of a bathroom with a toilet, sink, and red table. The toilet is on the left side of the room and the sink is on the right side. The table is in the middle of the room and has a red vase on it. The walls are white and the"}, "181739": {"image_id": 181739, "Bleu_1": 0.24561403508341031, "Bleu_2": 0.21964884254960187, "Bleu_3": 0.1739464085458427, "Bleu_4": 0.1307655887487331, "METEOR": 0.3265381817450725, "ROUGE_L": 0.3349807797913234, "CIDEr": 9.29168493946088e-14, "SPICE": {"All": {"pr": 0.125, "re": 0.13636363636363635, "f": 0.13043478260869565, "fn": 19.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a gray cat sitting on a wooden desk in front of a computer monitor. The cat is looking directly at the camera with its eyes. The desk has a few papers and a pen on it. There is a window in the background with a view of the outside. The lighting in the image"}, "185360": {"image_id": 185360, "Bleu_1": 0.2727272727223141, "Bleu_2": 0.17407765595250352, "Bleu_3": 0.08299846557886469, "Bleu_4": 1.0240041382038528e-05, "METEOR": 0.24343011185646413, "ROUGE_L": 0.24610951008645532, "CIDEr": 1.5206936330101441e-12, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.19230769230769232, "f": 0.17857142857142855, "fn": 21.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.08333333333333333, "f": 0.08333333333333333, "fn": 11.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.36363636363636365, "f": 0.32, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a car parked on the side of a dirt road. The car is covered in mud and has a broken windshield. There is a cow standing in the field next to the car. The cow is black and white and has a large horn on its head. The sky is cloudy and"}, "53450": {"image_id": 53450, "Bleu_1": 0.30769230768639055, "Bleu_2": 0.17368336856959593, "Bleu_3": 0.10646139506643364, "Bleu_4": 0.0704441761895975, "METEOR": 0.24567144242103145, "ROUGE_L": 0.2897862232779097, "CIDEr": 2.013130842406079e-10, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.1, "f": 0.0909090909090909, "fn": 18.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.5, "re": 0.3333333333333333, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Attribute": {"pr": 0.125, "re": 0.1, "f": 0.11111111111111112, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.125, "re": 0.16666666666666666, "f": 0.14285714285714288, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}}, "caption": "The image shows a large bus parked on the sidewalk in front of a building. The bus is blue and white and has the words \"tour bus\" written on the side. There are palm trees in the background and a few people walking on the sidewalk.\n\nThe image is taken in a"}, "565087": {"image_id": 565087, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.1593710424504603, "Bleu_3": 7.926525909642893e-07, "Bleu_4": 1.7765182019941675e-09, "METEOR": 0.1461151991306735, "ROUGE_L": 0.2083096186681844, "CIDEr": 7.925180196809933e-10, "SPICE": {"All": {"pr": 0.17647058823529413, "re": 0.10714285714285714, "f": 0.13333333333333333, "fn": 25.0, "numImages": 1.0, "fp": 14.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.25, "f": 0.2727272727272727, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a group of people on the beach, some of them surfing on the waves. The sky is blue and there are some clouds in the distance. The water is choppy and there are some waves crashing against the shore. The people are wearing wetsuits and some of them are holding"}, "54164": {"image_id": 54164, "Bleu_1": 0.2941176470530566, "Bleu_2": 0.20291986247433838, "Bleu_3": 0.09436646347757241, "Bleu_4": 1.1502783619667552e-05, "METEOR": 0.2541621953859228, "ROUGE_L": 0.25553560742070613, "CIDEr": 5.784167178113588e-11, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.2, "f": 0.1923076923076923, "fn": 20.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.7142857142857143, "f": 0.5263157894736842, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a person surfing on a wave in the ocean. The person is wearing a black wetsuit and holding onto the surfboard with one hand while riding the wave with the other. The wave is large and white, with a lot of foam on top. The sky is blue"}, "286774": {"image_id": 286774, "Bleu_1": 0.21999999999560005, "Bleu_2": 0.13401187884939042, "Bleu_3": 7.205793047605259e-07, "Bleu_4": 1.6797199261809358e-09, "METEOR": 0.18845373476324406, "ROUGE_L": 0.19830949284785435, "CIDEr": 2.5224320765321588e-11, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.1875, "f": 0.22641509433962265, "fn": 26.0, "numImages": 1.0, "fp": 15.0, "tp": 6.0}, "Relation": {"pr": 0.3333333333333333, "re": 0.25, "f": 0.28571428571428575, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3, "f": 0.3, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image is of a desk with a computer, keyboard, mouse, and other office supplies. The desk is made of wood and has a white top. There are two chairs in front of the desk, one with a cushion and the other with a backrest. The walls are painted a"}, "398031": {"image_id": 398031, "Bleu_1": 0.18518518518175586, "Bleu_2": 0.1182211503531082, "Bleu_3": 0.06453505325095796, "Bleu_4": 8.520286737324683e-06, "METEOR": 0.19037419942556272, "ROUGE_L": 0.29901960784313725, "CIDEr": 1.7549487639330447e-08, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.14285714285714285, "f": 0.16, "fn": 24.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3076923076923077, "f": 0.32, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a person standing on a beach holding a kite. The person is wearing a black shirt and pants, and has a backpack on their back. The sky is cloudy and there are some trees in the background. The beach is covered in sand and there are some rocks in the for"}, "108541": {"image_id": 108541, "Bleu_1": 0.21153846153439354, "Bleu_2": 0.11155016321612385, "Bleu_3": 6.290089214849726e-07, "Bleu_4": 1.5012173807660588e-09, "METEOR": 0.16630048452435234, "ROUGE_L": 0.18780788177339902, "CIDEr": 4.936698141435373e-12, "SPICE": {"All": {"pr": 0.16129032258064516, "re": 0.20833333333333334, "f": 0.1818181818181818, "fn": 19.0, "numImages": 1.0, "fp": 26.0, "tp": 5.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.125, "f": 0.1, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3333333333333333, "f": 0.27272727272727276, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a highway with a sign that reads \"Take it slow\"\n\nThe image shows a highway with a sign that reads \"Take it slow\" and a truck driving on the road. The sky is cloudy and there are trees on either side of the road.\n\nThe image is taken from"}, "40341": {"image_id": 40341, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.1748949264355777, "Bleu_3": 0.10767861130247747, "Bleu_4": 0.07141456170370668, "METEOR": 0.22985607796563032, "ROUGE_L": 0.2238532110091743, "CIDEr": 1.6053241486312019e-10, "SPICE": {"All": {"pr": 0.25925925925925924, "re": 0.25925925925925924, "f": 0.25925925925925924, "fn": 20.0, "numImages": 1.0, "fp": 20.0, "tp": 7.0}, "Relation": {"pr": 0.1, "re": 0.09090909090909091, "f": 0.09523809523809525, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.5454545454545454, "f": 0.4799999999999999, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The image shows a man standing in the snow holding a shovel. He is wearing a black jacket and black pants. There are several cars parked on the street in front of him. The sky is cloudy and there are no other people in the image.\n\nThe image is taken in"}, "16897": {"image_id": 16897, "Bleu_1": 0.31372549018992696, "Bleu_2": 0.13719886811129003, "Bleu_3": 7.269451760681339e-07, "Bleu_4": 1.681961051813922e-09, "METEOR": 0.2101587137400264, "ROUGE_L": 0.22584228063680117, "CIDEr": 7.910672255747253e-07, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.23809523809523808, "f": 0.22222222222222224, "fn": 16.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5, "f": 0.380952380952381, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a group of people sitting at a table, using laptops and tablets to work on a project. They are all wearing casual clothing and are in a conference room. There are several different images in the collage, including one of a person using a tablet, one of a"}, "351017": {"image_id": 351017, "Bleu_1": 0.30188679244713423, "Bleu_2": 0.2285817953234835, "Bleu_3": 0.19284292682237691, "Bleu_4": 0.15476590172985596, "METEOR": 0.31100469134498615, "ROUGE_L": 0.351295763060469, "CIDEr": 4.469908834527406e-10, "SPICE": {"All": {"pr": 0.10344827586206896, "re": 0.125, "f": 0.11320754716981132, "fn": 21.0, "numImages": 1.0, "fp": 26.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.42857142857142855, "f": 0.2857142857142857, "fn": 4.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a group of people sitting around a table in a restaurant. They are all wearing glasses and have their hands on their laps. There are several bottles of wine on the table. The background is a dark brown color with some lighting coming from the windows.\n\nThe image is taken"}, "230593": {"image_id": 230593, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.1814970425910117, "Bleu_3": 0.1103716852403353, "Bleu_4": 1.293698116812338e-05, "METEOR": 0.21629379905669666, "ROUGE_L": 0.2238532110091743, "CIDEr": 9.825784945902472e-11, "SPICE": {"All": {"pr": 0.10714285714285714, "re": 0.14285714285714285, "f": 0.12244897959183672, "fn": 18.0, "numImages": 1.0, "fp": 25.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.375, "f": 0.26086956521739135, "fn": 5.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}}, "caption": "The image shows a group of geese walking along a path in a park. The geese are white and have black heads and necks. They are walking in a line, with their beaks facing forward. The path is lined with trees and there are benches and a fountain in the distance."}, "129416": {"image_id": 129416, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.23431167444696221, "Bleu_3": 0.2077274647589439, "Bleu_4": 0.1829565422412541, "METEOR": 0.3098956318218857, "ROUGE_L": 0.314974182444062, "CIDEr": 1.1209183601260758e-09, "SPICE": {"All": {"pr": 0.07692307692307693, "re": 0.09090909090909091, "f": 0.08333333333333334, "fn": 20.0, "numImages": 1.0, "fp": 24.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.2222222222222222, "f": 0.2105263157894737, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a group of cows grazing in a field next to a river. The cows are brown and black and have long, curly horns. The field is covered in tall, green grass and there are trees in the background. The sky is clear and blue.\n\nThe image is taken"}, "80328": {"image_id": 80328, "Bleu_1": 0.30434782608034033, "Bleu_2": 0.2014440620697061, "Bleu_3": 0.09733867301596093, "Bleu_4": 1.2101715869638743e-05, "METEOR": 0.2862721689412582, "ROUGE_L": 0.24830393487109906, "CIDEr": 1.7885966003087645e-08, "SPICE": {"All": {"pr": 0.28, "re": 0.2692307692307692, "f": 0.27450980392156865, "fn": 19.0, "numImages": 1.0, "fp": 18.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.46153846153846156, "re": 0.6, "f": 0.5217391304347826, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a herd of zebras grazing in a grassy field. The zebras are standing in a line, with their heads down and their stripes visible. There are several other animals in the background, including a giraffe and a rhinoceros. The sky is clear and"}, "488476": {"image_id": 488476, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.1565560727682472, "Bleu_3": 0.09934208621394808, "Bleu_4": 1.1893260654297589e-05, "METEOR": 0.14866397774075324, "ROUGE_L": 0.15024630541871922, "CIDEr": 2.2722561871946363e-11, "SPICE": {"All": {"pr": 0.25806451612903225, "re": 0.4, "f": 0.3137254901960784, "fn": 12.0, "numImages": 1.0, "fp": 23.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2727272727272727, "re": 0.375, "f": 0.3157894736842105, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}, "Size": {"pr": 0.5, "re": 0.25, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Color": {"pr": 0.3333333333333333, "re": 0.6666666666666666, "f": 0.4444444444444444, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Object": {"pr": 0.45454545454545453, "re": 0.625, "f": 0.5263157894736842, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image is a window display with several white ceramic rabbits on the shelves. The rabbits are arranged in a row and are facing the camera. They are all different sizes and shapes, with some having long ears and others having short ears. The display is in a storefront window, with a"}, "40051": {"image_id": 40051, "Bleu_1": 0.20833333332899312, "Bleu_2": 0.14887283354071895, "Bleu_3": 0.07839548039992822, "Bleu_4": 1.017220586599864e-05, "METEOR": 0.26116830181871703, "ROUGE_L": 0.23461538461538461, "CIDEr": 6.737862247125685e-10, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.12, "f": 0.11538461538461538, "fn": 22.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 8.0, "numImages": 1.0, "fp": 12.0, "tp": 2.0}}, "caption": "The image shows a bathroom with a toilet, sink, and stove. The walls are painted blue and the floor is made of tile. There is a window on the left side of the room and a door on the right.\n\nThe toilet is a white porcelain bowl with"}, "51403": {"image_id": 51403, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.19154015512649578, "Bleu_3": 0.11216278231474784, "Bleu_4": 1.2897158477492775e-05, "METEOR": 0.21444702628476495, "ROUGE_L": 0.23565121412803533, "CIDEr": 6.15673866757001e-11, "SPICE": {"All": {"pr": 0.09375, "re": 0.13043478260869565, "f": 0.10909090909090909, "fn": 20.0, "numImages": 1.0, "fp": 29.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a black dog sitting in front of a wooden chair. The dog is looking up at the camera with its head tilted to the side. The background is a white wall with a wooden floor. The lighting is soft and natural, with shadows on the dog's face and body. The"}, "109907": {"image_id": 109907, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.20218927553934637, "Bleu_3": 0.13311021384054614, "Bleu_4": 0.08246430284993057, "METEOR": 0.21350606113863366, "ROUGE_L": 0.28367380833748546, "CIDEr": 8.928073552850955e-12, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.2857142857142857, "f": 0.24242424242424243, "fn": 10.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.8, "f": 0.6153846153846154, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 4.0}}, "caption": "The image shows a dog standing in the water, with its head tilted back and its tongue hanging out of its mouth. The dog is wearing a collar and tag on its neck. The background is a body of water with trees and grass on the shore. The sky is cloudy with a few"}, "366683": {"image_id": 366683, "Bleu_1": 0.2075471698074048, "Bleu_2": 0.12635334149896824, "Bleu_3": 0.0855482429777259, "Bleu_4": 1.057829854310235e-05, "METEOR": 0.204924428032552, "ROUGE_L": 0.24811156304474144, "CIDEr": 1.6906915354544925e-11, "SPICE": {"All": {"pr": 0.30434782608695654, "re": 0.25925925925925924, "f": 0.27999999999999997, "fn": 20.0, "numImages": 1.0, "fp": 16.0, "tp": 7.0}, "Relation": {"pr": 0.3333333333333333, "re": 0.13333333333333333, "f": 0.19047619047619044, "fn": 13.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.45454545454545453, "re": 0.7142857142857143, "f": 0.5555555555555556, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 5.0}}, "caption": "The image shows a table with a cup of coffee, a sandwich, and a passport on it. The table is made of wood and has a brown color. The cup of coffee is made of ceramic and has a white lid. The sandwich is made of bread and has a variety of toppings"}, "315868": {"image_id": 315868, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.21852940772098994, "Bleu_3": 0.1439792581584426, "Bleu_4": 0.08926896184927745, "METEOR": 0.20400181676362292, "ROUGE_L": 0.22732919254658387, "CIDEr": 1.2364606100343412e-10, "SPICE": {"All": {"pr": 0.08, "re": 0.09090909090909091, "f": 0.0851063829787234, "fn": 20.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.25, "f": 0.23529411764705882, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a motorcycle parked on the sidewalk in front of a building. The motorcycle is a black and white Harley Davidson with a red and white stripe on the side. The motorcycle has a black and white striped seat and a black and white striped handlebar. The motor"}, "316007": {"image_id": 316007, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.12366938847771936, "Bleu_3": 0.08546457042801381, "Bleu_4": 0.060052248933562154, "METEOR": 0.2192993954459013, "ROUGE_L": 0.20998278829604128, "CIDEr": 9.294361694450757e-10, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.24, "f": 0.22222222222222224, "fn": 19.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2222222222222222, "f": 0.26666666666666666, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.4444444444444444, "f": 0.34782608695652173, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows two people in the snow, one holding a snowboard and the other holding a snowboard. They are both wearing blue jackets and black pants. The sky is blue and there are trees in the background.\n\nThe image is in focus and the lighting is good. The people are"}, "430048": {"image_id": 430048, "Bleu_1": 0.2142857142818878, "Bleu_2": 0.10811249552151898, "Bleu_3": 0.06004165777772557, "Bleu_4": 7.994117708696575e-06, "METEOR": 0.1459249823915468, "ROUGE_L": 0.13863636363636364, "CIDEr": 1.0158481681289381e-11, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.23809523809523808, "f": 0.23255813953488372, "fn": 16.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.09090909090909091, "f": 0.09999999999999999, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.5714285714285714, "f": 0.47058823529411764, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a building with a large window on the side. The building has a red and white striped awning over the door. There are several cars parked in front of the building. The sidewalk is made of brick and there are several trees in the background. The sky is blue and there are clouds"}, "478742": {"image_id": 478742, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.1624659147420524, "Bleu_3": 0.10182666119201575, "Bleu_4": 0.06813136779964551, "METEOR": 0.18389668154241576, "ROUGE_L": 0.28773584905660377, "CIDEr": 1.2808310223263593e-11, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.21428571428571427, "f": 0.23529411764705882, "fn": 22.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.14285714285714285, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a bathroom with a toilet, sink, and shower. The walls are white and the floor is made of tile. There is a window on the wall that looks out onto the ocean. The room is well lit and has a large mirror on the wall.\n\nThe image shows a"}, "321742": {"image_id": 321742, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.1814970425910117, "Bleu_3": 0.12634404006922517, "Bleu_4": 0.08051110007483202, "METEOR": 0.2213808483376468, "ROUGE_L": 0.3051907442151345, "CIDEr": 1.5151000954655788e-11, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.16666666666666666, "f": 0.186046511627907, "fn": 20.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4444444444444444, "f": 0.4444444444444444, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a group of motorcycles parked in a parking lot. The motorcycles are black and have a sleek design. They are parked next to each other and appear to be in good condition. There are no people in the image. The background is a concrete parking lot with a"}, "293072": {"image_id": 293072, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.16568337391262167, "Bleu_3": 0.1188942937395292, "Bleu_4": 0.09147827112062705, "METEOR": 0.19279239008197957, "ROUGE_L": 0.3024793388429752, "CIDEr": 2.3550742160280557e-09, "SPICE": {"All": {"pr": 0.07142857142857142, "re": 0.07692307692307693, "f": 0.07407407407407408, "fn": 24.0, "numImages": 1.0, "fp": 26.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2222222222222222, "f": 0.1904761904761905, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a group of children sitting at a table with a cake in front of them. The children are wearing party hats and holding balloons. There are several adults standing behind them, watching them. The table is covered with a red and white checkered tablecloth and there are several"}, "287331": {"image_id": 287331, "Bleu_1": 0.32608695651465036, "Bleu_2": 0.2823298512804342, "Bleu_3": 0.26263562830749626, "Bleu_4": 0.2409464951786249, "METEOR": 0.33809709056393816, "ROUGE_L": 0.37245590230664866, "CIDEr": 3.542635925597916e-08, "SPICE": {"All": {"pr": 0.3333333333333333, "re": 0.26666666666666666, "f": 0.2962962962962963, "fn": 22.0, "numImages": 1.0, "fp": 16.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.5, "re": 0.3, "f": 0.37499999999999994, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.6666666666666666, "f": 0.5714285714285715, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 2.0}, "Object": {"pr": 0.4166666666666667, "re": 0.5, "f": 0.45454545454545453, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The dog is jumping up to catch a frisbee in the air. The dog is wearing a green collar and a green leash. The dog is jumping up to catch a frisbee in the air. The dog is wearing a green collar and a green leash."}, "306667": {"image_id": 306667, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.20550493309104517, "Bleu_3": 0.16163974909461523, "Bleu_4": 0.11458284589810852, "METEOR": 0.19299784324134361, "ROUGE_L": 0.25722891566265055, "CIDEr": 3.495575153156575e-11, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.38461538461538464, "f": 0.25, "fn": 8.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.1, "re": 0.25, "f": 0.14285714285714288, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.5, "f": 0.36363636363636365, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.2, "re": 0.4, "f": 0.26666666666666666, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a group of people standing on a soccer field. They are all wearing soccer jerseys and are watching a soccer game. There is a soccer ball on the field and a soccer goal in the background. The people are all standing in a line and are watching the game"}, "160855": {"image_id": 160855, "Bleu_1": 0.3207547169750801, "Bleu_2": 0.27206675522841467, "Bleu_3": 0.17972710952768617, "Bleu_4": 0.1038048610055842, "METEOR": 0.3042858130888694, "ROUGE_L": 0.3152325486014273, "CIDEr": 4.0786448406126384e-11, "SPICE": {"All": {"pr": 0.21621621621621623, "re": 0.3333333333333333, "f": 0.26229508196721313, "fn": 16.0, "numImages": 1.0, "fp": 29.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.5, "f": 0.28571428571428575, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.6666666666666666, "re": 0.6666666666666666, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 2.0}, "Object": {"pr": 0.46153846153846156, "re": 0.5454545454545454, "f": 0.4999999999999999, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a group of people flying kites in a park. The kites are made of colorful fabric and have tails that are attached to the ground. The people are standing on the grass and watching the kites fly. There are trees in the background and a blue sky above.\n\nThe image"}, "499622": {"image_id": 499622, "Bleu_1": 0.20370370369993146, "Bleu_2": 0.16402518938356242, "Bleu_3": 0.12743613201512047, "Bleu_4": 0.09491489917644469, "METEOR": 0.271141491043817, "ROUGE_L": 0.25326215895610915, "CIDEr": 1.0362072497306244e-12, "SPICE": {"All": {"pr": 0.26666666666666666, "re": 0.36363636363636365, "f": 0.30769230769230765, "fn": 14.0, "numImages": 1.0, "fp": 22.0, "tp": 8.0}, "Relation": {"pr": 0.2222222222222222, "re": 0.2222222222222222, "f": 0.2222222222222222, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.75, "f": 0.5454545454545454, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The image shows a police officer riding a motorcycle on the sidewalk. The officer is wearing a helmet and has a serious expression on his face. The motorcycle is white and has a blue and white striped seat. The officer is driving on the right side of the road and there are trees and"}, "124796": {"image_id": 124796, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.1596564940032875, "Bleu_3": 0.10132945277550723, "Bleu_4": 1.213365000609625e-05, "METEOR": 0.18890027921215505, "ROUGE_L": 0.26704190118824267, "CIDEr": 5.4334811853840346e-11, "SPICE": {"All": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 15.0, "numImages": 1.0, "fp": 20.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.125, "f": 0.18181818181818182, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.8, "f": 0.5000000000000001, "fn": 1.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a person standing in the water, holding a yellow plastic ball. The person is wearing a striped shirt and shorts, and has a look of concentration on their face. The background is a beach with sand and waves.\n\nThe image is taken from a low angle, looking down"}, "399666": {"image_id": 399666, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.16568337391262167, "Bleu_3": 0.08243669902025869, "Bleu_4": 1.0393938325822104e-05, "METEOR": 0.21567191970258032, "ROUGE_L": 0.28024502297090353, "CIDEr": 9.539970320834843e-08, "SPICE": {"All": {"pr": 0.1, "re": 0.14285714285714285, "f": 0.11764705882352941, "fn": 18.0, "numImages": 1.0, "fp": 27.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.375, "f": 0.3, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a person jumping over a sandy beach with a surfboard in the background. The person is wearing a red shirt and black shorts, and has a surfboard in their hand. The sky is blue and there are waves in the background. The image is in black and white"}, "359965": {"image_id": 359965, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.11651034560483009, "Bleu_3": 0.0815823712251846, "Bleu_4": 0.057696334521472034, "METEOR": 0.14919805224722962, "ROUGE_L": 0.18780788177339902, "CIDEr": 8.646911397834929e-10, "SPICE": {"All": {"pr": 0.125, "re": 0.1, "f": 0.11111111111111112, "fn": 27.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a toilet with a seat and a handle on the side. The toilet is in a bathroom with a sink and a mirror on the wall. There is a toilet paper holder on the wall next to the toilet. The floor is made of tile and there is a"}, "320425": {"image_id": 320425, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.19026059765810296, "Bleu_3": 0.142537531538372, "Bleu_4": 0.10426904694909196, "METEOR": 0.2317981028877836, "ROUGE_L": 0.26293103448275856, "CIDEr": 4.220007862841981e-11, "SPICE": {"All": {"pr": 0.22580645161290322, "re": 0.28, "f": 0.25, "fn": 18.0, "numImages": 1.0, "fp": 24.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.17647058823529413, "re": 0.3333333333333333, "f": 0.23076923076923078, "fn": 6.0, "numImages": 1.0, "fp": 14.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 1.0, "f": 0.33333333333333337, "fn": 0.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Object": {"pr": 0.5, "re": 0.4, "f": 0.4444444444444445, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 4.0}}, "caption": "The image shows a group of giraffes standing in a field with a fence in the background. The giraffes are all facing the same direction and appear to be looking at something in the distance. The sky is blue and there are some trees in the background.\n\nThe image is taken from"}, "352286": {"image_id": 352286, "Bleu_1": 0.17647058823183395, "Bleu_2": 0.08401680504001675, "Bleu_3": 5.242181839106549e-07, "Bleu_4": 1.3162056532389086e-09, "METEOR": 0.1602760171413934, "ROUGE_L": 0.18654434250764526, "CIDEr": 6.361887026082568e-11, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.17647058823529413, "f": 0.15, "fn": 14.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.5, "f": 0.37499999999999994, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The room is empty except for a red couch, a television, and a lamp. The walls are painted white and there are no windows. The floor is made of hardwood.\n\nThe lamp is hanging from the ceiling and has a red shade. The television is on the wall and has a"}, "79472": {"image_id": 79472, "Bleu_1": 0.33333333332679743, "Bleu_2": 0.19999999999603923, "Bleu_3": 0.11775100856360317, "Bleu_4": 0.07636830779166373, "METEOR": 0.2296011964906197, "ROUGE_L": 0.22889305816135083, "CIDEr": 3.164887149766583e-11, "SPICE": {"All": {"pr": 0.125, "re": 0.10714285714285714, "f": 0.11538461538461538, "fn": 25.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.08333333333333333, "f": 0.10526315789473685, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a desk with a computer, printer, and other office supplies. The desk is made of wood and has a lamp on it. The room has a window with curtains and a view of the backyard.\n\nThe image is taken in a home office with a wooden desk and"}, "450500": {"image_id": 450500, "Bleu_1": 0.19999999999600004, "Bleu_2": 0.1428571428542566, "Bleu_3": 0.10844960613620269, "Bleu_4": 0.0858329459813088, "METEOR": 0.1839302721580219, "ROUGE_L": 0.25386444708680145, "CIDEr": 8.479262815693285e-10, "SPICE": {"All": {"pr": 0.16, "re": 0.2, "f": 0.17777777777777778, "fn": 16.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.23076923076923078, "re": 0.42857142857142855, "f": 0.3, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a group of people standing on the sidewalk in front of a building. They are all wearing black clothing and have their hands in their pockets. The building is a large, white structure with a red roof and a sign that reads, \"Hollywoo\n\nThe image shows a"}, "16574": {"image_id": 16574, "Bleu_1": 0.2653061224435652, "Bleu_2": 0.14869042853022912, "Bleu_3": 0.09798645698584388, "Bleu_4": 0.06724888422819458, "METEOR": 0.14677079221704414, "ROUGE_L": 0.22679390259015986, "CIDEr": 7.774523848410721e-10, "SPICE": {"All": {"pr": 0.045454545454545456, "re": 0.05263157894736842, "f": 0.04878048780487805, "fn": 18.0, "numImages": 1.0, "fp": 21.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.1111111111111111, "re": 0.16666666666666666, "f": 0.13333333333333333, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}}, "caption": "The image shows a group of people dressed in medieval clothing, standing on a snowy field. They are holding swords and shields, and appear to be preparing for battle. The sky is dark and cloudy, with snow falling from the sky.\n\nThe image is well lit, with the snow"}, "370677": {"image_id": 370677, "Bleu_1": 0.27083333332769105, "Bleu_2": 0.18594210949553755, "Bleu_3": 0.11455372707190037, "Bleu_4": 0.07602453795872412, "METEOR": 0.1781420883280684, "ROUGE_L": 0.19110275689223058, "CIDEr": 6.543089770640619e-08, "SPICE": {"All": {"pr": 0.26666666666666666, "re": 0.3333333333333333, "f": 0.2962962962962963, "fn": 16.0, "numImages": 1.0, "fp": 22.0, "tp": 8.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.125, "f": 0.11764705882352941, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3, "re": 0.375, "f": 0.33333333333333326, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}, "Size": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5, "f": 0.4210526315789474, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a group of women standing in front of a bakery. They are wearing red and yellow uniforms and have their hair tied back in ponytails. They are smiling and looking at the camera. There are several baked goods on the shelves behind them, including bread,"}, "416739": {"image_id": 416739, "Bleu_1": 0.2075471698074048, "Bleu_2": 0.15475060698405138, "Bleu_3": 0.07772573699771664, "Bleu_4": 9.84421602524371e-06, "METEOR": 0.18838851608116866, "ROUGE_L": 0.21266705403834982, "CIDEr": 1.0852603485624582e-11, "SPICE": {"All": {"pr": 0.043478260869565216, "re": 0.041666666666666664, "f": 0.0425531914893617, "fn": 23.0, "numImages": 1.0, "fp": 22.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.1, "re": 0.1, "f": 0.10000000000000002, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}}, "caption": "The image shows a bedroom with a white bed, a wooden dresser, and a window with a view of the outside. The walls are painted white and there is a wooden floor. The room is dimly lit by a small window with a view of the outside.\n\nThe image shows a bedroom with"}, "136218": {"image_id": 136218, "Bleu_1": 0.35185185184533607, "Bleu_2": 0.3155641183413238, "Bleu_3": 0.2675411743116495, "Bleu_4": 0.22642529263775382, "METEOR": 0.32150062403851115, "ROUGE_L": 0.3355335533553355, "CIDEr": 1.215907590584442e-09, "SPICE": {"All": {"pr": 0.08, "re": 0.07407407407407407, "f": 0.07692307692307691, "fn": 25.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.18181818181818182, "f": 0.17391304347826086, "fn": 9.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image is of a city skyline with tall buildings and a large airplane flying overhead. The sky is clear and blue. There are no people in the image.\n\nThe image is in black and white. The buildings are made of glass and steel, and they are very tall. There are no trees or"}, "528516": {"image_id": 528516, "Bleu_1": 0.23913043477741028, "Bleu_2": 0.1457945082861785, "Bleu_3": 0.07846510342081583, "Bleu_4": 1.029533066265393e-05, "METEOR": 0.2552100195306546, "ROUGE_L": 0.33107191316146534, "CIDEr": 2.835441436174937e-09, "SPICE": {"All": {"pr": 0.12, "re": 0.16666666666666666, "f": 0.13953488372093023, "fn": 15.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1, "re": 0.25, "f": 0.14285714285714288, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2857142857142857, "f": 0.2222222222222222, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a frying pan with a variety of vegetables, including carrots, green beans, and bell peppers, cooking in it. The pan is on a stove and there is a wooden spoon in the pan. The vegetables are cut into small pieces and are cooking"}, "340884": {"image_id": 340884, "Bleu_1": 0.22916666666189242, "Bleu_2": 0.12094486170917773, "Bleu_3": 0.06825571246936571, "Bleu_4": 9.168559860005064e-06, "METEOR": 0.23410718363899558, "ROUGE_L": 0.28696236559139787, "CIDEr": 3.734060843320475e-10, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.18181818181818182, "f": 0.16, "fn": 18.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4, "f": 0.380952380952381, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows an elephant standing in the dirt next to a small pond. The elephant is brown with a long, curved tusk and large ears. The elephant is standing on its hind legs and looking down at the water in the pond. There are some rocks and"}, "411241": {"image_id": 411241, "Bleu_1": 0.27906976743537054, "Bleu_2": 0.14118624159718107, "Bleu_3": 7.863217694465758e-07, "Bleu_4": 1.867175871883868e-09, "METEOR": 0.16697853081804223, "ROUGE_L": 0.24881033310673015, "CIDEr": 1.5190229449384259e-06, "SPICE": {"All": {"pr": 0.12903225806451613, "re": 0.18181818181818182, "f": 0.1509433962264151, "fn": 18.0, "numImages": 1.0, "fp": 27.0, "tp": 4.0}, "Relation": {"pr": 0.07142857142857142, "re": 0.1111111111111111, "f": 0.08695652173913043, "fn": 8.0, "numImages": 1.0, "fp": 13.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}}, "caption": "The man is sitting on the couch with his laptop on his lap. He is wearing a blue shirt and black pants. There is a blanket on the couch next to him. The room is dimly lit by a lamp on the table."}, "76454": {"image_id": 76454, "Bleu_1": 0.2826086956460303, "Bleu_2": 0.20966962740242623, "Bleu_3": 0.1441827180568874, "Bleu_4": 0.0913729807004326, "METEOR": 0.250343711187177, "ROUGE_L": 0.3319226118500605, "CIDEr": 1.2748121644817984e-07, "SPICE": {"All": {"pr": 0.28, "re": 0.3333333333333333, "f": 0.30434782608695654, "fn": 14.0, "numImages": 1.0, "fp": 18.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.46153846153846156, "re": 0.5454545454545454, "f": 0.4999999999999999, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 6.0}}, "caption": "The image shows a giraffe standing on a fence in front of a city skyline. The giraffe is looking towards the skyline, with its head tilted to the side. The skyline is made up of tall buildings and skyscrapers, with a large bridge in the background"}, "227599": {"image_id": 227599, "Bleu_1": 0.31249999999348965, "Bleu_2": 0.21573723181113055, "Bleu_3": 0.1593617613384745, "Bleu_4": 0.11580903993958114, "METEOR": 0.2699765952832612, "ROUGE_L": 0.31282051282051276, "CIDEr": 4.0208112491385595e-09, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.08333333333333333, "f": 0.08333333333333333, "fn": 22.0, "numImages": 1.0, "fp": 22.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a group of people standing on the beach with surfboards. They are all wearing swimsuits and sunglasses. The man in the center is holding a surfboard and the woman on the right is holding a surfboard as well. The other people are standing around them"}, "233528": {"image_id": 233528, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.09288407280072536, "Bleu_3": 5.60482607100497e-07, "Bleu_4": 1.3839209880654026e-09, "METEOR": 0.14944296484153258, "ROUGE_L": 0.23181324647122695, "CIDEr": 6.478789262739876e-08, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.11428571428571428, "f": 0.12903225806451613, "fn": 31.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.25, "f": 0.28571428571428575, "fn": 12.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a cluttered office with a desk, chair, and computer. There are papers and books on the desk and shelves. The walls are painted white and there are windows on one side of the room. The floor is made of hardwood. The room is well lit by natural light"}, "69668": {"image_id": 69668, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.17733172552966228, "Bleu_3": 0.12196360807045269, "Bleu_4": 0.07722895417973391, "METEOR": 0.2785846377699328, "ROUGE_L": 0.2853801169590643, "CIDEr": 5.813272693332004e-11, "SPICE": {"All": {"pr": 0.09375, "re": 0.0967741935483871, "f": 0.09523809523809523, "fn": 28.0, "numImages": 1.0, "fp": 29.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.25, "f": 0.23076923076923075, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a man in a yellow shirt and black pants standing in front of an oven. He is wearing a white apron and has a look of concentration on his face as he works on something inside the oven. The oven is large and has a glass door that is open, reve"}, "446651": {"image_id": 446651, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.20195126571496114, "Bleu_3": 0.16085047993120216, "Bleu_4": 0.13646505647648105, "METEOR": 0.32585564418145074, "ROUGE_L": 0.3433395872420263, "CIDEr": 7.460895708648763e-10, "SPICE": {"All": {"pr": 0.10714285714285714, "re": 0.16666666666666666, "f": 0.13043478260869565, "fn": 15.0, "numImages": 1.0, "fp": 25.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.42857142857142855, "f": 0.2857142857142857, "fn": 4.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a man and a woman riding on the back of a motorcycle. The man is wearing a helmet and the woman is wearing a helmet and a jacket. They are both smiling and appear to be enjoying the ride. The motorcycle is a black and silver color and"}, "127068": {"image_id": 127068, "Bleu_1": 0.23809523808956923, "Bleu_2": 0.201619459631519, "Bleu_3": 0.12667132083273588, "Bleu_4": 0.08496549121092281, "METEOR": 0.22390765407296503, "ROUGE_L": 0.308080808080808, "CIDEr": 4.2959755116030214e-07, "SPICE": {"All": {"pr": 0.3, "re": 0.375, "f": 0.33333333333333326, "fn": 10.0, "numImages": 1.0, "fp": 14.0, "tp": 6.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.6666666666666666, "f": 0.5333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a man standing next to a surfboard on the beach. The surfboard has a logo on it and the man is wearing a white shirt and shorts. The sky is blue and there are palm trees in the background."}, "390292": {"image_id": 390292, "Bleu_1": 0.26923076922559175, "Bleu_2": 0.17797249265986642, "Bleu_3": 0.13633224439757732, "Bleu_4": 0.11160412118083013, "METEOR": 0.22724217973464766, "ROUGE_L": 0.28175519630484985, "CIDEr": 1.488864060101496e-09, "SPICE": {"All": {"pr": 0.23809523809523808, "re": 0.2, "f": 0.21739130434782608, "fn": 20.0, "numImages": 1.0, "fp": 16.0, "tp": 5.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2222222222222222, "re": 0.2222222222222222, "f": 0.2222222222222222, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 1.0, "f": 0.4, "fn": 0.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Object": {"pr": 0.3333333333333333, "re": 0.2222222222222222, "f": 0.26666666666666666, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}}, "caption": "The image shows a woman sitting in a chair, looking at her phone. She is wearing a white shirt and black pants. The background is a blue and white striped wall with a red and white striped floor. There is a blue and white striped curtain on the window. The woman is"}, "579158": {"image_id": 579158, "Bleu_1": 0.3061224489733445, "Bleu_2": 0.1956151991049542, "Bleu_3": 0.11764652669092619, "Bleu_4": 1.3716549481276805e-05, "METEOR": 0.22909500159536583, "ROUGE_L": 0.26940063091482647, "CIDEr": 1.1172067933022633e-09, "SPICE": {"All": {"pr": 0.42105263157894735, "re": 0.2962962962962963, "f": 0.34782608695652173, "fn": 19.0, "numImages": 1.0, "fp": 11.0, "tp": 8.0}, "Relation": {"pr": 0.25, "re": 0.2857142857142857, "f": 0.26666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.25, "re": 0.1, "f": 0.14285714285714288, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.7142857142857143, "re": 0.5, "f": 0.588235294117647, "fn": 5.0, "numImages": 1.0, "fp": 2.0, "tp": 5.0}}, "caption": "The image shows a blue and white airplane on the tarmac at an airport. The plane has its landing gear extended and is ready to take off. There are other airplanes in the background, parked on the tarmac. The sky is clear and there are no clouds in sight."}, "396051": {"image_id": 396051, "Bleu_1": 0.2745098039161861, "Bleu_2": 0.16568337391262167, "Bleu_3": 0.10386373237947197, "Bleu_4": 1.2360545409967114e-05, "METEOR": 0.22500104202699464, "ROUGE_L": 0.2501464557703574, "CIDEr": 2.0062234954220113e-10, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.18181818181818182, "f": 0.18604651162790697, "fn": 18.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.25, "f": 0.23529411764705882, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a large airport terminal with several planes parked on the tarmac. There are several people standing around the terminal, looking at their phones or talking to each other. The sky is cloudy and there are some clouds in the distance. The airport is surrounded by a fence and"}, "149252": {"image_id": 149252, "Bleu_1": 0.2448979591786756, "Bleu_2": 0.15971914124669148, "Bleu_3": 0.08157154566934656, "Bleu_4": 1.042232377987996e-05, "METEOR": 0.17457976031642, "ROUGE_L": 0.26341764342998153, "CIDEr": 1.171823208611988e-09, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 20.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.3, "re": 0.3, "f": 0.3, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows two zebras standing in a field with trees in the background. The zebras are standing close together and looking at each other. They are both wearing brown and white stripes.\n\nThe image is taken in a dry, arid environment with a clear blue sky. The zeb"}, "405365": {"image_id": 405365, "Bleu_1": 0.36538461537758876, "Bleu_2": 0.2073316795323305, "Bleu_3": 0.09508684604043703, "Bleu_4": 1.1509098628155357e-05, "METEOR": 0.27137855182658144, "ROUGE_L": 0.28773584905660377, "CIDEr": 2.6568694149585236e-11, "SPICE": {"All": {"pr": 0.3333333333333333, "re": 0.36363636363636365, "f": 0.34782608695652173, "fn": 14.0, "numImages": 1.0, "fp": 16.0, "tp": 8.0}, "Relation": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.6666666666666666, "f": 0.5714285714285715, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 6.0}}, "caption": "The image shows a group of people skiing down a snowy slope. They are wearing ski gear and standing on skis. The snow is covered in trees and there is a mountain in the background. The sky is cloudy and there is a lot of snow on the ground.\n\nThe image is"}, "314757": {"image_id": 314757, "Bleu_1": 0.23214285713871177, "Bleu_2": 0.14527180078437643, "Bleu_3": 0.09211555193837166, "Bleu_4": 1.1019978611152551e-05, "METEOR": 0.10955213840416902, "ROUGE_L": 0.1698218262806236, "CIDEr": 7.117675662303565e-11, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.19047619047619047, "f": 0.18604651162790697, "fn": 17.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.125, "re": 0.16666666666666666, "f": 0.14285714285714288, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.42857142857142855, "f": 0.3529411764705882, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a living room with a couch, a coffee table, and a television. There are two people sitting on the couch, one holding a laptop and the other holding a remote control. The walls are painted a light blue color and there are windows on either side of the room. The floor is made"}, "245301": {"image_id": 245301, "Bleu_1": 0.34693877550312374, "Bleu_2": 0.24046440328937668, "Bleu_3": 0.15454026854146363, "Bleu_4": 0.11255098183926564, "METEOR": 0.20825642277143078, "ROUGE_L": 0.23303196230739842, "CIDEr": 4.967542835215872e-10, "SPICE": {"All": {"pr": 0.16, "re": 0.15384615384615385, "f": 0.1568627450980392, "fn": 22.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a woman riding a horse on a grassy field. The woman is wearing a red coat and riding boots, and the horse is wearing a saddle and bridle. The background is a cloudy sky with some trees in the distance.\n\nThe image is of a woman"}, "215650": {"image_id": 215650, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.1447907475849832, "Bleu_3": 0.0930756528209598, "Bleu_4": 1.1213327957896794e-05, "METEOR": 0.16197795459662143, "ROUGE_L": 0.14896214896214896, "CIDEr": 1.2923122682094497e-13, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.29411764705882354, "f": 0.23255813953488372, "fn": 12.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.375, "re": 0.75, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.2222222222222222, "re": 0.4, "f": 0.2857142857142857, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "This is a black and white photograph of a city street in the rain. There are several cars parked on the side of the road, and people are walking down the sidewalk. There are also several buildings visible in the background.\n\nThe image is taken from a high angle, looking down on the street."}, "475856": {"image_id": 475856, "Bleu_1": 0.2931034482708086, "Bleu_2": 0.21512668786420022, "Bleu_3": 0.14896589444910865, "Bleu_4": 0.10470850599225107, "METEOR": 0.2796067018048704, "ROUGE_L": 0.31803962460896773, "CIDEr": 2.031222541842481e-12, "SPICE": {"All": {"pr": 0.25, "re": 0.23809523809523808, "f": 0.24390243902439024, "fn": 16.0, "numImages": 1.0, "fp": 15.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.4, "f": 0.4444444444444445, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 4.0}}, "caption": "The man in the image is wearing a black suit and tie, and is standing in front of a door. He is holding a briefcase in one hand and has the other hand on his hip. The floor is made of wood and there is a rug in front of the door. The walls are painted white and"}, "387776": {"image_id": 387776, "Bleu_1": 0.25999999999480006, "Bleu_2": 0.07284313590699668, "Bleu_3": 4.799308605281424e-07, "Bleu_4": 1.2383960073346406e-09, "METEOR": 0.13061786439205095, "ROUGE_L": 0.19488817891373802, "CIDEr": 5.908686076507705e-11, "SPICE": {"All": {"pr": 0.041666666666666664, "re": 0.043478260869565216, "f": 0.0425531914893617, "fn": 22.0, "numImages": 1.0, "fp": 23.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.1, "re": 0.1111111111111111, "f": 0.10526315789473685, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}}, "caption": "The image shows a person snowboarding down a snowy slope. The person is wearing a red and black snowboarding suit and has their arms outstretched as they jump off a ramp. The snowy slope is covered in trees and there is a mountain in the background. The sky is blue"}, "332877": {"image_id": 332877, "Bleu_1": 0.2264150943353507, "Bleu_2": 0.0933181271719728, "Bleu_3": 5.547797716282368e-07, "Bleu_4": 1.3594021798767564e-09, "METEOR": 0.15922022024603982, "ROUGE_L": 0.18100890207715134, "CIDEr": 2.160551798257552e-12, "SPICE": {"All": {"pr": 0.03571428571428571, "re": 0.07142857142857142, "f": 0.047619047619047616, "fn": 13.0, "numImages": 1.0, "fp": 27.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.08333333333333333, "re": 0.25, "f": 0.125, "fn": 3.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}}, "caption": "The image shows a busy street with many motorcycles and cars driving down the road. There are trees on either side of the road and people walking on the sidewalk. The sky is clear and blue.\n\nThe image shows a busy street with many motorcycles and cars driving down the road. There are"}, "325385": {"image_id": 325385, "Bleu_1": 0.2264150943353507, "Bleu_2": 0.13197176106186115, "Bleu_3": 0.06989787123302858, "Bleu_4": 9.090870283563952e-06, "METEOR": 0.1975279361954025, "ROUGE_L": 0.21721068249258166, "CIDEr": 9.760259605260011e-12, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.2, "f": 0.23529411764705882, "fn": 24.0, "numImages": 1.0, "fp": 15.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.375, "re": 0.42857142857142855, "f": 0.39999999999999997, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}, "Size": {"pr": 0.6666666666666666, "re": 0.6666666666666666, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 2.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.2727272727272727, "f": 0.3, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a large open office space with several desks and chairs. There are several computers on the desks and a large screen on the wall. The walls are painted red and there are several windows on the ceiling.\n\nThe image is taken from a bird's eye view, showing the entire"}, "535997": {"image_id": 535997, "Bleu_1": 0.23214285713871177, "Bleu_2": 0.12993504870706918, "Bleu_3": 6.787131897112119e-07, "Bleu_4": 1.5584603208953362e-09, "METEOR": 0.1651609497992605, "ROUGE_L": 0.2426136363636364, "CIDEr": 1.159337032833313e-13, "SPICE": {"All": {"pr": 0.12, "re": 0.14285714285714285, "f": 0.13043478260869565, "fn": 18.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.16666666666666666, "f": 0.13333333333333333, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.25, "f": 0.23529411764705882, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a church with a tall steeple and a clock tower. The church is made of stone and has a pointed roof. The clock tower is made of metal and has a large clock face on it. The sky is dark and there are stars visible in the sky.\n\nThe image is taken at"}, "445038": {"image_id": 445038, "Bleu_1": 0.1694915254208561, "Bleu_2": 0.1324145236660892, "Bleu_3": 0.09735820273347665, "Bleu_4": 0.06371363809162262, "METEOR": 0.21175288209263132, "ROUGE_L": 0.20344635908838243, "CIDEr": 2.8956591534314774e-15, "SPICE": {"All": {"pr": 0.03571428571428571, "re": 0.038461538461538464, "f": 0.03703703703703704, "fn": 25.0, "numImages": 1.0, "fp": 27.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.07142857142857142, "re": 0.1, "f": 0.08333333333333333, "fn": 9.0, "numImages": 1.0, "fp": 13.0, "tp": 1.0}}, "caption": "The image shows a plane flying in the sky with a sunset in the background. The plane is flying low and appears to be on a mission. The sky is filled with clouds and the sun is setting in the background. The image is taken from a distance and the plane is small in the frame. The colors in"}, "558784": {"image_id": 558784, "Bleu_1": 0.2941176470530566, "Bleu_2": 0.21693045781436016, "Bleu_3": 0.12430587656291649, "Bleu_4": 0.07953502757115866, "METEOR": 0.1905703266914544, "ROUGE_L": 0.26116207951070336, "CIDEr": 2.0263419546075863e-10, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.23076923076923078, "f": 0.22641509433962265, "fn": 20.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.1111111111111111, "f": 0.1111111111111111, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.14285714285714285, "f": 0.16666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4, "f": 0.34782608695652173, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "This is a picture of a man standing on the edge of a pond, holding a fishing rod. The man is wearing a baseball cap and sunglasses, and he is standing in front of a green grassy area with trees in the background. The pond is surrounded by a dirt path"}, "301221": {"image_id": 301221, "Bleu_1": 0.34693877550312374, "Bleu_2": 0.20824828195446654, "Bleu_3": 0.09735426869589826, "Bleu_4": 1.190081738688603e-05, "METEOR": 0.20756402587000186, "ROUGE_L": 0.27096057745696833, "CIDEr": 1.2755845257568674e-07, "SPICE": {"All": {"pr": 0.3333333333333333, "re": 0.2608695652173913, "f": 0.2926829268292683, "fn": 17.0, "numImages": 1.0, "fp": 12.0, "tp": 6.0}, "Relation": {"pr": 0.2, "re": 0.09090909090909091, "f": 0.12500000000000003, "fn": 10.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.375, "f": 0.39999999999999997, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 3.0}}, "caption": "The image shows a person sitting at a table with a pizza in front of them. The pizza has eggs, bacon, and spinach on it. There are also cups of coffee and a knife on the table. The person is wearing a black shirt and black pants. The background"}, "355956": {"image_id": 355956, "Bleu_1": 0.2909090909038017, "Bleu_2": 0.194191754997615, "Bleu_3": 0.11247911738077473, "Bleu_4": 1.2861839592392574e-05, "METEOR": 0.2300439327212483, "ROUGE_L": 0.27006087437742116, "CIDEr": 2.591016128506508e-08, "SPICE": {"All": {"pr": 0.2, "re": 0.3076923076923077, "f": 0.24242424242424246, "fn": 9.0, "numImages": 1.0, "fp": 16.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.3333333333333333, "f": 0.2, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.6, "f": 0.4615384615384615, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a group of people playing baseball on a field. The players are wearing baseball uniforms and are standing on the field, ready to play. The sky is clear and blue, with a few clouds in the distance. The field is grassy and has a few trees in the background. There is a"}, "482970": {"image_id": 482970, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.16817499303341832, "Bleu_3": 0.10219406022809462, "Bleu_4": 0.06730826637311355, "METEOR": 0.2080054515804839, "ROUGE_L": 0.24610951008645532, "CIDEr": 1.182052320010798e-12, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.2, "f": 0.1886792452830189, "fn": 20.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.2222222222222222, "f": 0.23529411764705882, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The laptop is sitting on a desk in a room with a wooden floor and a wooden ceiling. There is a lamp on the desk and a pair of headphones on the desk next to the laptop. The laptop has a screen that is displaying a message in English. The room has a wooden door"}, "301765": {"image_id": 301765, "Bleu_1": 0.21276595744228163, "Bleu_2": 0.09618052714368557, "Bleu_3": 0.05901837724618784, "Bleu_4": 8.267558544502552e-06, "METEOR": 0.20107719928186715, "ROUGE_L": 0.20346897931954633, "CIDEr": 4.936527801514607e-09, "SPICE": {"All": {"pr": 0.2631578947368421, "re": 0.20833333333333334, "f": 0.23255813953488372, "fn": 19.0, "numImages": 1.0, "fp": 14.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.125, "f": 0.18181818181818182, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.5714285714285714, "f": 0.47058823529411764, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a street sign with the words \"prince\" and \"rue\" written on it. The sign is attached to a pole and is surrounded by trees and buildings. The sky is clear and blue.\n\nThe image shows a street sign with the words \"prince\" and \"rue\""}, "511662": {"image_id": 511662, "Bleu_1": 0.22448979591378598, "Bleu_2": 0.13677530110522793, "Bleu_3": 0.07355956748968669, "Bleu_4": 9.644726515398138e-06, "METEOR": 0.23426275967849916, "ROUGE_L": 0.2622527944969905, "CIDEr": 1.0832276788063937e-07, "SPICE": {"All": {"pr": 0.30434782608695654, "re": 0.175, "f": 0.2222222222222222, "fn": 33.0, "numImages": 1.0, "fp": 16.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 19.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.1111111111111111, "f": 0.15384615384615383, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 6.0}}, "caption": "The image shows a cruise ship in the foreground, with palm trees and a beach in the background. The ship is surrounded by colorful umbrellas and chairs on the sand. The sky is clear and blue, with a few clouds in the distance.\n\nThe image is taken from a"}, "461722": {"image_id": 461722, "Bleu_1": 0.18867924527945892, "Bleu_2": 0.14754891442234067, "Bleu_3": 0.11952367853238387, "Bleu_4": 0.10060701777161159, "METEOR": 0.23969350078860624, "ROUGE_L": 0.25341246290801184, "CIDEr": 3.173302623565623e-11, "SPICE": {"All": {"pr": 0.28, "re": 0.3888888888888889, "f": 0.32558139534883723, "fn": 11.0, "numImages": 1.0, "fp": 18.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.2857142857142857, "f": 0.2857142857142857, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.4166666666666667, "re": 0.7142857142857143, "f": 0.5263157894736842, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a young girl sitting at a table with a plate of food in front of her. She is wearing a green shirt and has long blonde hair. The background is a kitchen with a countertop and cabinets. There is a window in the background with a view of the outside."}, "406253": {"image_id": 406253, "Bleu_1": 0.3199999999936, "Bleu_2": 0.2555506259948128, "Bleu_3": 0.189478914657865, "Bleu_4": 0.14435323317269658, "METEOR": 0.32720052488247475, "ROUGE_L": 0.2866252726967612, "CIDEr": 1.7219464637217797e-08, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.23076923076923078, "f": 0.22222222222222224, "fn": 20.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.5555555555555556, "f": 0.4545454545454546, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a group of motorcycles parked on the side of a city street. The motorcycles are blue and green, and they are parked next to each other on the sidewalk. There are also cars parked on the street, and people walking down the sidewalk. The buildings in the"}, "463174": {"image_id": 463174, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.13026252642719696, "Bleu_3": 6.97519462339574e-07, "Bleu_4": 1.6222516606715129e-09, "METEOR": 0.2302231737668764, "ROUGE_L": 0.1920654911838791, "CIDEr": 8.813246266530747e-12, "SPICE": {"All": {"pr": 0.25, "re": 0.2222222222222222, "f": 0.23529411764705882, "fn": 21.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5454545454545454, "re": 0.5454545454545454, "f": 0.5454545454545454, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 6.0}}, "caption": "The image shows a woman playing tennis on a court. She is wearing a white shirt and pink leopard print leggings. She has a tennis racket in her hand and is swinging it to hit the ball. The ball is flying through the air and the woman is running to hit it"}, "287320": {"image_id": 287320, "Bleu_1": 0.36734693876801333, "Bleu_2": 0.29014422873101564, "Bleu_3": 0.22067870246906, "Bleu_4": 0.14702404168406807, "METEOR": 0.2710576386089794, "ROUGE_L": 0.3059013163786155, "CIDEr": 2.1911074145106115e-09, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.125, "f": 0.12000000000000001, "fn": 21.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.23076923076923078, "f": 0.21428571428571427, "fn": 10.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}}, "caption": "The image shows a bathroom with a toilet, sink, and mirror. The walls are painted white and the floor is made of tile. There is a wooden vanity with a sink and mirror on it. The toilet is a standard toilet with a seat and lid. The bathroom has"}, "135266": {"image_id": 135266, "Bleu_1": 0.19999999999636367, "Bleu_2": 0.06085806194390174, "Bleu_3": 4.1189525480153644e-07, "Bleu_4": 1.0766864701111474e-09, "METEOR": 0.10799000572228118, "ROUGE_L": 0.17579250720461098, "CIDEr": 6.1969050191322615e-12, "SPICE": {"All": {"pr": 0.17647058823529413, "re": 0.17647058823529413, "f": 0.17647058823529413, "fn": 28.0, "numImages": 1.0, "fp": 28.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.07142857142857142, "f": 0.09523809523809523, "fn": 13.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.45454545454545453, "f": 0.4, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows a group of people on a boat, one of them is holding a plate of food and the other is eating it. The people are all wearing life jackets and the boat is in the water.\n\nThe image is taken from a low angle, looking down at the people on the boat"}, "187901": {"image_id": 187901, "Bleu_1": 0.26785714285235973, "Bleu_2": 0.1709408646863767, "Bleu_3": 0.08148906680071043, "Bleu_4": 1.0052070847735423e-05, "METEOR": 0.2665978733261029, "ROUGE_L": 0.2476798143851508, "CIDEr": 8.559758347015355e-14, "SPICE": {"All": {"pr": 0.07692307692307693, "re": 0.08333333333333333, "f": 0.08, "fn": 22.0, "numImages": 1.0, "fp": 24.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2222222222222222, "f": 0.1904761904761905, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a tall clock tower with a large clock face on the front. The clock face has Roman numerals and hands. The tower is made of brick and has a steeple on top. There are two smaller clocks on either side of the main clock. The sky is blue and there are trees in"}, "135872": {"image_id": 135872, "Bleu_1": 0.1730769230735947, "Bleu_2": 0.10090091909748747, "Bleu_3": 5.8831069922091e-07, "Bleu_4": 1.4277627265895718e-09, "METEOR": 0.20191239286328186, "ROUGE_L": 0.22536945812807885, "CIDEr": 3.6705625384323765e-09, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 18.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "This is a computer room with a desk, chair, and computer. There is also a window with a view of the outside.\n\nThe room is painted in a light gray color and has a wooden floor. There are two bookshelves on the wall with books and other items on them. There is"}, "97610": {"image_id": 97610, "Bleu_1": 0.22916666666189242, "Bleu_2": 0.13965509692979164, "Bleu_3": 0.07512513229843845, "Bleu_4": 9.852258743685704e-06, "METEOR": 0.20038630756352882, "ROUGE_L": 0.20497311827956988, "CIDEr": 2.550133909829628e-10, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.23809523809523808, "f": 0.22727272727272724, "fn": 16.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.7142857142857143, "f": 0.588235294117647, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "The image shows a group of elephants walking along a sidewalk. They are all wearing collars and appear to be in a zoo or wildlife sanctuary. The elephants are all different sizes and colors, with some having spots and others having stripes. The background is a concrete wall"}, "568132": {"image_id": 568132, "Bleu_1": 0.19230769230399414, "Bleu_2": 0.1624659147420524, "Bleu_3": 0.13820004148932422, "Bleu_4": 0.12115660853589845, "METEOR": 0.3276020340972747, "ROUGE_L": 0.29397590361445786, "CIDEr": 1.812979240176924e-11, "SPICE": {"All": {"pr": 0.2413793103448276, "re": 0.28, "f": 0.25925925925925924, "fn": 18.0, "numImages": 1.0, "fp": 22.0, "tp": 7.0}, "Relation": {"pr": 0.125, "re": 0.2, "f": 0.15384615384615385, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2727272727272727, "re": 0.23076923076923078, "f": 0.24999999999999994, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.3, "re": 0.42857142857142855, "f": 0.3529411764705882, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a young boy in a baseball uniform, holding a baseball bat and standing on the pitcher's mound. The boy is wearing a red and white striped shirt and black pants, and has a red hat on his head. There are several other boys in the background, some of"}, "404209": {"image_id": 404209, "Bleu_1": 0.33333333332679743, "Bleu_2": 0.23094010767127685, "Bleu_3": 0.17589664289627485, "Bleu_4": 0.13580418932543062, "METEOR": 0.29060621354172617, "ROUGE_L": 0.362506753106429, "CIDEr": 2.1379842752272287e-08, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.125, "f": 0.13559322033898305, "fn": 28.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.26666666666666666, "f": 0.28571428571428575, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a cup of coffee and a slice of cake on a plate\n\nThe cup of coffee is sitting on a table with a slice of cake on the plate. The cake is a yellow sponge cake with a creamy frosting on top. The cup of coffee is a"}, "74967": {"image_id": 74967, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.1650868521781302, "Bleu_3": 0.10158210507506187, "Bleu_4": 0.06733182132555685, "METEOR": 0.1637647720370701, "ROUGE_L": 0.20962199312714777, "CIDEr": 1.3153571993077487e-12, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.12, "f": 0.13953488372093023, "fn": 22.0, "numImages": 1.0, "fp": 15.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.1111111111111111, "f": 0.15384615384615383, "fn": 8.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows a group of people in a hospital room, with one person holding a baby in a bassinet and another person holding a scalpel. The people in the room are wearing scrubs and masks. The baby is lying on a table with a blanket over it. The room has white walls and"}, "200365": {"image_id": 200365, "Bleu_1": 0.43749999999088546, "Bleu_2": 0.27288841144916204, "Bleu_3": 0.16934714128636405, "Bleu_4": 0.10192489527760024, "METEOR": 0.2504157081893134, "ROUGE_L": 0.2870138017565872, "CIDEr": 0.002280221667636794, "SPICE": {"All": {"pr": 0.21428571428571427, "re": 0.25, "f": 0.23076923076923075, "fn": 18.0, "numImages": 1.0, "fp": 22.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.6, "re": 0.6666666666666666, "f": 0.631578947368421, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 6.0}}, "caption": "The image shows a bicycle parked next to a table with a hot dog and a drink on it. The table is on the sidewalk in front of a building. There are several cars parked nearby. The sky is cloudy and there are some trees in the background."}, "357941": {"image_id": 357941, "Bleu_1": 0.27586206896076104, "Bleu_2": 0.2307304479757116, "Bleu_3": 0.18809327886505683, "Bleu_4": 0.1568309548408512, "METEOR": 0.2676895908294461, "ROUGE_L": 0.3277052954719877, "CIDEr": 6.482993964957297e-12, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.12903225806451613, "f": 0.13559322033898305, "fn": 27.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.0625, "f": 0.08333333333333333, "fn": 15.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.3, "f": 0.25, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a black and white cat sitting on top of a red blanket in front of a television. The cat has green eyes and is looking directly at the camera. The television is playing a movie with a woman's face on it. The room is dimly lit and there are several other objects in the"}, "222467": {"image_id": 222467, "Bleu_1": 0.24999999999553577, "Bleu_2": 0.17837651699995472, "Bleu_3": 0.12091150252325192, "Bleu_4": 0.09037311357156819, "METEOR": 0.24798231222146305, "ROUGE_L": 0.26940063091482647, "CIDEr": 1.0679289412791097e-11, "SPICE": {"All": {"pr": 0.2727272727272727, "re": 0.2608695652173913, "f": 0.2666666666666666, "fn": 17.0, "numImages": 1.0, "fp": 16.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.1, "f": 0.13333333333333333, "fn": 9.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5555555555555556, "f": 0.5263157894736842, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "The image shows a cat sitting on top of a television set. The cat is looking directly at the camera with its eyes wide open. The television set is in the background, with a blank screen showing. The room is dimly lit, with only a few light sources coming from the windows. The cat's fur"}, "375566": {"image_id": 375566, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.2711630722680545, "Bleu_3": 0.2274366019481347, "Bleu_4": 0.17603995399180622, "METEOR": 0.3261329819511617, "ROUGE_L": 0.29397590361445786, "CIDEr": 8.383642244747455e-11, "SPICE": {"All": {"pr": 0.24, "re": 0.15789473684210525, "f": 0.19047619047619047, "fn": 32.0, "numImages": 1.0, "fp": 19.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.6, "re": 0.375, "f": 0.4615384615384615, "fn": 10.0, "numImages": 1.0, "fp": 4.0, "tp": 6.0}}, "caption": "The image shows a woman walking down a street with a yellow umbrella over her head. She is wearing a pink shirt and pants, and has a white hat on her head. The street is lined with trees and there are cars parked on the side of the road. The sky is"}, "312204": {"image_id": 312204, "Bleu_1": 0.20408163264889634, "Bleu_2": 0.14580296087694455, "Bleu_3": 0.11070962028887511, "Bleu_4": 0.08764094705114601, "METEOR": 0.2417412704256413, "ROUGE_L": 0.26940063091482647, "CIDEr": 2.4186170102408316e-10, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.13793103448275862, "f": 0.14285714285714285, "fn": 25.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.09090909090909091, "re": 0.16666666666666666, "f": 0.11764705882352942, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Size": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.25, "f": 0.3, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a small elephant standing in a grassy field with trees in the background. The elephant is brown and has a long, curved tusk. It is standing on its hind legs and appears to be looking around. There are no other animals or people in the image."}, "417606": {"image_id": 417606, "Bleu_1": 0.22807017543459532, "Bleu_2": 0.14270041527824082, "Bleu_3": 0.07180630754382089, "Bleu_4": 9.09962522947786e-06, "METEOR": 0.1942690594296097, "ROUGE_L": 0.20178630499503802, "CIDEr": 6.097572048535367e-14, "SPICE": {"All": {"pr": 0.22727272727272727, "re": 0.20833333333333334, "f": 0.21739130434782608, "fn": 19.0, "numImages": 1.0, "fp": 17.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5555555555555556, "f": 0.5263157894736842, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "The image shows a stop sign on the side of a road in a wooded area. The sign is white with black letters and has a red background. The road is lined with trees and there are no other signs or objects in the image.\n\nThe image is taken from a distance and the camera is position"}, "508731": {"image_id": 508731, "Bleu_1": 0.2777777777700618, "Bleu_2": 0.21821789022984422, "Bleu_3": 0.14096477244685904, "Bleu_4": 0.09598524129527268, "METEOR": 0.22087061297212815, "ROUGE_L": 0.29047619047619044, "CIDEr": 3.1843721743470815e-05, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.19047619047619047, "f": 0.18604651162790697, "fn": 17.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4444444444444444, "f": 0.4444444444444444, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a group of elephants standing in a field with trees in the background. The elephants are wearing collars and appear to be in a controlled environment. The image is in black and white."}, "382411": {"image_id": 382411, "Bleu_1": 0.3199999999936, "Bleu_2": 0.18070158057739935, "Bleu_3": 0.11080794149538384, "Bleu_4": 0.07335060507704623, "METEOR": 0.28694668505601073, "ROUGE_L": 0.29647630619684084, "CIDEr": 3.872441766320896e-10, "SPICE": {"All": {"pr": 0.0967741935483871, "re": 0.15789473684210525, "f": 0.12000000000000001, "fn": 16.0, "numImages": 1.0, "fp": 28.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.5, "f": 0.3529411764705882, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a man playing tennis on a court. He is wearing a white shirt and black shorts, and has a tennis racket in his hand. The court is made of grass and there are trees in the background. The man is swinging his racket to hit the ball."}, "57107": {"image_id": 57107, "Bleu_1": 0.16326530611911708, "Bleu_2": 0.1010152544531381, "Bleu_3": 0.06010242872773111, "Bleu_4": 8.288569027871298e-06, "METEOR": 0.1868910783656874, "ROUGE_L": 0.16126900198281557, "CIDEr": 5.2737258231913944e-11, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.3333333333333333, "f": 0.2553191489361702, "fn": 12.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.4, "f": 0.3076923076923077, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5714285714285714, "f": 0.4210526315789474, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of giraffes standing in a field. They are all wearing collars and appear to be in good health. The giraffes are standing in a line, with their heads down and their necks stretched out. They are all looking in the same direction, as if"}, "59015": {"image_id": 59015, "Bleu_1": 0.23214285713871177, "Bleu_2": 0.14527180078437643, "Bleu_3": 7.311216202480129e-07, "Bleu_4": 1.6478711585270885e-09, "METEOR": 0.14765466452947218, "ROUGE_L": 0.23775055679287305, "CIDEr": 6.536819385698187e-09, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.1111111111111111, "f": 0.125, "fn": 32.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3076923076923077, "f": 0.32, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a group of people standing on the deck of a boat in the middle of a river. The boat is surrounded by trees and there are other boats in the background. The people are wearing life jackets and are standing in the water.\n\nThe image is taken from a low angle, looking down"}, "117328": {"image_id": 117328, "Bleu_1": 0.2909090909038017, "Bleu_2": 0.14679516868082104, "Bleu_3": 7.408254355300373e-07, "Bleu_4": 1.6721917553985766e-09, "METEOR": 0.18594685893421395, "ROUGE_L": 0.22724853645556145, "CIDEr": 1.6067360694293947e-11, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.29411764705882354, "f": 0.21276595744680848, "fn": 12.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.3333333333333333, "f": 0.30769230769230765, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.6666666666666666, "f": 0.5714285714285715, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 2.0}, "Object": {"pr": 0.23076923076923078, "re": 0.42857142857142855, "f": 0.3, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a dog standing in a field of grass with its paws on the ground. The dog is wearing a white shoe on its left paw and its right paw is in the air. The dog's fur is brown and white and its eyes are brown. The background is a field of"}, "28982": {"image_id": 28982, "Bleu_1": 0.23636363635933888, "Bleu_2": 0.14793756076371775, "Bleu_3": 0.10739913020247734, "Bleu_4": 0.06986340319005355, "METEOR": 0.18944405859242372, "ROUGE_L": 0.27555053642010163, "CIDEr": 3.934521021132929e-12, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.2631578947368421, "f": 0.2127659574468085, "fn": 14.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.42857142857142855, "f": 0.3157894736842105, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a red stop sign with white letters on it. The sign is mounted on a pole in the middle of a parking lot. There are cars parked in the lot, but they are not visible in the image. The sign is in good condition and appears to be well maintained.\n\nThe image"}, "200252": {"image_id": 200252, "Bleu_1": 0.31249999999348965, "Bleu_2": 0.1630820182599269, "Bleu_3": 8.330762903217117e-07, "Bleu_4": 1.893260841337042e-09, "METEOR": 0.2641058906470734, "ROUGE_L": 0.22426470588235295, "CIDEr": 3.841275098881608e-08, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.23529411764705882, "f": 0.19999999999999998, "fn": 13.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.42857142857142855, "f": 0.3157894736842105, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a bedroom with a bed, a desk, and a closet. There are two pairs of shoes on the floor next to the bed. One pair is green and the other is white. The shoes appear to be sneakers.\n\nThe image is taken in a bedroom"}, "133750": {"image_id": 133750, "Bleu_1": 0.20408163264889634, "Bleu_2": 0.18442777838702637, "Bleu_3": 0.14251861755017195, "Bleu_4": 0.08906647055194382, "METEOR": 0.18512231429222686, "ROUGE_L": 0.23303196230739842, "CIDEr": 8.334287204039491e-10, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.1111111111111111, "f": 0.12698412698412698, "fn": 32.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.14285714285714285, "f": 0.16666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 12.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a zebra standing in a pen with other zebras in the background\n\nThe zebra is standing in a pen with other zebras in the background. The pen is made of wood and has a fence around it. The zebras are all standing on the ground and"}, "172686": {"image_id": 172686, "Bleu_1": 0.22916666666189242, "Bleu_2": 0.15613914506966814, "Bleu_3": 0.08092609555849525, "Bleu_4": 1.0417495275554298e-05, "METEOR": 0.19726928737450372, "ROUGE_L": 0.23680124223602486, "CIDEr": 5.658392801819325e-10, "SPICE": {"All": {"pr": 0.25925925925925924, "re": 0.4117647058823529, "f": 0.3181818181818182, "fn": 10.0, "numImages": 1.0, "fp": 20.0, "tp": 7.0}, "Relation": {"pr": 0.1, "re": 0.16666666666666666, "f": 0.125, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.38461538461538464, "re": 0.625, "f": 0.4761904761904762, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a bowl of oranges on a wooden table. There are several oranges in the bowl, some of which are sliced and some of which are whole. The oranges are arranged in a pattern of alternating sliced and whole oranges. There is a wooden spoon in"}, "295765": {"image_id": 295765, "Bleu_1": 0.2711864406733697, "Bleu_2": 0.19340379080858328, "Bleu_3": 0.10948678456064569, "Bleu_4": 0.0695783730372136, "METEOR": 0.27739839259912524, "ROUGE_L": 0.2326975476839237, "CIDEr": 1.5817840677681294e-15, "SPICE": {"All": {"pr": 0.25925925925925924, "re": 0.2916666666666667, "f": 0.27450980392156865, "fn": 17.0, "numImages": 1.0, "fp": 20.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.14285714285714285, "f": 0.16666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.6666666666666666, "f": 0.5714285714285715, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 6.0}}, "caption": "The image shows a small plane flying in the sky with a blue and white body and red propellers. The plane is flying low to the ground and appears to be in good condition. The sky is clear and blue, with a few clouds scattered in the distance. The plane is flying towards the left side of the image"}, "154867": {"image_id": 154867, "Bleu_1": 0.24074074073628263, "Bleu_2": 0.06739642517978216, "Bleu_3": 4.4370071280182084e-07, "Bleu_4": 1.143997435182621e-09, "METEOR": 0.11614004645029605, "ROUGE_L": 0.19741100323624594, "CIDEr": 3.776226009841446e-12, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.21052631578947367, "f": 0.1951219512195122, "fn": 15.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a metal plate on top of a rocky outcropping overlooking a body of water. The plate has a small hole in the center and is surrounded by rocks and grass. The sky is blue and there are clouds in the distance.\n\nThe image is taken from a low angle, looking down"}, "233868": {"image_id": 233868, "Bleu_1": 0.32727272726677686, "Bleu_2": 0.2201927530212316, "Bleu_3": 0.09707559423882013, "Bleu_4": 1.151679688155912e-05, "METEOR": 0.2330095912304027, "ROUGE_L": 0.2987174504469491, "CIDEr": 1.0188944539373536e-11, "SPICE": {"All": {"pr": 0.12903225806451613, "re": 0.18181818181818182, "f": 0.1509433962264151, "fn": 18.0, "numImages": 1.0, "fp": 27.0, "tp": 4.0}, "Relation": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.375, "f": 0.3157894736842105, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a man standing in front of a television, holding a controller in his hand. The television is showing a video game with a boxing match on it. The man is wearing a white shirt and black pants, and has a look of concentration on his face. The room is well lit and"}, "466239": {"image_id": 466239, "Bleu_1": 0.19999999999636367, "Bleu_2": 0.13608276348545723, "Bleu_3": 7.043309782608963e-07, "Bleu_4": 1.6100218008380818e-09, "METEOR": 0.19118884427144991, "ROUGE_L": 0.24013046901360924, "CIDEr": 1.2259302591833421e-12, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.16129032258064516, "f": 0.18518518518518517, "fn": 26.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 14.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.3076923076923077, "f": 0.33333333333333337, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a train crossing a bridge over a body of water. The train is a silver and blue passenger train with a large engine at the front. The train is traveling across the bridge, which is made of steel girders and has a large concrete abutment at either end. The water below the"}, "361830": {"image_id": 361830, "Bleu_1": 0.22807017543459532, "Bleu_2": 0.1276351315918384, "Bleu_3": 0.06665907105567281, "Bleu_4": 8.605894205852283e-06, "METEOR": 0.19361844460307986, "ROUGE_L": 0.2335630674980855, "CIDEr": 7.814282992719253e-14, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.19047619047619047, "f": 0.16326530612244897, "fn": 17.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5, "f": 0.36363636363636365, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a stop sign on a road with grass growing around it. The sign is red and white with the words stop written in black letters. The background is a blue sky with some clouds. The foreground is a road with a few cars parked on it. The image is taken from a bird's"}, "102348": {"image_id": 102348, "Bleu_1": 0.34042553190765057, "Bleu_2": 0.22760466886504874, "Bleu_3": 0.16636805405423472, "Bleu_4": 0.13311273207086213, "METEOR": 0.3284834352015992, "ROUGE_L": 0.2848565710473649, "CIDEr": 1.4269610566774148e-08, "SPICE": {"All": {"pr": 0.2916666666666667, "re": 0.2413793103448276, "f": 0.26415094339622647, "fn": 22.0, "numImages": 1.0, "fp": 17.0, "tp": 7.0}, "Relation": {"pr": 0.2, "re": 0.15384615384615385, "f": 0.17391304347826086, "fn": 11.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5555555555555556, "re": 0.5, "f": 0.5263157894736842, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 5.0}}, "caption": "The image shows a fire hydrant covered in snow. The hydrant is made of orange metal and has a handle on top. There is a small tree growing out of the ground next to the hydrant. The sky is cloudy and there is snow on the ground."}, "524621": {"image_id": 524621, "Bleu_1": 0.15999999999680004, "Bleu_2": 0.11428571428340528, "Bleu_3": 0.06480087729347118, "Bleu_4": 8.722906144737809e-06, "METEOR": 0.1993379187963363, "ROUGE_L": 0.1937738246505718, "CIDEr": 5.954899945315215e-11, "SPICE": {"All": {"pr": 0.23809523809523808, "re": 0.2, "f": 0.21739130434782608, "fn": 20.0, "numImages": 1.0, "fp": 16.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.125, "f": 0.18181818181818182, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4, "f": 0.380952380952381, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows two giraffes standing in a clearing surrounded by trees. They are both looking up at the sky. The sky is blue and there are some clouds in the background. The giraffes are both brown with white spots on their backs. They are standing on their hind legs"}, "323729": {"image_id": 323729, "Bleu_1": 0.2727272727223141, "Bleu_2": 0.14213381090113214, "Bleu_3": 0.09135163170376533, "Bleu_4": 0.06187801819921283, "METEOR": 0.20366569973097493, "ROUGE_L": 0.2363032650802435, "CIDEr": 1.1580220406186897e-11, "SPICE": {"All": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 20.0, "numImages": 1.0, "fp": 15.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.14285714285714285, "f": 0.2, "fn": 6.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.5, "f": 0.4444444444444445, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "This is a living room with a couch, coffee table, and chairs. There is a window on the left side of the room and a door on the right side. The floor is made of wood and there is a rug in the center of the room.\n\nThe walls are painted white and there are"}, "505461": {"image_id": 505461, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.18314741859482847, "Bleu_3": 0.13715745469957313, "Bleu_4": 0.10029505427142453, "METEOR": 0.21930314468743783, "ROUGE_L": 0.2445589919816724, "CIDEr": 2.6293748871781336e-12, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.12, "f": 0.12499999999999997, "fn": 22.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3, "f": 0.3, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "This image shows a group of boats docked at a pier on a cloudy day. The boats are white and blue and have a wooden hull. There are several people standing on the pier, looking out at the water. The pier is made of wooden planks and has a railing around it. There are"}, "113945": {"image_id": 113945, "Bleu_1": 0.472222222209105, "Bleu_2": 0.36731544333587757, "Bleu_3": 0.2707217535971643, "Bleu_4": 0.1565899483659936, "METEOR": 0.2592243493622423, "ROUGE_L": 0.38730158730158726, "CIDEr": 0.0005069292192264103, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.26666666666666666, "f": 0.19047619047619047, "fn": 11.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.3333333333333333, "f": 0.2222222222222222, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.4, "f": 0.23529411764705882, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a red heart shaped cake with two bears on top of it. The bears are wearing wedding rings and are standing on top of the cake. The cake is on a white pedestal."}, "185598": {"image_id": 185598, "Bleu_1": 0.24999999999479172, "Bleu_2": 0.1458649914948234, "Bleu_3": 0.07733595213409626, "Bleu_4": 1.0068921364329411e-05, "METEOR": 0.21129147767180273, "ROUGE_L": 0.3057644110275689, "CIDEr": 1.825519675582552e-09, "SPICE": {"All": {"pr": 0.3, "re": 0.2608695652173913, "f": 0.27906976744186046, "fn": 17.0, "numImages": 1.0, "fp": 14.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.5, "re": 0.5555555555555556, "f": 0.5263157894736842, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "The image shows a woman surfing on a wave in the ocean. She is wearing a black wetsuit and standing on a surfboard. The wave is large and white, and the sky is blue. The woman is smiling and appears to be enjoying the ride.\n\nThe image is"}, "344364": {"image_id": 344364, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.09891091079132044, "Bleu_3": 5.730093203419742e-07, "Bleu_4": 1.3858898705876826e-09, "METEOR": 0.22228444363698416, "ROUGE_L": 0.21403508771929822, "CIDEr": 2.00985318793918e-12, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.125, "f": 0.1276595744680851, "fn": 21.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.125, "f": 0.13333333333333333, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.25, "f": 0.2105263157894737, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a man playing tennis on a court. He is wearing a white shirt and black shorts, and has a tennis racket in his hand. The court is made of blue and white tiles, and there are two lines on the court that mark the boundaries of the game. The man is"}, "331317": {"image_id": 331317, "Bleu_1": 0.20370370369993146, "Bleu_2": 0.12399138853119372, "Bleu_3": 6.66182522208817e-07, "Bleu_4": 1.5516807838487133e-09, "METEOR": 0.14889479108721024, "ROUGE_L": 0.20962199312714777, "CIDEr": 7.962365451582232e-11, "SPICE": {"All": {"pr": 0.2, "re": 0.2857142857142857, "f": 0.23529411764705882, "fn": 10.0, "numImages": 1.0, "fp": 16.0, "tp": 4.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.75, "f": 0.39999999999999997, "fn": 1.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image is of a large clock with a gold frame and a black face. The clock has a large, ornate face with Roman numerals and hands. The clock is surrounded by a large, ornate frame with a gold border. The image is taken in a large, dimly lit room with a high ce"}, "412286": {"image_id": 412286, "Bleu_1": 0.21568627450557482, "Bleu_2": 0.16087993330478273, "Bleu_3": 0.12831848461027234, "Bleu_4": 0.10719805878488435, "METEOR": 0.27335797556414165, "ROUGE_L": 0.314974182444062, "CIDEr": 7.533718557288281e-10, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.23076923076923078, "f": 0.25531914893617025, "fn": 20.0, "numImages": 1.0, "fp": 15.0, "tp": 6.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.1111111111111111, "f": 0.13333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5555555555555556, "re": 0.4166666666666667, "f": 0.4761904761904762, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 5.0}}, "caption": "The image shows a man playing tennis on a court. He is wearing a white shirt and black shorts, and has a tennis racket in his hand. The court is made of grass and there are trees in the background. The sky is blue and there are clouds in the sky."}, "77394": {"image_id": 77394, "Bleu_1": 0.24999999999479172, "Bleu_2": 0.12632278815731815, "Bleu_3": 7.026437559457351e-07, "Bleu_4": 1.6662810872720655e-09, "METEOR": 0.12172837475766748, "ROUGE_L": 0.2147887323943662, "CIDEr": 2.014508315066082e-08, "SPICE": {"All": {"pr": 0.10344827586206896, "re": 0.17647058823529413, "f": 0.13043478260869565, "fn": 14.0, "numImages": 1.0, "fp": 26.0, "tp": 3.0}, "Relation": {"pr": 0.1, "re": 0.1111111111111111, "f": 0.10526315789473685, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2857142857142857, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a living room with a couch, a table, and a chandelier hanging from the ceiling. There are also several decorations on the walls, including a wreath made of ornaments and a large Christmas tree in the corner of the room. The room is decorated in"}, "519569": {"image_id": 519569, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.14254579382569355, "Bleu_3": 0.07358334830308164, "Bleu_4": 9.448049464513672e-06, "METEOR": 0.18235223533680298, "ROUGE_L": 0.18496058217101274, "CIDEr": 6.709435875644728e-12, "SPICE": {"All": {"pr": 0.09375, "re": 0.3333333333333333, "f": 0.14634146341463417, "fn": 6.0, "numImages": 1.0, "fp": 29.0, "tp": 3.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.3333333333333333, "f": 0.13333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.13333333333333333, "re": 0.5, "f": 0.2105263157894737, "fn": 2.0, "numImages": 1.0, "fp": 13.0, "tp": 2.0}}, "caption": "This is a kitchen with white cabinets and countertops. There is a large island in the center of the room with two stools on either side. The walls are painted white and there are windows on either side of the room. The floor is made of hardwood.\n\nThe kitchen has a large window"}, "127496": {"image_id": 127496, "Bleu_1": 0.33333333332679743, "Bleu_2": 0.2449489742734669, "Bleu_3": 0.15429747117624054, "Bleu_4": 0.09353169330469448, "METEOR": 0.23561593770175526, "ROUGE_L": 0.3285457809694794, "CIDEr": 2.176037327030627e-09, "SPICE": {"All": {"pr": 0.23076923076923078, "re": 0.3157894736842105, "f": 0.26666666666666666, "fn": 13.0, "numImages": 1.0, "fp": 20.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.5714285714285714, "f": 0.47058823529411764, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a bathroom with a white toilet, a white sink, and a white bathtub. The walls are painted white and there is a window on the left side of the room. The floor is made of white tiles and there is a white shower curtain hanging from the ce"}, "164555": {"image_id": 164555, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.19154015512649578, "Bleu_3": 0.11216278231474784, "Bleu_4": 1.2897158477492775e-05, "METEOR": 0.2177097889145048, "ROUGE_L": 0.21863799283154117, "CIDEr": 5.65354841607026e-13, "SPICE": {"All": {"pr": 0.07142857142857142, "re": 0.1111111111111111, "f": 0.08695652173913043, "fn": 16.0, "numImages": 1.0, "fp": 26.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.25, "f": 0.2105263157894737, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "This image shows a group of people on a beach with a large body of water in the background. There are several baskets on the sand, and some people are flying kites in the sky.\n\nThe sky is blue and there are some clouds in the distance. The beach is sandy and there are"}, "572477": {"image_id": 572477, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.13453455879664997, "Bleu_3": 0.07126876576918603, "Bleu_4": 9.270974964697601e-06, "METEOR": 0.2281625532891913, "ROUGE_L": 0.25722891566265055, "CIDEr": 8.92165810736693e-12, "SPICE": {"All": {"pr": 0.19230769230769232, "re": 0.23809523809523808, "f": 0.21276595744680848, "fn": 16.0, "numImages": 1.0, "fp": 21.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.38461538461538464, "re": 0.5555555555555556, "f": 0.4545454545454546, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a man playing tennis on a court. He is wearing a black shirt and jeans, and has a tennis racket in his hand. The court is made of concrete and has a net in the middle. There are trees and a fence in the background.\n\nThe image is taken"}, "40037": {"image_id": 40037, "Bleu_1": 0.2931034482708086, "Bleu_2": 0.24840690229653717, "Bleu_3": 0.1876852661328498, "Bleu_4": 0.14808018927006594, "METEOR": 0.28834823936809073, "ROUGE_L": 0.2973997833152763, "CIDEr": 4.001779954759115e-14, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.15789473684210525, "f": 0.13333333333333333, "fn": 16.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.1, "re": 0.16666666666666666, "f": 0.125, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2857142857142857, "f": 0.2105263157894737, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a large building with a clock tower on top. The building is made of stone and has a green roof. There are several windows on the sides of the building and a large door on the front. The building is surrounded by a large parking lot and there are several cars parked in the lot."}, "504487": {"image_id": 504487, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.22140372138072437, "Bleu_3": 0.15769573216073673, "Bleu_4": 1.681961051814596e-05, "METEOR": 0.21537860894787597, "ROUGE_L": 0.1920654911838791, "CIDEr": 6.016289106524147e-12, "SPICE": {"All": {"pr": 0.08, "re": 0.09523809523809523, "f": 0.08695652173913043, "fn": 19.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2, "f": 0.17391304347826086, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a group of giraffes standing in a fenced in area with a tree in the background. The giraffes are standing on the ground and looking at each other. There is a small pond in the background with some plants and trees around it. The sky is cloudy and there"}, "547047": {"image_id": 547047, "Bleu_1": 0.34782608694896033, "Bleu_2": 0.23260756692519302, "Bleu_3": 0.1349817327318928, "Bleu_4": 1.5464615808906712e-05, "METEOR": 0.24094304937933209, "ROUGE_L": 0.3290577799088948, "CIDEr": 9.18396037005055e-05, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.16666666666666666, "f": 0.1702127659574468, "fn": 20.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.36363636363636365, "f": 0.380952380952381, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image shows a sign that reads, \"Attention dog owners, please pick up after your dogs.\"\n\nThe sign is on a wooden post in the middle of a dirt lot. There are several other signs nearby, including one that reads, \"No trespassing.\"\n\nThe image is in"}, "501835": {"image_id": 501835, "Bleu_1": 0.1666666666635803, "Bleu_2": 0.0971285862339108, "Bleu_3": 5.66104921326939e-07, "Bleu_4": 1.3733465907666018e-09, "METEOR": 0.15872381777308517, "ROUGE_L": 0.1821983273596177, "CIDEr": 2.7290996111841293e-12, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.2, "f": 0.1818181818181818, "fn": 16.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.125, "re": 0.125, "f": 0.125, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.375, "f": 0.33333333333333326, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a horse and carriage driving down a dirt road through a forest. The horse is wearing a black and white blanket and the carriage is decorated with flowers and ribbons. The trees in the background are tall and green, with leaves that are starting to change color. The sun is sh"}, "92248": {"image_id": 92248, "Bleu_1": 0.339622641503026, "Bleu_2": 0.16163173752903065, "Bleu_3": 0.08001308872460663, "Bleu_4": 1.006070177716116e-05, "METEOR": 0.19311322180589194, "ROUGE_L": 0.30598351845216765, "CIDEr": 1.009120407861363e-08, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.23529411764705882, "f": 0.186046511627907, "fn": 13.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.14285714285714285, "f": 0.16666666666666666, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a large building with a clock tower on top. The building has a large stone facade with columns and arches. There are two statues on either side of the entrance, one of a lion and the other of a dragon. The sky is clear and blue with a few clouds."}, "105465": {"image_id": 105465, "Bleu_1": 0.19999999999600004, "Bleu_2": 0.15649215928402857, "Bleu_3": 0.11524490328511486, "Bleu_4": 0.0898357773815822, "METEOR": 0.1928763024907392, "ROUGE_L": 0.23252858958068615, "CIDEr": 2.4666962770774603e-10, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.1111111111111111, "f": 0.13333333333333333, "fn": 24.0, "numImages": 1.0, "fp": 15.0, "tp": 3.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.07692307692307693, "f": 0.10526315789473684, "fn": 12.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.2, "f": 0.22222222222222224, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}}, "caption": "The image shows a purple and blue airplane with purple and blue seats and purple and blue windows. There are no people in the image.\n\nThe airplane has a purple and blue interior with purple and blue seats and purple and blue windows. There are no people in the image."}, "506942": {"image_id": 506942, "Bleu_1": 0.23529411764244526, "Bleu_2": 0.15339299776643633, "Bleu_3": 0.07830779525362613, "Bleu_4": 1.0001000249872884e-05, "METEOR": 0.236524163696248, "ROUGE_L": 0.22889305816135083, "CIDEr": 1.521900491998404e-11, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.13636363636363635, "f": 0.12244897959183673, "fn": 19.0, "numImages": 1.0, "fp": 24.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2857142857142857, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a group of people sitting on the beach in the sand, under an umbrella. There are several beach chairs and umbrellas on the sand, and a volleyball net in the background. The sky is blue and there are some clouds in the distance. The beach is empty except"}, "115455": {"image_id": 115455, "Bleu_1": 0.19999999999600004, "Bleu_2": 0.12777531299740644, "Bleu_3": 0.10067569617030046, "Bleu_4": 0.08117578843784566, "METEOR": 0.21533424958980463, "ROUGE_L": 0.26228501228501233, "CIDEr": 1.7490699485145209e-10, "SPICE": {"All": {"pr": 0.25, "re": 0.21428571428571427, "f": 0.23076923076923075, "fn": 22.0, "numImages": 1.0, "fp": 18.0, "tp": 6.0}, "Relation": {"pr": 0.125, "re": 0.2, "f": 0.15384615384615385, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.1, "f": 0.14285714285714288, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.3076923076923077, "f": 0.32, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows an elderly woman sitting on the ground, holding a basket in her lap. She is wearing a white shawl and a black skirt. The background is a green forest with trees and bushes.\n\nThe woman is sitting on the ground, holding a basket in her lap. She"}, "129135": {"image_id": 129135, "Bleu_1": 0.26315789473222534, "Bleu_2": 0.18136906252429263, "Bleu_3": 0.12151456238738459, "Bleu_4": 0.07592290299162799, "METEOR": 0.2538264459975469, "ROUGE_L": 0.25779186476492344, "CIDEr": 7.693676126904819e-13, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.11538461538461539, "f": 0.12244897959183673, "fn": 23.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a yellow and blue train parked at a train station. The train has a number of passengers sitting on the platform, looking at the train. There are also several people standing on the platform, looking at the train. The train has a number of windows on the sides and a large front window. The"}, "26908": {"image_id": 26908, "Bleu_1": 0.2105263157857803, "Bleu_2": 0.12262786789482262, "Bleu_3": 0.06490406357826438, "Bleu_4": 8.435396018679067e-06, "METEOR": 0.21461248600304472, "ROUGE_L": 0.23682750970604544, "CIDEr": 3.1806841305325516e-13, "SPICE": {"All": {"pr": 0.11764705882352941, "re": 0.08, "f": 0.09523809523809526, "fn": 23.0, "numImages": 1.0, "fp": 15.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.2, "f": 0.23529411764705882, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The dog is lying on the ground, chewing on a bone. The road is empty and there are no other vehicles or people in sight. The sky is clear and there are no clouds. The grass is green and there are no other plants or trees in sight. The dog is black and has a white patch"}, "367329": {"image_id": 367329, "Bleu_1": 0.19230769230399414, "Bleu_2": 0.06140634384744192, "Bleu_3": 4.224923263061475e-07, "Bleu_4": 1.1138202232666472e-09, "METEOR": 0.17046743869765474, "ROUGE_L": 0.18780788177339902, "CIDEr": 9.612532709880884e-12, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.17391304347826086, "f": 0.1702127659574468, "fn": 19.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.2, "re": 0.1111111111111111, "f": 0.14285714285714285, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows two zebras standing in a fenced in area. They are both wearing collars and appear to be healthy. The fence is made of chain link and has a gate in the center. The ground is covered in dirt and there are some rocks in the background. The sky is"}, "521601": {"image_id": 521601, "Bleu_1": 0.4374999999863282, "Bleu_2": 0.31430927853687557, "Bleu_3": 0.18744522493373353, "Bleu_4": 0.12275983234334825, "METEOR": 0.26809888325015924, "ROUGE_L": 0.34659090909090906, "CIDEr": 0.0015229882614991841, "SPICE": {"All": {"pr": 0.21739130434782608, "re": 0.23809523809523808, "f": 0.22727272727272724, "fn": 16.0, "numImages": 1.0, "fp": 18.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.5, "f": 0.4444444444444445, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image is a cup with a yellow handle and a brown lid. The cup is sitting on a table with a white tablecloth. There is a small hole in the lid."}, "344094": {"image_id": 344094, "Bleu_1": 0.2181818181778513, "Bleu_2": 0.14213381090113214, "Bleu_3": 0.07250583813278431, "Bleu_4": 9.252921909857738e-06, "METEOR": 0.2201829917829972, "ROUGE_L": 0.2363032650802435, "CIDEr": 1.9445140636321393e-12, "SPICE": {"All": {"pr": 0.03333333333333333, "re": 0.043478260869565216, "f": 0.03773584905660378, "fn": 22.0, "numImages": 1.0, "fp": 29.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.08333333333333333, "re": 0.09090909090909091, "f": 0.08695652173913043, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}}, "caption": "The image is of a plant with long, thin leaves and a tall stem. The plant is sitting in a white vase on a wooden table. There are some flowers in the vase, and a small white flower on the table. The background is a dark brown wall with a white tablecloth on top of"}, "396209": {"image_id": 396209, "Bleu_1": 0.23076923076479294, "Bleu_2": 0.11651034560483009, "Bleu_3": 6.475197095245912e-07, "Bleu_4": 1.5342308957715506e-09, "METEOR": 0.14506809033576906, "ROUGE_L": 0.1920654911838791, "CIDEr": 6.738453310121595e-12, "SPICE": {"All": {"pr": 0.1, "re": 0.09523809523809523, "f": 0.0975609756097561, "fn": 19.0, "numImages": 1.0, "fp": 18.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.25, "f": 0.23529411764705882, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a narrow cobblestone street with buildings on either side. The buildings are made of stone and have balconies with plants growing on them. There is a small table and chairs on the sidewalk. The sky is blue and there are some clouds in it. The overall mood of the"}, "529798": {"image_id": 529798, "Bleu_1": 0.17999999999640004, "Bleu_2": 0.12121830534381621, "Bleu_3": 6.739562828886045e-07, "Bleu_4": 1.5975311320250798e-09, "METEOR": 0.17386555891533662, "ROUGE_L": 0.18944099378881987, "CIDEr": 5.0532371564523546e-11, "SPICE": {"All": {"pr": 0.19047619047619047, "re": 0.13793103448275862, "f": 0.16, "fn": 25.0, "numImages": 1.0, "fp": 17.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.08333333333333333, "f": 0.10526315789473685, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.3333333333333333, "f": 0.35294117647058826, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}}, "caption": "The image shows a flock of sheep grazing on a rocky hillside. The sheep are of various colors, including white, black, and brown. There are several people standing on the hillside, watching the sheep. In the background, there is a small lake with a white flag on the shore. The"}, "62985": {"image_id": 62985, "Bleu_1": 0.20408163264889634, "Bleu_2": 0.11293848786082757, "Bleu_3": 6.47433787030964e-07, "Bleu_4": 1.558500916173644e-09, "METEOR": 0.11167667546467248, "ROUGE_L": 0.15394321766561514, "CIDEr": 7.499602925878621e-10, "SPICE": {"All": {"pr": 0.13793103448275862, "re": 0.15384615384615385, "f": 0.14545454545454548, "fn": 22.0, "numImages": 1.0, "fp": 25.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.36363636363636365, "f": 0.33333333333333337, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a person skiing down a snowy mountain slope. The person is wearing a red jacket and blue pants, and is holding a snowboard. The sky is cloudy and there are mountains in the background.\n\nThe image is in black and white, and the lighting is soft"}, "517674": {"image_id": 517674, "Bleu_1": 0.28846153845599115, "Bleu_2": 0.15041420939612582, "Bleu_3": 0.09672659665842727, "Bleu_4": 0.06555569265748387, "METEOR": 0.21552409596335761, "ROUGE_L": 0.28175519630484985, "CIDEr": 5.293290605116309e-11, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.25, "f": 0.22641509433962265, "fn": 18.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.125, "f": 0.11764705882352941, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.3, "f": 0.3, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a street with cars driving down it. There are buildings on either side of the street and a traffic light at the intersection. The sky is cloudy and there is no sun.\n\nThe image is taken at night, as evidenced by the streetlights on the side of the road."}, "575691": {"image_id": 575691, "Bleu_1": 0.2448979591786756, "Bleu_2": 0.17496355305233346, "Bleu_3": 0.10921336092038615, "Bleu_4": 0.07294866882651736, "METEOR": 0.19655076619302247, "ROUGE_L": 0.20158625247851947, "CIDEr": 7.026678582375547e-11, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.2, "f": 0.1702127659574468, "fn": 16.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.375, "f": 0.2857142857142857, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a woman standing next to a giraffe in a zoo. The woman is wearing a red and white striped shirt and a pair of sunglasses. The giraffe is standing on its hind legs and reaching up to the woman's face with its long neck. The"}, "267690": {"image_id": 267690, "Bleu_1": 0.25641025640368187, "Bleu_2": 0.11616917255653597, "Bleu_3": 0.07144853954819219, "Bleu_4": 1.0032736169840087e-05, "METEOR": 0.16757449344311415, "ROUGE_L": 0.2601279317697228, "CIDEr": 3.896241183316504e-06, "SPICE": {"All": {"pr": 0.08, "re": 0.09523809523809523, "f": 0.08695652173913043, "fn": 19.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.18181818181818182, "f": 0.1904761904761905, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a group of boys in suits and hats standing on a sidewalk in front of a building. They are all smiling and looking at the camera. The building appears to be a train station or depot."}, "119641": {"image_id": 119641, "Bleu_1": 0.31818181817458685, "Bleu_2": 0.25806238489967687, "Bleu_3": 0.18510580472301696, "Bleu_4": 0.11152418657443797, "METEOR": 0.2561947720681639, "ROUGE_L": 0.29985955056179775, "CIDEr": 9.531540585771544e-07, "SPICE": {"All": {"pr": 0.13043478260869565, "re": 0.14285714285714285, "f": 0.13636363636363635, "fn": 18.0, "numImages": 1.0, "fp": 20.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.2, "f": 0.28571428571428575, "fn": 4.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.2222222222222222, "re": 0.25, "f": 0.23529411764705882, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}}, "caption": "The image shows a group of people riding on the backs of elephants through a river. The elephants are wearing saddles and the people are wearing hats and sunglasses. The river is surrounded by lush green trees and there are mountains in the background."}}}