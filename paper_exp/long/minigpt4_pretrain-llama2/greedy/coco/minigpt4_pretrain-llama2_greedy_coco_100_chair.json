{"overall": {"Bleu_1": 0.22656617993401998, "Bleu_2": 0.15077305298856739, "Bleu_3": 0.09704238377880191, "Bleu_4": 0.06320745061910013, "METEOR": 0.20666894994097093, "ROUGE_L": 0.22858155722544385, "CIDEr": 0.00020553561476600244, "SPICE": 0.16607814015596262}, "imgToEval": {"472621": {"image_id": 472621, "Bleu_1": 0.255813953482423, "Bleu_2": 0.13517553494737666, "Bleu_3": 0.07638430164587322, "Bleu_4": 1.0273965107862501e-05, "METEOR": 0.19106786480893023, "ROUGE_L": 0.21801286633309508, "CIDEr": 2.491211196342638e-07, "SPICE": {"All": {"pr": 0.2631578947368421, "re": 0.29411764705882354, "f": 0.27777777777777773, "fn": 12.0, "numImages": 1.0, "fp": 14.0, "tp": 5.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.5714285714285714, "f": 0.5, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "This is a bathroom with a white bathtub, a white sink, and a white toilet. The walls are painted yellow and there is a wooden floor. There is a mirror on the wall above the sink and a window on the opposite wall."}, "196053": {"image_id": 196053, "Bleu_1": 0.23255813952947546, "Bleu_2": 0.1288848155535838, "Bleu_3": 7.399571153427093e-07, "Bleu_4": 1.7839797285693462e-09, "METEOR": 0.24169870837006527, "ROUGE_L": 0.25003415767181314, "CIDEr": 2.483292729000642e-07, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.23809523809523808, "f": 0.22222222222222224, "fn": 16.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.5555555555555556, "f": 0.43478260869565216, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows a woman playing tennis on a court. She is wearing a white shirt and blue shorts, and has a yellow racket in her hand. The net is in the background, and there are other players on the court as well."}, "42276": {"image_id": 42276, "Bleu_1": 0.36585365852766216, "Bleu_2": 0.25303041366748835, "Bleu_3": 0.18726083047926023, "Bleu_4": 0.13634738359895024, "METEOR": 0.2828646156981469, "ROUGE_L": 0.4013157894736842, "CIDEr": 2.023924425152811e-05, "SPICE": {"All": {"pr": 0.1388888888888889, "re": 0.20833333333333334, "f": 0.16666666666666669, "fn": 19.0, "numImages": 1.0, "fp": 31.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.29411764705882354, "re": 0.625, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 12.0, "tp": 5.0}}, "caption": "The image is a bathroom with a toilet, sink, and shower. There are several pairs of shoes on the floor, including skis, snowboards, and boots. The walls are painted white and there is a window on one side of the room."}, "530226": {"image_id": 530226, "Bleu_1": 0.23749999999703125, "Bleu_2": 0.14506654954516193, "Bleu_3": 0.09319381094551911, "Bleu_4": 0.05694002092095135, "METEOR": 0.262204372998383, "ROUGE_L": 0.27188006482982174, "CIDEr": 2.5726571851168952e-28, "SPICE": {"All": {"pr": 0.25, "re": 0.20833333333333334, "f": 0.22727272727272727, "fn": 19.0, "numImages": 1.0, "fp": 15.0, "tp": 5.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4, "f": 0.380952380952381, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a polar bear standing on the edge of a frozen lake, looking out at the water. The bear is covered in snow and ice, and its fur is white and fluffy. The reflection of the bear's face can be seen in the water, and there are some small ice floes floating in the lake. The sky is clear and blue, with some clouds scattered across it. The overall mood of the image is peaceful and serene."}, "487421": {"image_id": 487421, "Bleu_1": 0.1917808219151811, "Bleu_2": 0.1154041274513772, "Bleu_3": 0.057243754641339774, "Bleu_4": 7.194847176814373e-06, "METEOR": 0.1729021915223996, "ROUGE_L": 0.17321344060577376, "CIDEr": 9.592080970887062e-21, "SPICE": {"All": {"pr": 0.12903225806451613, "re": 0.25, "f": 0.1702127659574468, "fn": 12.0, "numImages": 1.0, "fp": 27.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.2, "re": 0.6, "f": 0.3, "fn": 2.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}}, "caption": "The image shows a bowl filled with a delicious vegetarian dish. The bowl is made of white ceramic and has a small handle on one side. Inside the bowl, there are strands of pasta mixed with vegetables such as broccoli, carrots, and cherry tomatoes. The pasta is topped with a creamy sauce made from cashews and has a sprinkle of red pepper flakes on top. The overall presentation is visually appealing and appetizing."}, "536990": {"image_id": 536990, "Bleu_1": 0.37499999999062505, "Bleu_2": 0.27735009810559197, "Bleu_3": 0.15938019318635085, "Bleu_4": 1.8187597339048353e-05, "METEOR": 0.2456178830129746, "ROUGE_L": 0.34078212290502796, "CIDEr": 5.7946215055434544e-05, "SPICE": {"All": {"pr": 0.2222222222222222, "re": 0.4, "f": 0.2857142857142857, "fn": 9.0, "numImages": 1.0, "fp": 21.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.75, "f": 0.6, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 6.0}}, "caption": "The image is of a vase with flowers in it sitting on a shelf next to a window. The wall behind the vase has a painting hanging on it. There are also some books and other objects on the shelf."}, "101762": {"image_id": 101762, "Bleu_1": 0.09999999999909093, "Bleu_2": 0.06772854614724111, "Bleu_3": 0.03489045103070811, "Bleu_4": 4.46358688955196e-06, "METEOR": 0.14317887115968653, "ROUGE_L": 0.11823232975836671, "CIDEr": 6.326770274440874e-60, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.18518518518518517, "f": 0.18181818181818182, "fn": 22.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.16666666666666666, "f": 0.2105263157894737, "fn": 10.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.375, "f": 0.3157894736842105, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a small black and white cat sitting next to a bicycle with red wheels. The cat is looking up at the bicycle with interest, as if it is trying to figure out what it is. The bicycle appears to be in good condition, with no visible damage or wear on the tires or frame. The cat's fur is smooth and shiny, and it looks like it has been groomed recently. The room appears to be a bedroom, with a bed in the background and a window open to let in natural light. There are no other objects in the room, just the bicycle and the cat."}, "100329": {"image_id": 100329, "Bleu_1": 0.1486486486466399, "Bleu_2": 0.11053369236966407, "Bleu_3": 0.07984714116161684, "Bleu_4": 0.05174637617240157, "METEOR": 0.19510117093410254, "ROUGE_L": 0.1881057268722467, "CIDEr": 1.3873853409555238e-24, "SPICE": {"All": {"pr": 0.13513513513513514, "re": 0.16666666666666666, "f": 0.14925373134328357, "fn": 25.0, "numImages": 1.0, "fp": 32.0, "tp": 5.0}, "Relation": {"pr": 0.07692307692307693, "re": 0.14285714285714285, "f": 0.1, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.08333333333333333, "re": 0.07142857142857142, "f": 0.07692307692307691, "fn": 13.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3333333333333333, "f": 0.28571428571428575, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a brown bear standing on a rocky outcropping, looking down at the water below. The bear is standing on its hind legs and has a large paw on the ground. The water in the background is calm and clear, with some small waves breaking on the surface. There are some trees in the background, with their leaves rustling in the wind. The overall mood of the image is peaceful and serene."}, "105234": {"image_id": 105234, "Bleu_1": 0.1969696969667126, "Bleu_2": 0.16514456476643266, "Bleu_3": 0.12867856229804236, "Bleu_4": 0.10036327540492503, "METEOR": 0.18739169698963018, "ROUGE_L": 0.21243781094527364, "CIDEr": 3.082381171557402e-20, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.09375, "f": 0.11320754716981132, "fn": 29.0, "numImages": 1.0, "fp": 18.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.25, "f": 0.25, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "This image shows a bride and groom standing in front of a large window with a view of the outdoors. The bride is holding a bouquet of flowers and the groom is holding a glass of champagne. There are several other people in the room, including guests and a wedding party. The walls are painted white and there are several tables set up for the reception."}, "577355": {"image_id": 577355, "Bleu_1": 0.23529411764359862, "Bleu_2": 0.118521770187436, "Bleu_3": 0.05970592636432839, "Bleu_4": 7.564578631392497e-06, "METEOR": 0.1761563051638729, "ROUGE_L": 0.14805825242718448, "CIDEr": 2.375938682842101e-21, "SPICE": {"All": {"pr": 0.1, "re": 0.18181818181818182, "f": 0.12903225806451613, "fn": 18.0, "numImages": 1.0, "fp": 36.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.15384615384615385, "re": 0.3333333333333333, "f": 0.21052631578947367, "fn": 4.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2222222222222222, "f": 0.18181818181818185, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image is of a street with traffic lights at an intersection. There are two traffic lights on the left side of the road, one red and one green. The sky in the background is cloudy and has a pinkish hue. There are buildings on either side of the road, some of which have windows and balconies. There are also cars parked along the side of the road."}, "526645": {"image_id": 526645, "Bleu_1": 0.15662650602220934, "Bleu_2": 0.11563111774940502, "Bleu_3": 0.07911555781146852, "Bleu_4": 8.870006214300136e-06, "METEOR": 0.23212063974296765, "ROUGE_L": 0.20041067761806985, "CIDEr": 1.5882539400315467e-32, "SPICE": {"All": {"pr": 0.075, "re": 0.2, "f": 0.10909090909090907, "fn": 12.0, "numImages": 1.0, "fp": 37.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.06666666666666667, "re": 0.2, "f": 0.1, "fn": 4.0, "numImages": 1.0, "fp": 14.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.13333333333333333, "re": 0.3333333333333333, "f": 0.19047619047619044, "fn": 4.0, "numImages": 1.0, "fp": 13.0, "tp": 2.0}}, "caption": "The image shows a giraffe standing in the middle of a forest. The giraffe is brown and has spots on its back. It is walking towards the camera, with its long neck stretched out in front of it. The trees in the background are tall and green, with branches reaching up towards the sky. There is a path leading through the trees, and the giraffe is walking along it. The sun is shining down on the scene, casting dappled shadows on the ground."}, "476939": {"image_id": 476939, "Bleu_1": 0.24358974358662064, "Bleu_2": 0.19483849199031042, "Bleu_3": 0.13567567067113057, "Bleu_4": 0.07596458453556026, "METEOR": 0.19662605420884585, "ROUGE_L": 0.20908311910882607, "CIDEr": 1.19194619514292e-27, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.17391304347826086, "f": 0.1568627450980392, "fn": 19.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.4444444444444444, "f": 0.34782608695652173, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a hospital room with several medical equipment and supplies. There are two doctors in scrubs standing next to a patient on a gurney, looking at a monitor. The patient is lying on the gurney with an oxygen mask over their face. There are several other medical supplies and equipment in the room, including a stretcher, a sink, and a cabinet. The walls are painted white and there are windows on one side of the room."}, "546444": {"image_id": 546444, "Bleu_1": 0.23333333332944448, "Bleu_2": 0.08893595740479847, "Bleu_3": 5.147253906747477e-07, "Bleu_4": 1.2436923987937538e-09, "METEOR": 0.1499944831311116, "ROUGE_L": 0.17058165548098433, "CIDEr": 9.837284541660344e-16, "SPICE": {"All": {"pr": 0.10526315789473684, "re": 0.21052631578947367, "f": 0.14035087719298245, "fn": 15.0, "numImages": 1.0, "fp": 34.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.21052631578947367, "re": 0.5, "f": 0.2962962962962963, "fn": 4.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}}, "caption": "The image shows a group of people sitting on the floor in front of a television. They are all wearing casual clothing and appear to be watching a show or movie on the TV. The room is dimly lit and has a fireplace in the corner. There are no other objects in the room, just the people and the TV."}, "285734": {"image_id": 285734, "Bleu_1": 0.11111111110973938, "Bleu_2": 0.0833333333322981, "Bleu_3": 0.05602047721643111, "Bleu_4": 6.890273363797702e-06, "METEOR": 0.1752659153930152, "ROUGE_L": 0.15823605706874186, "CIDEr": 4.675947797583229e-32, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.24, "f": 0.20689655172413793, "fn": 19.0, "numImages": 1.0, "fp": 27.0, "tp": 6.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.07692307692307693, "re": 0.14285714285714285, "f": 0.1, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.36363636363636365, "f": 0.32, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a young boy holding a kite in his hand, standing in the middle of a grassy field. The kite is made of white and blue paper with a red tail and has a smiley face on it. The boy is wearing a red t-shirt and blue shorts, and his hair is messy and unkempt. In the background, there are trees and buildings visible through the fence. The sky is clear and sunny, with a few clouds scattered about."}, "191013": {"image_id": 191013, "Bleu_1": 0.26315789473337947, "Bleu_2": 0.19645989724889837, "Bleu_3": 0.12777876871057986, "Bleu_4": 0.08695024460222954, "METEOR": 0.22123647504191918, "ROUGE_L": 0.228369384359401, "CIDEr": 1.5746125518612654e-24, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.23529411764705882, "f": 0.17777777777777778, "fn": 13.0, "numImages": 1.0, "fp": 24.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.6666666666666666, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image is a large billboard advertising a coffee shop. It has a red and white striped awning over the top of it, with a clock on the side. There are several cars parked in front of the billboard, and people walking down the street. The sky is blue and cloudy.\n\nThe billboard reads, \"Coffee Shop Open 24 Hours\" in white letters. The clock on the side of the billboard shows the time as 10:30 am."}, "50350": {"image_id": 50350, "Bleu_1": 0.2941176470544983, "Bleu_2": 0.2479060684747145, "Bleu_3": 0.1867996852515097, "Bleu_4": 0.14152030756991021, "METEOR": 0.2525118508671572, "ROUGE_L": 0.24049563503238522, "CIDEr": 3.056464755130669e-20, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.16, "f": 0.14545454545454545, "fn": 21.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4, "f": 0.34782608695652173, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a bathroom with a white bathtub in the center of the room. The walls are made of wood and there is a window on one side of the room that lets in natural light. There is a sink on the counter next to the bathtub and a toilet in the corner of the room. The floor is covered in a blue and white striped rug."}, "412676": {"image_id": 412676, "Bleu_1": 0.21212121211799817, "Bleu_2": 0.1142524093978523, "Bleu_3": 5.886406548165311e-07, "Bleu_4": 1.341382242741507e-09, "METEOR": 0.17687293161961112, "ROUGE_L": 0.22655524605385333, "CIDEr": 4.4434158870350495e-18, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.2, "f": 0.14285714285714285, "fn": 16.0, "numImages": 1.0, "fp": 32.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 4.0, "numImages": 1.0, "fp": 12.0, "tp": 4.0}}, "caption": "The image shows a large, modern building with several floors and a large sign on the front that reads \"GPM\". The building appears to be made of brick and has large windows on each floor. There are several cars parked in front of the building, and a pedestrian crossing is visible in the foreground. The sky is cloudy and there are some trees in the background."}, "151742": {"image_id": 151742, "Bleu_1": 0.2641509433912425, "Bleu_2": 0.20159019488753482, "Bleu_3": 0.1684638136346001, "Bleu_4": 0.1398467788935822, "METEOR": 0.26705450845052203, "ROUGE_L": 0.3190005810575247, "CIDEr": 7.04663611453347e-11, "SPICE": {"All": {"pr": 0.2, "re": 0.2857142857142857, "f": 0.23529411764705882, "fn": 15.0, "numImages": 1.0, "fp": 24.0, "tp": 6.0}, "Relation": {"pr": 0.125, "re": 0.125, "f": 0.125, "fn": 7.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1, "re": 0.2, "f": 0.13333333333333333, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 4.0}}, "caption": "The image shows a man in a black suit and tie holding a baby in his arms. The man is standing in front of a large church with stained glass windows and pews. There are other people in the background of the image, but they are not visible due to the bright lighting."}, "433924": {"image_id": 433924, "Bleu_1": 0.24999999999652778, "Bleu_2": 0.16783627165699036, "Bleu_3": 0.1341554393108527, "Bleu_4": 0.11501022225669484, "METEOR": 0.24895083563817497, "ROUGE_L": 0.2772727272727273, "CIDEr": 1.7848573217287511e-22, "SPICE": {"All": {"pr": 0.10526315789473684, "re": 0.13793103448275862, "f": 0.11940298507462686, "fn": 25.0, "numImages": 1.0, "fp": 34.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.23529411764705882, "re": 0.3333333333333333, "f": 0.27586206896551724, "fn": 8.0, "numImages": 1.0, "fp": 13.0, "tp": 4.0}}, "caption": "The image shows a woman holding a pair of scissors in her hand. She is wearing a white shirt and black pants, and has long brown hair. The background is a dark color, possibly black or navy blue. The woman's face is not visible, as she is looking down at the scissors in her hand. The scissors are open and appear to be sharp, with a small blade on each side."}, "160104": {"image_id": 160104, "Bleu_1": 0.2777777777726338, "Bleu_2": 0.21718612137747484, "Bleu_3": 0.1219636080704527, "Bleu_4": 1.3733465907666018e-05, "METEOR": 0.222039502101296, "ROUGE_L": 0.2684268426842684, "CIDEr": 1.3916503238383814e-11, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.16, "f": 0.1568627450980392, "fn": 21.0, "numImages": 1.0, "fp": 22.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4, "f": 0.380952380952381, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a table with two plates and two cups on it. One plate has a bowl of soup in it, while the other has a piece of toast. There is also a glass of orange juice on the table. The background is a wooden floor with a rug on top of it."}, "174198": {"image_id": 174198, "Bleu_1": 0.24590163934023115, "Bleu_2": 0.16937687147073976, "Bleu_3": 0.09907454029581565, "Bleu_4": 1.1379273899381188e-05, "METEOR": 0.25463129398752365, "ROUGE_L": 0.22652519893899206, "CIDEr": 9.275453139041563e-17, "SPICE": {"All": {"pr": 0.12903225806451613, "re": 0.14814814814814814, "f": 0.13793103448275862, "fn": 23.0, "numImages": 1.0, "fp": 27.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.36363636363636365, "f": 0.32, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows a train station with a green and white train on the tracks. The train is moving slowly through the station, and there are people standing on the platform looking at it. There are also other trains in the background, parked on the tracks. The station appears to be well maintained and clean, with white walls and green trim."}, "313341": {"image_id": 313341, "Bleu_1": 0.33333333332679743, "Bleu_2": 0.23094010767127685, "Bleu_3": 0.1483569743356351, "Bleu_4": 0.09081773498656424, "METEOR": 0.24981508573881242, "ROUGE_L": 0.25831820931639443, "CIDEr": 1.396959773781212e-10, "SPICE": {"All": {"pr": 0.2608695652173913, "re": 0.3, "f": 0.27906976744186046, "fn": 14.0, "numImages": 1.0, "fp": 17.0, "tp": 6.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.25, "f": 0.18181818181818182, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2857142857142857, "re": 0.2222222222222222, "f": 0.25, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.42857142857142855, "f": 0.375, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a group of people sitting at a table with laptops in front of them. They are all wearing black shirts and red hair. There is a white tablecloth on the table and a vase of flowers on the side. The walls are painted with orange and black stripes."}, "92355": {"image_id": 92355, "Bleu_1": 0.2608695652117203, "Bleu_2": 0.2014440620697061, "Bleu_3": 0.14038665933156294, "Bleu_4": 0.08956271819971037, "METEOR": 0.18846100921079673, "ROUGE_L": 0.2526627218934911, "CIDEr": 3.871869079995214e-07, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.25925925925925924, "f": 0.20289855072463767, "fn": 20.0, "numImages": 1.0, "fp": 35.0, "tp": 7.0}, "Relation": {"pr": 0.07692307692307693, "re": 0.07692307692307693, "f": 0.07692307692307693, "fn": 12.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.35294117647058826, "re": 0.6, "f": 0.4444444444444445, "fn": 4.0, "numImages": 1.0, "fp": 11.0, "tp": 6.0}}, "caption": "The image shows a kitchen with a stainless steel refrigerator, a stove, and a sink. There are several pots and pans on the countertops, as well as a coffee maker and a kettle. The walls are painted white and there are some rocks on the countertops."}, "196681": {"image_id": 196681, "Bleu_1": 0.31147540983095945, "Bleu_2": 0.22784329769447842, "Bleu_3": 0.1521108401517212, "Bleu_4": 0.10495922718422696, "METEOR": 0.22355550706102179, "ROUGE_L": 0.2214500570480241, "CIDEr": 9.881914252347182e-16, "SPICE": {"All": {"pr": 0.10204081632653061, "re": 0.21739130434782608, "f": 0.1388888888888889, "fn": 18.0, "numImages": 1.0, "fp": 44.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 15.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Attribute": {"pr": 0.0625, "re": 0.25, "f": 0.1, "fn": 3.0, "numImages": 1.0, "fp": 15.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.2222222222222222, "re": 0.4, "f": 0.2857142857142857, "fn": 6.0, "numImages": 1.0, "fp": 14.0, "tp": 4.0}}, "caption": "The image is of a small plane flying over a mountain range with trees and mountains in the background. The sky is clear and blue, with a few clouds scattered throughout. The plane is white with a red tail and wings, and it appears to be flying low over the mountains. There are no people or buildings visible in the image."}, "53457": {"image_id": 53457, "Bleu_1": 0.3448275862009513, "Bleu_2": 0.25796448307633496, "Bleu_3": 0.2026173424743597, "Bleu_4": 0.16582854486666088, "METEOR": 0.3118181299448422, "ROUGE_L": 0.33701657458563533, "CIDEr": 2.1719789295047355e-12, "SPICE": {"All": {"pr": 0.12, "re": 0.13636363636363635, "f": 0.1276595744680851, "fn": 19.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.375, "f": 0.33333333333333326, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a cat sitting on top of a television set. The cat is looking directly at the camera with its eyes wide open and its tail hanging down. The television set has a flat screen and is mounted on the wall. There are no other objects in the room, just the cat and the television set."}, "245301": {"image_id": 245301, "Bleu_1": 0.2833333333286111, "Bleu_2": 0.18334617314770857, "Bleu_3": 0.12024831150317533, "Bleu_4": 0.08837878467024232, "METEOR": 0.20942493394246434, "ROUGE_L": 0.17058165548098433, "CIDEr": 7.033018201797428e-16, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.2777777777777778, "f": 0.2380952380952381, "fn": 13.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.2, "f": 0.15384615384615385, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.36363636363636365, "re": 0.5714285714285714, "f": 0.4444444444444444, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a woman riding a horse on a grassy field. The woman is wearing a red coat and white pants, and she is holding the reins of the horse with her left hand. The horse is white with black spots and is wearing a saddle and bridle. The background is a green field with trees in the distance."}, "297574": {"image_id": 297574, "Bleu_1": 0.2888888888824692, "Bleu_2": 0.19847906537508855, "Bleu_3": 0.14007503420163694, "Bleu_4": 0.08994110735166029, "METEOR": 0.21416979234678926, "ROUGE_L": 0.25258799171842644, "CIDEr": 6.522755771094433e-08, "SPICE": {"All": {"pr": 0.2631578947368421, "re": 0.15625, "f": 0.19607843137254902, "fn": 27.0, "numImages": 1.0, "fp": 14.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.1111111111111111, "f": 0.16666666666666666, "fn": 8.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4, "re": 0.3333333333333333, "f": 0.3636363636363636, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 4.0}}, "caption": "The image is a plate with three slices of pizza on it. Each slice has cheese and toppings on it, such as pepperoni and mushrooms. The plate is sitting on a wooden surface, and there are utensils, such as forks and knives, next to it."}, "392048": {"image_id": 392048, "Bleu_1": 0.24615384615005917, "Bleu_2": 0.13867504905415717, "Bleu_3": 0.08483245476200657, "Bleu_4": 0.05601748457752527, "METEOR": 0.16354260868186518, "ROUGE_L": 0.20758385999027706, "CIDEr": 1.7223380526035802e-18, "SPICE": {"All": {"pr": 0.12, "re": 0.15, "f": 0.1333333333333333, "fn": 17.0, "numImages": 1.0, "fp": 22.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.42857142857142855, "f": 0.3529411764705882, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a plate with a stack of pancakes, a cup of coffee, and a baby sitting on the bed. The baby is wearing a onesie and has a pacifier in its mouth. The mother is sitting next to the baby, holding a cup of coffee and looking at the baby. The room is dimly lit and there are some toys on the floor."}, "434511": {"image_id": 434511, "Bleu_1": 0.1739130434757404, "Bleu_2": 0.15171652122503715, "Bleu_3": 0.11976235238743241, "Bleu_4": 0.09400145334112084, "METEOR": 0.23974533850607957, "ROUGE_L": 0.23405275779376497, "CIDEr": 4.633562541678471e-22, "SPICE": {"All": {"pr": 0.08108108108108109, "re": 0.12, "f": 0.0967741935483871, "fn": 22.0, "numImages": 1.0, "fp": 34.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2727272727272727, "f": 0.20689655172413793, "fn": 8.0, "numImages": 1.0, "fp": 15.0, "tp": 3.0}}, "caption": "The image shows a man sitting at a table in a coffee shop. He is wearing a blue shirt and jeans, and has a laptop open in front of him. There are several cups of coffee on the table, as well as a cup of tea. The walls of the coffee shop are painted a light green color, and there are several chairs and tables scattered around the room."}, "454348": {"image_id": 454348, "Bleu_1": 0.19672131147218494, "Bleu_2": 0.09917694073445357, "Bleu_3": 5.503721732456255e-07, "Bleu_4": 1.302073057386813e-09, "METEOR": 0.13613646664947035, "ROUGE_L": 0.16495402920497565, "CIDEr": 6.354941835429857e-17, "SPICE": {"All": {"pr": 0.21212121212121213, "re": 0.22580645161290322, "f": 0.21875, "fn": 24.0, "numImages": 1.0, "fp": 26.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3888888888888889, "re": 0.5384615384615384, "f": 0.45161290322580644, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 7.0}}, "caption": "The image shows a wooden bench sitting in the middle of a grassy field. The bench is made of old, weathered wood and has a few cracks and splinters on it. There are some trees nearby, with their branches stretching out towards the sky. The ground is covered in green grass and there are no other objects or people in sight."}, "554459": {"image_id": 554459, "Bleu_1": 0.1739130434757404, "Bleu_2": 0.08759357436978878, "Bleu_3": 0.048561254579027095, "Bleu_4": 6.454036835718624e-06, "METEOR": 0.12761093602688428, "ROUGE_L": 0.14628297362110312, "CIDEr": 3.6815545515050773e-20, "SPICE": {"All": {"pr": 0.05714285714285714, "re": 0.08333333333333333, "f": 0.06779661016949153, "fn": 22.0, "numImages": 1.0, "fp": 33.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.14285714285714285, "re": 0.2222222222222222, "f": 0.17391304347826086, "fn": 7.0, "numImages": 1.0, "fp": 12.0, "tp": 2.0}}, "caption": "The image shows a group of people standing in front of an airplane. The plane is white with red and blue stripes on the wings. There are two people standing in front of the plane, one holding a guitar and the other holding a suitcase. There are two people standing behind the plane, one holding a camera and the other holding a backpack. The sky is clear and blue."}, "455981": {"image_id": 455981, "Bleu_1": 0.2656249999958496, "Bleu_2": 0.11246692635353345, "Bleu_3": 0.058868909064751736, "Bleu_4": 7.60469905289444e-06, "METEOR": 0.19819469908855708, "ROUGE_L": 0.1840305711987128, "CIDEr": 3.610676689129231e-18, "SPICE": {"All": {"pr": 0.2, "re": 0.3181818181818182, "f": 0.2456140350877193, "fn": 15.0, "numImages": 1.0, "fp": 28.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.4, "f": 0.3076923076923077, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3125, "re": 0.625, "f": 0.4166666666666667, "fn": 3.0, "numImages": 1.0, "fp": 11.0, "tp": 5.0}}, "caption": "The image shows a bathroom with a toilet, sink, and shower. The toilet is white and has a seat and lid. The sink is made of white porcelain and has two faucets. The shower is tiled in white and has a curtain that is open. There is a mirror on the wall above the sink. The floor is made of black and white tiles."}, "498363": {"image_id": 498363, "Bleu_1": 0.08181818181743802, "Bleu_2": 0.06126277504545533, "Bleu_3": 0.047064878654255196, "Bleu_4": 0.037362356666909825, "METEOR": 0.11940436712346789, "ROUGE_L": 0.13729903536977492, "CIDEr": 7.980726781610743e-62, "SPICE": {"All": {"pr": 0.20833333333333334, "re": 0.18518518518518517, "f": 0.19607843137254902, "fn": 22.0, "numImages": 1.0, "fp": 19.0, "tp": 5.0}, "Relation": {"pr": 0.2, "re": 0.1, "f": 0.13333333333333333, "fn": 9.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.09090909090909091, "f": 0.11764705882352942, "fn": 10.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.5, "f": 0.3157894736842105, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image is a black and white photograph of a person sitting on a couch with their feet up on the table. There are several items on the table, including a pair of scissors, a pencil, and a notebook. The person is wearing a black shirt and jeans, and has a cigarette in their hand. The background is a dark brown color.\n\nThe image appears to be taken in a living room or study area, as there are books and papers scattered around the table. The person seems to be relaxing and enjoying themselves, as they are smoking a cigarette. The overall mood of the image is calm and peaceful."}, "122572": {"image_id": 122572, "Bleu_1": 0.2153846153813018, "Bleu_2": 0.058011935110632705, "Bleu_3": 3.7661538030416923e-07, "Bleu_4": 9.634420923396615e-10, "METEOR": 0.14049133957607984, "ROUGE_L": 0.15365239294710328, "CIDEr": 1.9901073426205417e-19, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.2, "f": 0.16, "fn": 16.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5714285714285714, "f": 0.38095238095238093, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows two white sheep walking down a dirt road in the middle of a forest. They are both wearing collars and appear to be on a leash. The trees in the background are tall and green, with branches stretching up towards the sky. The sky is clear and blue, with no clouds visible. The ground is covered in pine needles and small rocks."}, "516143": {"image_id": 516143, "Bleu_1": 0.18518518518175586, "Bleu_2": 0.1447907475849832, "Bleu_3": 0.10654502541910188, "Bleu_4": 0.0829879120277201, "METEOR": 0.2385291534799211, "ROUGE_L": 0.25507765830346474, "CIDEr": 3.537838234806503e-13, "SPICE": {"All": {"pr": 0.1, "re": 0.18181818181818182, "f": 0.12903225806451613, "fn": 18.0, "numImages": 1.0, "fp": 36.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 16.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.26666666666666666, "re": 0.6666666666666666, "f": 0.3809523809523809, "fn": 2.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "The image shows a bus stop on the side of a road with a green and white bus parked at it. There are several people standing around the bus, looking at it. In the background, there is a building with a sign that reads \"Welcome to the Beach\". The sky is clear and blue."}, "478621": {"image_id": 478621, "Bleu_1": 0.26984126983698664, "Bleu_2": 0.19791540342072636, "Bleu_3": 0.1475252820820806, "Bleu_4": 0.10171137527770421, "METEOR": 0.21112830162824933, "ROUGE_L": 0.2663755458515284, "CIDEr": 1.3186132818874097e-14, "SPICE": {"All": {"pr": 0.15789473684210525, "re": 0.10714285714285714, "f": 0.1276595744680851, "fn": 25.0, "numImages": 1.0, "fp": 16.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.07142857142857142, "f": 0.1, "fn": 13.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.3333333333333333, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.2857142857142857, "re": 0.2857142857142857, "f": 0.2857142857142857, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The image shows a man in a blue umbrella walking down the sidewalk in front of a row of houses. The houses are painted in various colors, including white, beige, and gray. There are trees and bushes on either side of the sidewalk, and a streetlight is visible in the distance. The sky is cloudy and there is a light drizzle of rain."}, "353935": {"image_id": 353935, "Bleu_1": 0.16981132075151303, "Bleu_2": 0.09897882079639587, "Bleu_3": 0.057699421416016236, "Bleu_4": 7.872924608075425e-06, "METEOR": 0.16289255945832795, "ROUGE_L": 0.18908865468071917, "CIDEr": 2.2745630340954463e-12, "SPICE": {"All": {"pr": 0.10526315789473684, "re": 0.15384615384615385, "f": 0.125, "fn": 22.0, "numImages": 1.0, "fp": 34.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 15.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23529411764705882, "re": 0.5714285714285714, "f": 0.3333333333333333, "fn": 3.0, "numImages": 1.0, "fp": 13.0, "tp": 4.0}}, "caption": "The image shows a plate with three blueberry pancakes on it. The pancakes are topped with sliced strawberries and whipped cream. There is a fork on the plate, and a glass of orange juice is sitting next to it. The background of the image appears to be a red and white checkered tablecloth."}, "443952": {"image_id": 443952, "Bleu_1": 0.23076923076568048, "Bleu_2": 0.20801257358123573, "Bleu_3": 0.14005529698464508, "Bleu_4": 0.09702512940047975, "METEOR": 0.22740031110327222, "ROUGE_L": 0.27164769915883225, "CIDEr": 6.758252493370477e-19, "SPICE": {"All": {"pr": 0.10810810810810811, "re": 0.25, "f": 0.1509433962264151, "fn": 12.0, "numImages": 1.0, "fp": 33.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.2, "f": 0.14285714285714285, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 1.0, "f": 0.33333333333333337, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Object": {"pr": 0.17647058823529413, "re": 0.5, "f": 0.2608695652173913, "fn": 3.0, "numImages": 1.0, "fp": 14.0, "tp": 3.0}}, "caption": "This image is a bathroom with a large walk in shower, a toilet, and a sink. The walls are white and the floor is made of wood. There is a mirror on one wall and a window on the other. The shower has a glass door and a shower head. The toilet is next to the sink and there is a towel rack above it."}, "364399": {"image_id": 364399, "Bleu_1": 0.32692307691678996, "Bleu_2": 0.24019223070296639, "Bleu_3": 0.15127124704939993, "Bleu_4": 0.10902491814160621, "METEOR": 0.3138541493156856, "ROUGE_L": 0.2688916876574307, "CIDEr": 1.0312314169741096e-11, "SPICE": {"All": {"pr": 0.08, "re": 0.11764705882352941, "f": 0.09523809523809526, "fn": 15.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.4, "f": 0.25000000000000006, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "This image shows a table with four bowls of food on it. The bowls are filled with different types of vegetables, including carrots, broccoli, and green beans. There is also a plate of pasta in one of the bowls. The table has a white surface and is surrounded by a black background."}, "281733": {"image_id": 281733, "Bleu_1": 0.12345679012193266, "Bleu_2": 0.05555555555486541, "Bleu_3": 0.0339319875900869, "Bleu_4": 4.730786229096295e-06, "METEOR": 0.1565530298761824, "ROUGE_L": 0.13186338089061822, "CIDEr": 3.9754144122777276e-32, "SPICE": {"All": {"pr": 0.06666666666666667, "re": 0.15, "f": 0.09230769230769231, "fn": 17.0, "numImages": 1.0, "fp": 42.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 18.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.1111111111111111, "re": 0.2857142857142857, "f": 0.16, "fn": 5.0, "numImages": 1.0, "fp": 16.0, "tp": 2.0}}, "caption": "The image shows a young boy sitting at a table with a donut in his hand. He is wearing a white shirt and blue jeans, and has a look of concentration on his face as he takes a bite of the donut. The background is a busy restaurant with tables and chairs, and there are other people eating and talking in the background. The lighting is bright and natural, with shadows cast by the overhead lights on the walls and ceiling."}, "250167": {"image_id": 250167, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.194191754997615, "Bleu_3": 0.11247911738077475, "Bleu_4": 1.2861839592392576e-05, "METEOR": 0.257311445227661, "ROUGE_L": 0.24610951008645532, "CIDEr": 5.057502940063479e-13, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.26666666666666666, "f": 0.13559322033898305, "fn": 11.0, "numImages": 1.0, "fp": 40.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.06666666666666667, "re": 0.5, "f": 0.11764705882352941, "fn": 1.0, "numImages": 1.0, "fp": 14.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.42857142857142855, "f": 0.24, "fn": 4.0, "numImages": 1.0, "fp": 15.0, "tp": 3.0}}, "caption": "The image shows a man standing on the beach holding a surfboard. He is wearing a black shirt and shorts, and has his arms crossed over his chest. The sky is cloudy and there are people in the background walking on the beach. The sand is white and there are waves crashing against the shore."}, "460346": {"image_id": 460346, "Bleu_1": 0.19230769230522685, "Bleu_2": 0.08655927155605073, "Bleu_3": 4.619601686292339e-07, "Bleu_4": 1.0707499987319657e-09, "METEOR": 0.16032654398994192, "ROUGE_L": 0.16353887399463807, "CIDEr": 1.7440457834721893e-23, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.18181818181818182, "f": 0.16326530612244897, "fn": 18.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.07692307692307693, "re": 0.1111111111111111, "f": 0.09090909090909093, "fn": 8.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.75, "f": 0.375, "fn": 1.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a young boy standing on top of a skateboard ramp, with his arms outstretched and his body silhouetted against the sunset. The ramp is made of concrete and has a smooth surface. The boy is wearing a black shirt and jeans, and his hair is messy and unkempt. The sky is orange and pink, with clouds scattered across it. The image is taken from a low angle, looking up at the boy as he skates."}, "246263": {"image_id": 246263, "Bleu_1": 0.29508196720827734, "Bleu_2": 0.17177950029132097, "Bleu_3": 0.11448202542261449, "Bleu_4": 0.07131747850504763, "METEOR": 0.22631284663748583, "ROUGE_L": 0.28053142565150746, "CIDEr": 6.635623363398611e-16, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.14814814814814814, "f": 0.14814814814814814, "fn": 23.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "This image shows a bathroom with a toilet, sink, and shower. The walls are white and the floor is made of tile. There is a window on one side of the room that lets in natural light. The shower has a glass door and the sink is located next to it. The toilet is located in the corner of the room."}, "402588": {"image_id": 402588, "Bleu_1": 0.4102564102458909, "Bleu_2": 0.31171460007158086, "Bleu_3": 0.23591787962790103, "Bleu_4": 0.16434349396401762, "METEOR": 0.3127598977527131, "ROUGE_L": 0.4326241134751773, "CIDEr": 0.0004652183142641713, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.10714285714285714, "f": 0.11111111111111112, "fn": 25.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3333333333333333, "f": 0.3, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a man in a black suit and tie walking down the street with other people. He is holding a briefcase in one hand and gesturing with the other. There are buildings and cars in the background."}, "470779": {"image_id": 470779, "Bleu_1": 0.26785714285235973, "Bleu_2": 0.15604694598027796, "Bleu_3": 0.11059767924186374, "Bleu_4": 0.08452750341737084, "METEOR": 0.19650835254151056, "ROUGE_L": 0.2830626450116009, "CIDEr": 4.0048563726640813e-11, "SPICE": {"All": {"pr": 0.06451612903225806, "re": 0.10526315789473684, "f": 0.07999999999999999, "fn": 17.0, "numImages": 1.0, "fp": 29.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.25, "f": 0.2105263157894737, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a group of people standing on a snowy slope at night. They are all wearing ski gear, including skis, poles, and helmets. One person is holding a ski pole in their hand, while the others are standing around them. The background is dark and there are no other objects visible in the image."}, "306281": {"image_id": 306281, "Bleu_1": 0.15714285714061227, "Bleu_2": 0.08265771673866477, "Bleu_3": 0.046489259876401, "Bleu_4": 6.222942289398482e-06, "METEOR": 0.19513185044302436, "ROUGE_L": 0.22867853795688844, "CIDEr": 1.645385502843291e-20, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.23809523809523808, "f": 0.19607843137254902, "fn": 16.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.2, "f": 0.20000000000000004, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 0.5, "f": 0.28571428571428575, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5714285714285714, "f": 0.38095238095238093, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image is a pan with a variety of ingredients, including broccoli, mushrooms, and soy sauce. The broccoli is cut into small florets and the mushrooms are sliced thinly. The soy sauce is poured over the top of the ingredients and there are some sesame seeds sprinkled on top. The pan appears to be a stainless steel or non-stick pan and it looks like it has been cooked in it."}, "307790": {"image_id": 307790, "Bleu_1": 0.31372549018992696, "Bleu_2": 0.17712297710451136, "Bleu_3": 8.61888809829365e-07, "Bleu_4": 1.9110767207150235e-09, "METEOR": 0.21416979234678923, "ROUGE_L": 0.23416506717850286, "CIDEr": 6.305942511388683e-11, "SPICE": {"All": {"pr": 0.12903225806451613, "re": 0.12903225806451613, "f": 0.12903225806451613, "fn": 27.0, "numImages": 1.0, "fp": 27.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2222222222222222, "f": 0.1904761904761905, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a brown chicken standing on the ground in a jungle environment. The chicken has a distinctive red crest on its head and is looking around with its beak open. The background of the image is made up of green foliage, with leaves and branches visible in the foreground."}, "353231": {"image_id": 353231, "Bleu_1": 0.28070175438104034, "Bleu_2": 0.07079923253922574, "Bleu_3": 4.500196423316003e-07, "Bleu_4": 1.1397908406297796e-09, "METEOR": 0.1279420589633536, "ROUGE_L": 0.17086834733893558, "CIDEr": 1.508335496120362e-13, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.2, "f": 0.18604651162790697, "fn": 16.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.125, "re": 0.14285714285714285, "f": 0.13333333333333333, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.5, "f": 0.3157894736842105, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image shows a body of water with a large ship docked at the pier. There are several buildings on the shore, including a large white building with a red roof. The sky is clear and blue, with a few clouds scattered across it. The water is calm and there are no waves or ripples in it."}, "199688": {"image_id": 199688, "Bleu_1": 0.34883720929421314, "Bleu_2": 0.24112141107953186, "Bleu_3": 0.1415487039853373, "Bleu_4": 1.6317905930043706e-05, "METEOR": 0.25769929982078676, "ROUGE_L": 0.28561872909699, "CIDEr": 3.4029637724908786e-06, "SPICE": {"All": {"pr": 0.15789473684210525, "re": 0.2608695652173913, "f": 0.19672131147540983, "fn": 17.0, "numImages": 1.0, "fp": 32.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.375, "re": 0.6, "f": 0.4615384615384615, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 6.0}}, "caption": "This is an image of a living room with a couch, coffee table, and chairs. There are windows on the walls and a door leading to another room. The floor is made of wood and there are plants in pots on the table."}, "13992": {"image_id": 13992, "Bleu_1": 0.339622641503026, "Bleu_2": 0.1979576415927917, "Bleu_3": 0.09159212225374062, "Bleu_4": 1.1133996756281149e-05, "METEOR": 0.23068389722651034, "ROUGE_L": 0.3152325486014273, "CIDEr": 8.61789775955057e-12, "SPICE": {"All": {"pr": 0.25925925925925924, "re": 0.18421052631578946, "f": 0.2153846153846154, "fn": 31.0, "numImages": 1.0, "fp": 20.0, "tp": 7.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.05555555555555555, "f": 0.06896551724137931, "fn": 17.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.46153846153846156, "f": 0.4444444444444445, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The image is of a sign that reads \"Toilets\" in blue letters on a white background. The sign is mounted on a pole next to a sidewalk in front of a building with windows and a door. There are trees and other plants nearby, and people can be seen walking on the sidewalk."}, "567315": {"image_id": 567315, "Bleu_1": 0.23809523809145886, "Bleu_2": 0.15179418517730023, "Bleu_3": 0.07228699064395765, "Bleu_4": 8.907530052593212e-06, "METEOR": 0.19949488104528076, "ROUGE_L": 0.19273301737756712, "CIDEr": 3.4557577433275155e-17, "SPICE": {"All": {"pr": 0.08, "re": 0.125, "f": 0.09756097560975609, "fn": 14.0, "numImages": 1.0, "fp": 23.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.2857142857142857, "f": 0.23529411764705882, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image shows a train traveling along a railroad track through a green and lush landscape. The train is a red and yellow tanker car with the words \"Railway Company\" written on the side. The train is moving quickly, with smoke billowing from its engine. In the background, there are trees and a small body of water. The sky is clear and blue."}, "437351": {"image_id": 437351, "Bleu_1": 0.20967741935145687, "Bleu_2": 0.10154803902700668, "Bleu_3": 5.559861127421403e-07, "Bleu_4": 1.3064264215491097e-09, "METEOR": 0.19116752147064642, "ROUGE_L": 0.19307870858830975, "CIDEr": 4.7083104127915414e-15, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.16129032258064516, "f": 0.16393442622950818, "fn": 26.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.35714285714285715, "f": 0.3846153846153846, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a woman sitting on the floor next to a pile of suitcases. She is wearing a red hat and black boots, and has her arms crossed over her chest. The suitcases are stacked up against the wall, with one open and one closed. The room appears to be dimly lit, with shadows cast by the suitcases on the wall."}, "531602": {"image_id": 531602, "Bleu_1": 0.29268292682212976, "Bleu_2": 0.17107978454943554, "Bleu_3": 0.09087496586948689, "Bleu_4": 1.1854610697110774e-05, "METEOR": 0.21292999670910762, "ROUGE_L": 0.2713120830244626, "CIDEr": 5.693453441504228e-07, "SPICE": {"All": {"pr": 0.041666666666666664, "re": 0.034482758620689655, "f": 0.03773584905660377, "fn": 28.0, "numImages": 1.0, "fp": 23.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.1, "re": 0.1, "f": 0.10000000000000002, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}}, "caption": "The image shows a young boy holding a toothbrush in his hand. He is wearing a yellow shirt and has short, black hair. The background of the image appears to be a bathroom with a sink and toilet in the foreground."}, "430546": {"image_id": 430546, "Bleu_1": 0.18965517241052324, "Bleu_2": 0.09990921468676503, "Bleu_3": 5.627830660728098e-07, "Bleu_4": 1.3417298862153888e-09, "METEOR": 0.17336545922477, "ROUGE_L": 0.20220994475138118, "CIDEr": 1.168248742840951e-14, "SPICE": {"All": {"pr": 0.1276595744680851, "re": 0.2608695652173913, "f": 0.1714285714285714, "fn": 17.0, "numImages": 1.0, "fp": 41.0, "tp": 6.0}, "Relation": {"pr": 0.13333333333333333, "re": 0.25, "f": 0.1739130434782609, "fn": 6.0, "numImages": 1.0, "fp": 13.0, "tp": 2.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.4444444444444444, "f": 0.2758620689655173, "fn": 5.0, "numImages": 1.0, "fp": 16.0, "tp": 4.0}}, "caption": "The image shows a person riding a motorcycle down a winding road. The person is wearing a helmet and has their arms outstretched as they ride the bike. The road is lined with trees on either side and there are other cars driving in the opposite direction. The sky is blue and there are clouds in the distance."}, "309366": {"image_id": 309366, "Bleu_1": 0.1690140845046618, "Bleu_2": 0.09827485785206054, "Bleu_3": 5.192126352208003e-07, "Bleu_4": 1.1977928361466131e-09, "METEOR": 0.1770646713892061, "ROUGE_L": 0.19999999999999998, "CIDEr": 5.417626955053117e-24, "SPICE": {"All": {"pr": 0.22580645161290322, "re": 0.30434782608695654, "f": 0.25925925925925924, "fn": 16.0, "numImages": 1.0, "fp": 24.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.6666666666666666, "f": 0.5217391304347826, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The image shows two trains traveling on a train track. One of the trains is yellow and has the words \"Ferroviaria\" written on it in black letters. The other train is white and has the words \"Ferroviaria\" written on it in black letters as well. Both trains are moving at a slow pace and appear to be carrying passengers. There are trees and buildings visible in the background of the image."}, "560111": {"image_id": 560111, "Bleu_1": 0.3090909090852893, "Bleu_2": 0.23924685418403427, "Bleu_3": 0.17544010109425545, "Bleu_4": 0.12004755374502663, "METEOR": 0.2919173353056903, "ROUGE_L": 0.3099943534726144, "CIDEr": 2.198423031022814e-12, "SPICE": {"All": {"pr": 0.27586206896551724, "re": 0.34782608695652173, "f": 0.3076923076923077, "fn": 15.0, "numImages": 1.0, "fp": 21.0, "tp": 8.0}, "Relation": {"pr": 0.1, "re": 0.125, "f": 0.11111111111111112, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2222222222222222, "f": 0.26666666666666666, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.6666666666666666, "f": 0.8, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.38461538461538464, "re": 0.8333333333333334, "f": 0.5263157894736842, "fn": 1.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a red and yellow train on the tracks at a train station. The train is parked next to a platform with people standing on it. There are other trains in the background, some of which are also parked on the platform. The sky is clear and there are no clouds in sight."}, "152946": {"image_id": 152946, "Bleu_1": 0.21052631578670358, "Bleu_2": 0.14017532880823577, "Bleu_3": 0.08098050504475202, "Bleu_4": 0.051934343452729, "METEOR": 0.18575825100654828, "ROUGE_L": 0.18310463121783876, "CIDEr": 2.1125286162788178e-26, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.35714285714285715, "f": 0.2380952380952381, "fn": 9.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.2222222222222222, "re": 0.4, "f": 0.2857142857142857, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.6, "f": 0.42857142857142855, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 3.0}}, "caption": "The image shows a group of people standing in front of a large pile of skis and snowboards. There are several people in the image, including a man and woman wearing ski gear and standing next to each other. They are both holding ski poles and looking at something on the ground. In the background, there are several trees and a mountain range. The sky is clear and blue, with some clouds visible in the distance."}, "364016": {"image_id": 364016, "Bleu_1": 0.20454545454313017, "Bleu_2": 0.15333287901261244, "Bleu_3": 0.11793456961415753, "Bleu_4": 0.09911025978954341, "METEOR": 0.2459770192789478, "ROUGE_L": 0.23497688751926038, "CIDEr": 2.8578606701085853e-32, "SPICE": {"All": {"pr": 0.10714285714285714, "re": 0.13636363636363635, "f": 0.11999999999999998, "fn": 19.0, "numImages": 1.0, "fp": 25.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 12.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.3, "f": 0.24, "fn": 7.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}}, "caption": "The image shows a group of people sitting around a table in a room. They are all wearing white shirts and black pants, and they appear to be working on something together. There is a large wooden desk in the center of the room with various objects on it, including a laptop, a notebook, and several pens and pencils. The walls of the room are painted a light blue color and there are several windows on the left side of the image that allow natural light to enter."}, "449603": {"image_id": 449603, "Bleu_1": 0.22727272726928377, "Bleu_2": 0.14484136487336885, "Bleu_3": 0.08687171852836896, "Bleu_4": 1.0100053456190439e-05, "METEOR": 0.21518137846182528, "ROUGE_L": 0.2427860696517413, "CIDEr": 1.0058769568573812e-19, "SPICE": {"All": {"pr": 0.3888888888888889, "re": 0.21875, "f": 0.28, "fn": 25.0, "numImages": 1.0, "fp": 11.0, "tp": 7.0}, "Relation": {"pr": 0.2, "re": 0.058823529411764705, "f": 0.0909090909090909, "fn": 16.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.5, "f": 0.5, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "The image shows a person surfing on a wave in the ocean. The person is holding onto the rope and jumping off the wave, with their board underneath them. The wave is large and white, with foam on top of it. The sky is blue and cloudy, with some clouds visible in the distance. The water is choppy and turbulent, with whitecaps on top of it."}, "350289": {"image_id": 350289, "Bleu_1": 0.33898305084171215, "Bleu_2": 0.1872624152237565, "Bleu_3": 0.10715656750302463, "Bleu_4": 0.06846475974034474, "METEOR": 0.2193743753919883, "ROUGE_L": 0.20344635908838243, "CIDEr": 9.476447050215449e-11, "SPICE": {"All": {"pr": 0.15151515151515152, "re": 0.1724137931034483, "f": 0.16129032258064518, "fn": 24.0, "numImages": 1.0, "fp": 28.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.23529411764705882, "re": 0.4, "f": 0.29629629629629634, "fn": 6.0, "numImages": 1.0, "fp": 13.0, "tp": 4.0}}, "caption": "This image shows a group of people standing on the beach, looking out at the ocean. They are wearing surfboards and wetsuits, and one person is holding a surfboard under their arm. The sky is cloudy and there are waves crashing against the shore. The sand is wet and there are footprints leading from the water to the shore."}, "460621": {"image_id": 460621, "Bleu_1": 0.34615384614053263, "Bleu_2": 0.23533936215658835, "Bleu_3": 0.13214760629595454, "Bleu_4": 1.779764404504326e-05, "METEOR": 0.3308736195471718, "ROUGE_L": 0.39019189765458434, "CIDEr": 0.013160604216091717, "SPICE": {"All": {"pr": 0.1875, "re": 0.24, "f": 0.21052631578947367, "fn": 19.0, "numImages": 1.0, "fp": 26.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1, "re": 0.125, "f": 0.11111111111111112, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Size": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.5555555555555556, "f": 0.43478260869565216, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image is a bathroom with a white bathtub and toilet. The walls are blue and there are no windows or other furniture in the room."}, "473199": {"image_id": 473199, "Bleu_1": 0.3584905660309719, "Bleu_2": 0.16606082435802713, "Bleu_3": 8.146819001060783e-07, "Bleu_4": 1.813420619820594e-09, "METEOR": 0.22147094094854816, "ROUGE_L": 0.18496058217101274, "CIDEr": 1.1134362146233301e-11, "SPICE": {"All": {"pr": 0.15384615384615385, "re": 0.10526315789473684, "f": 0.125, "fn": 17.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3333333333333333, "re": 0.2857142857142857, "f": 0.30769230769230765, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}}, "caption": "The image shows a group of people standing on the sidewalk in front of a large building with yellow umbrellas. There are several potted plants and flowers in the area, and the building has a red awning over the entrance. The sky is clear and blue, with some white clouds in the distance."}, "322944": {"image_id": 322944, "Bleu_1": 0.2909090909038017, "Bleu_2": 0.20759971843926334, "Bleu_3": 0.15960600175638903, "Bleu_4": 0.0940342527703212, "METEOR": 0.20264500295020985, "ROUGE_L": 0.2658241638522715, "CIDEr": 4.2096714978137267e-10, "SPICE": {"All": {"pr": 0.07407407407407407, "re": 0.1111111111111111, "f": 0.08888888888888888, "fn": 16.0, "numImages": 1.0, "fp": 25.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.14285714285714285, "re": 0.2857142857142857, "f": 0.19047619047619047, "fn": 5.0, "numImages": 1.0, "fp": 12.0, "tp": 2.0}}, "caption": "The image shows a young woman holding a stuffed animal, possibly a teddy bear. She is wearing a black shirt and has long, dark hair. Her eyes are closed and she appears to be in a state of deep contemplation or sadness. The background is dark and the lighting is dim, creating an eerie atmosphere."}, "110027": {"image_id": 110027, "Bleu_1": 0.22222222221810703, "Bleu_2": 0.17131872291347974, "Bleu_3": 0.1311863860222524, "Bleu_4": 0.09700218812496947, "METEOR": 0.22478572330462376, "ROUGE_L": 0.21863799283154117, "CIDEr": 4.2079822716162264e-13, "SPICE": {"All": {"pr": 0.18421052631578946, "re": 0.3333333333333333, "f": 0.23728813559322035, "fn": 14.0, "numImages": 1.0, "fp": 31.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.15384615384615385, "re": 0.3333333333333333, "f": 0.21052631578947367, "fn": 4.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 0.625, "f": 0.45454545454545453, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "This is a garage with a wooden table and chairs in the middle of the room. There are shelves on the walls with various tools and equipment hanging from them. The ceiling is made of wood beams and there is a large window on one side of the room that lets in natural light."}, "74492": {"image_id": 74492, "Bleu_1": 0.1807228915640877, "Bleu_2": 0.1408383446287542, "Bleu_3": 0.10698156244502917, "Bleu_4": 0.08845536998553584, "METEOR": 0.2458822026279349, "ROUGE_L": 0.17280453257790365, "CIDEr": 1.51193312776047e-29, "SPICE": {"All": {"pr": 0.11428571428571428, "re": 0.26666666666666666, "f": 0.16, "fn": 11.0, "numImages": 1.0, "fp": 31.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.2, "re": 0.5, "f": 0.28571428571428575, "fn": 3.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}}, "caption": "The image shows a person holding a remote control in their hand. The remote control has a small screen on it that displays an image of a TV. The image appears to be a still shot of a TV screen showing a black and white image of a person's face. The person's face is shown in profile, with their eyes closed and their mouth open in a relaxed expression. The image is quite small, measuring about 1 inch by 1 inch."}, "280211": {"image_id": 280211, "Bleu_1": 0.19148936169805347, "Bleu_2": 0.09124485969890858, "Bleu_3": 5.698161672784481e-07, "Bleu_4": 1.4319831453628615e-09, "METEOR": 0.1677111089920429, "ROUGE_L": 0.1667805878332194, "CIDEr": 6.143662559010986e-09, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.17391304347826086, "f": 0.15094339622641512, "fn": 19.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5, "f": 0.380952380952381, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image is of a train station with a large, metal fence surrounding it. There are several cars parked on the tracks, and a sign on the side of the building reads \"train station\". The sky is clear and blue, and there are no people in sight."}, "41247": {"image_id": 41247, "Bleu_1": 0.16249999999796877, "Bleu_2": 0.11109352884659052, "Bleu_3": 0.05408717607299722, "Bleu_4": 6.732836204342859e-06, "METEOR": 0.16258030995864706, "ROUGE_L": 0.17956265769554244, "CIDEr": 2.581373931532142e-26, "SPICE": {"All": {"pr": 0.16, "re": 0.17391304347826086, "f": 0.16666666666666666, "fn": 19.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.1, "re": 0.14285714285714285, "f": 0.11764705882352941, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.42857142857142855, "f": 0.3157894736842105, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a group of people surfing in the ocean. They are standing on the beach, holding their surfboards and looking out at the water. The sky is clear and blue, with a few clouds scattered about. The waves are crashing against the shore, creating a white foam on the surface of the water. The sand is wet and sandy, with some shells and other debris scattered about. There are no other people or objects visible in the image."}, "419379": {"image_id": 419379, "Bleu_1": 0.2162162162103726, "Bleu_2": 2.450715406912204e-09, "Bleu_3": 5.556985188906216e-12, "Bleu_4": 2.665383471016607e-13, "METEOR": 0.11546391752577319, "ROUGE_L": 0.1508656224237428, "CIDEr": 2.036854101347626e-06, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.34782608695652173, "f": 0.3137254901960784, "fn": 15.0, "numImages": 1.0, "fp": 20.0, "tp": 8.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2857142857142857, "f": 0.30769230769230765, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.42857142857142855, "re": 0.8571428571428571, "f": 0.5714285714285714, "fn": 1.0, "numImages": 1.0, "fp": 8.0, "tp": 6.0}}, "caption": "The image is of a tray with three donuts on it. The donuts are made of a light brown color and have sprinkles on top. There is a sign next to the tray that reads, \"Glazed Donuts\"."}, "581731": {"image_id": 581731, "Bleu_1": 0.20689655172057078, "Bleu_2": 0.08520286456697912, "Bleu_3": 5.061044102501325e-07, "Bleu_4": 1.2390516745962156e-09, "METEOR": 0.13282090832845153, "ROUGE_L": 0.13480662983425412, "CIDEr": 1.3034990465690284e-14, "SPICE": {"All": {"pr": 0.13793103448275862, "re": 0.13793103448275862, "f": 0.13793103448275862, "fn": 25.0, "numImages": 1.0, "fp": 25.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.1, "f": 0.11764705882352941, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.23076923076923078, "f": 0.23076923076923078, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "This image shows a forest with tall trees and a path leading through the woods. The trees are bare of leaves, and the sun is shining down on the forest floor. There are several wooden posts scattered throughout the forest, some of which have birdhouses on top. The sky is clear and blue, with no clouds in sight."}, "103584": {"image_id": 103584, "Bleu_1": 0.2545454545408265, "Bleu_2": 0.194191754997615, "Bleu_3": 0.12875644765560088, "Bleu_4": 0.09518826316467809, "METEOR": 0.29170666383042776, "ROUGE_L": 0.31642651296829977, "CIDEr": 2.1976715248713973e-11, "SPICE": {"All": {"pr": 0.0967741935483871, "re": 0.1875, "f": 0.12765957446808507, "fn": 13.0, "numImages": 1.0, "fp": 28.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.2, "re": 0.6, "f": 0.3, "fn": 2.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}}, "caption": "The image shows a golden retriever sitting on a bed, looking up at the camera. The dog is wearing a collar and has its tongue hanging out of its mouth. The background is a red wall with a white ceiling and a white bedspread. There is a lamp on the nightstand next to the bed."}, "350132": {"image_id": 350132, "Bleu_1": 0.1911764705854239, "Bleu_2": 0.14132820350390682, "Bleu_3": 0.0845891340407322, "Bleu_4": 0.05524049926540425, "METEOR": 0.21151424148590056, "ROUGE_L": 0.2575046904315197, "CIDEr": 3.1554807682952355e-20, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 15.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.25, "f": 0.15384615384615383, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 1.0, "f": 0.4, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.2857142857142857, "re": 0.5714285714285714, "f": 0.38095238095238093, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image is a photograph of a person playing frisbee in a park. The person is wearing a red shirt and black pants, and is holding the frisbee in their right hand. The background is a city skyline with tall buildings and trees. The image is taken from a bird's eye view, showing the person and the frisbee in the foreground, and the city in the background."}, "210448": {"image_id": 210448, "Bleu_1": 0.14772727272559402, "Bleu_2": 0.10902340300629078, "Bleu_3": 0.05170275343435692, "Bleu_4": 6.350099387678949e-06, "METEOR": 0.14498308323248835, "ROUGE_L": 0.1644838212634823, "CIDEr": 2.7394449448382833e-37, "SPICE": {"All": {"pr": 0.05, "re": 0.07407407407407407, "f": 0.05970149253731344, "fn": 25.0, "numImages": 1.0, "fp": 38.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.125, "re": 0.15384615384615385, "f": 0.13793103448275862, "fn": 11.0, "numImages": 1.0, "fp": 14.0, "tp": 2.0}}, "caption": "The image shows a group of zebras standing in a enclosure made of wood and metal bars. The zebras are all facing the same direction, with their heads down and ears up. They appear to be grazing on some grass that is growing in the enclosure. There are some trees in the background, providing shade for the zebras. The enclosure is surrounded by a fence, with a gate in the center. There are some rocks and boulders scattered around the enclosure, as well as some bushes and plants."}, "267300": {"image_id": 267300, "Bleu_1": 0.15555555555382716, "Bleu_2": 0.11061060806475878, "Bleu_3": 0.08223504976816709, "Bleu_4": 0.0661748888044883, "METEOR": 0.19651003714307502, "ROUGE_L": 0.21585279547062985, "CIDEr": 2.0768262124325837e-34, "SPICE": {"All": {"pr": 0.2857142857142857, "re": 0.19047619047619047, "f": 0.22857142857142854, "fn": 17.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}, "Relation": {"pr": 0.6666666666666666, "re": 0.2, "f": 0.30769230769230765, "fn": 8.0, "numImages": 1.0, "fp": 1.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.2222222222222222, "f": 0.25, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 2.0}}, "caption": "The image shows a dog sitting on the floor next to a plate of food. The dog is looking up at the camera with its mouth open, as if it is about to eat something. The plate of food appears to be a bowl of kibble, and there are also some bones on the floor next to the dog. The background of the image is a wooden floor with a pattern of wood grain. There is also a window in the background that allows natural light to enter the room."}, "107939": {"image_id": 107939, "Bleu_1": 0.20289855072169713, "Bleu_2": 0.16387257228732635, "Bleu_3": 0.13397681929347618, "Bleu_4": 0.10225076764052626, "METEOR": 0.2649873962350802, "ROUGE_L": 0.23008015087223005, "CIDEr": 5.4915027791373596e-21, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.22727272727272727, "f": 0.19607843137254902, "fn": 17.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.09090909090909091, "re": 0.09090909090909091, "f": 0.09090909090909091, "fn": 10.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.36363636363636365, "f": 0.33333333333333337, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image is a photograph of a street scene with a stop sign on the side of the road. The stop sign is white with red letters that read \"stop\" in bold, black letters. The sign is mounted on a pole next to a tree and there are other trees and houses visible in the background. The sky is clear and blue with some clouds visible in the distance."}, "352760": {"image_id": 352760, "Bleu_1": 0.24561403508341031, "Bleu_2": 0.1480872194371519, "Bleu_3": 0.10615274788259942, "Bleu_4": 0.06860408726311093, "METEOR": 0.2154070182065701, "ROUGE_L": 0.23682750970604544, "CIDEr": 3.30163936683149e-13, "SPICE": {"All": {"pr": 0.2777777777777778, "re": 0.18518518518518517, "f": 0.22222222222222224, "fn": 22.0, "numImages": 1.0, "fp": 13.0, "tp": 5.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.5714285714285714, "re": 0.5714285714285714, "f": 0.5714285714285714, "fn": 3.0, "numImages": 1.0, "fp": 3.0, "tp": 4.0}}, "caption": "This is an image of a person skiing in the air. The person is wearing a yellow and black suit with a helmet on their head. They are holding onto two skis that are attached to their feet. The person is jumping off a cliff and flying through the air. There is a cloudy sky behind them."}, "58210": {"image_id": 58210, "Bleu_1": 0.32142857142283165, "Bleu_2": 0.24174688920325796, "Bleu_3": 0.14807546137424474, "Bleu_4": 1.5732341081594925e-05, "METEOR": 0.215742965952138, "ROUGE_L": 0.2476798143851508, "CIDEr": 9.401909331103365e-13, "SPICE": {"All": {"pr": 0.21875, "re": 0.25925925925925924, "f": 0.23728813559322032, "fn": 20.0, "numImages": 1.0, "fp": 25.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.375, "re": 0.3, "f": 0.33333333333333326, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 3.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 1.0, "f": 0.4, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.26666666666666666, "re": 0.5, "f": 0.3478260869565218, "fn": 4.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "The image is of a small bird standing on the grass. It has a white belly and black and white stripes on its back. It is looking down at the ground with its beak open, as if it is about to take a drink. There are some leaves in the background, and the sky is cloudy."}, "529592": {"image_id": 529592, "Bleu_1": 0.2931034482708086, "Bleu_2": 0.2028233864039703, "Bleu_3": 0.14323066483979047, "Bleu_4": 0.10167023617400972, "METEOR": 0.18010242092682063, "ROUGE_L": 0.16522210184182015, "CIDEr": 3.4197465242283904e-13, "SPICE": {"All": {"pr": 0.075, "re": 0.13043478260869565, "f": 0.09523809523809523, "fn": 20.0, "numImages": 1.0, "fp": 37.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 15.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.14285714285714285, "f": 0.125, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.125, "re": 0.2, "f": 0.15384615384615385, "fn": 8.0, "numImages": 1.0, "fp": 14.0, "tp": 2.0}}, "caption": "The image shows a group of people playing a game of dodgeball on a sandy beach. There are five people in total, all wearing yellow shirts and black pants. One person is holding a dodgeball while the others are standing around him, ready to catch it. The sky is cloudy and there are some trees in the background."}, "191112": {"image_id": 191112, "Bleu_1": 0.1704545454526085, "Bleu_2": 0.10842271852867472, "Bleu_3": 0.0742941233835847, "Bleu_4": 0.04686634828104279, "METEOR": 0.20676698148541037, "ROUGE_L": 0.16297709923664122, "CIDEr": 1.4779334698457476e-35, "SPICE": {"All": {"pr": 0.11764705882352941, "re": 0.2, "f": 0.14814814814814817, "fn": 16.0, "numImages": 1.0, "fp": 30.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.125, "re": 0.5, "f": 0.2, "fn": 1.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.25, "re": 1.0, "f": 0.4, "fn": 0.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Object": {"pr": 0.17647058823529413, "re": 0.42857142857142855, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 14.0, "tp": 3.0}}, "caption": "The image shows a group of people standing in front of a soccer goal. They are all wearing black and white uniforms with the words \"Team 1\" written on the back. One person is holding a soccer ball while another is standing on the goal line with their arms crossed. There are two other people standing on either side of the goal, one with their hands on their hips and the other with their arms extended. The background is a green field with a white fence surrounding it."}, "549338": {"image_id": 549338, "Bleu_1": 0.16494845360654692, "Bleu_2": 0.12435400084216307, "Bleu_3": 0.06879263926761682, "Bleu_4": 0.04313944121483135, "METEOR": 0.23113259616999038, "ROUGE_L": 0.1946118397731301, "CIDEr": 2.3753078291773596e-45, "SPICE": {"All": {"pr": 0.125, "re": 0.2777777777777778, "f": 0.1724137931034483, "fn": 13.0, "numImages": 1.0, "fp": 35.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.3125, "re": 0.8333333333333334, "f": 0.45454545454545453, "fn": 1.0, "numImages": 1.0, "fp": 11.0, "tp": 5.0}}, "caption": "The image shows a group of horses grazing in a green pasture surrounded by tall trees. The horses are white and brown, and they are standing in a fenced area with a white picket fence. In the background, there is a mountain range with green trees and blue sky.\n\nThe image is taken from a distance, so the details of the horses and the fence are not very clear. However, it gives a sense of peacefulness and tranquility. The green grass and trees create a beautiful landscape, and the horses seem to be enjoying their grazing time."}, "369849": {"image_id": 369849, "Bleu_1": 0.27586206896076104, "Bleu_2": 0.19676758717389048, "Bleu_3": 0.1403652795382032, "Bleu_4": 0.10014092478468753, "METEOR": 0.3024051792390713, "ROUGE_L": 0.2862356621480709, "CIDEr": 2.2977266972004407e-13, "SPICE": {"All": {"pr": 0.06976744186046512, "re": 0.07692307692307693, "f": 0.07317073170731707, "fn": 36.0, "numImages": 1.0, "fp": 40.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 15.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.21428571428571427, "f": 0.1875, "fn": 11.0, "numImages": 1.0, "fp": 15.0, "tp": 3.0}}, "caption": "The image shows a group of horses grazing in a field next to a dirt road. The horses are brown and white, and they are standing in a line along the side of the road. There is a sign in the background that reads \"Caution: Wildlife Crossing\". The sky is cloudy and there are trees in the background."}, "206271": {"image_id": 206271, "Bleu_1": 0.2549019607793157, "Bleu_2": 0.1596564940032875, "Bleu_3": 0.11599326778225837, "Bleu_4": 0.07551170533670244, "METEOR": 0.17943764695099532, "ROUGE_L": 0.2238532110091743, "CIDEr": 2.154050493752607e-11, "SPICE": {"All": {"pr": 0.18518518518518517, "re": 0.1724137931034483, "f": 0.17857142857142858, "fn": 24.0, "numImages": 1.0, "fp": 22.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.09090909090909091, "f": 0.12500000000000003, "fn": 10.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.26666666666666666, "re": 0.4444444444444444, "f": 0.33333333333333337, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "This is a bathroom with two sinks and a toilet. The walls are made of tiles in different colors, including yellow, blue, and green. There is a mirror on the wall above the sink, and a light bulb hanging from the ceiling. The floor is made of white and black tiles."}, "442301": {"image_id": 442301, "Bleu_1": 0.27272727272107444, "Bleu_2": 0.15927956195690762, "Bleu_3": 0.12191693142366875, "Bleu_4": 0.09696378326119114, "METEOR": 0.26529363799171995, "ROUGE_L": 0.2840606705694519, "CIDEr": 6.069703623813185e-07, "SPICE": {"All": {"pr": 0.05660377358490566, "re": 0.14285714285714285, "f": 0.0810810810810811, "fn": 18.0, "numImages": 1.0, "fp": 50.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 15.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0625, "re": 0.16666666666666666, "f": 0.09090909090909091, "fn": 5.0, "numImages": 1.0, "fp": 15.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.09090909090909091, "re": 0.2222222222222222, "f": 0.1290322580645161, "fn": 7.0, "numImages": 1.0, "fp": 20.0, "tp": 2.0}}, "caption": "The image shows a person holding a carrot in their hand. The carrot is shaped like a finger and has a small piece of cheese on top of it. There are also some other food items on the table, such as bread and fruit."}, "578788": {"image_id": 578788, "Bleu_1": 0.15189873417529243, "Bleu_2": 0.08825915632701321, "Bleu_3": 0.05870651110804434, "Bleu_4": 0.04039348326006448, "METEOR": 0.16694271722070314, "ROUGE_L": 0.19877800407331978, "CIDEr": 2.057861430767489e-27, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.2777777777777778, "f": 0.20833333333333334, "fn": 13.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.2, "f": 0.14285714285714285, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.5714285714285714, "f": 0.34782608695652173, "fn": 3.0, "numImages": 1.0, "fp": 12.0, "tp": 4.0}}, "caption": "The image shows a person standing on a skateboard on a ramp leading up to a set of stairs. The person is wearing a black and white striped shirt, black pants, and black shoes. They are holding onto the handlebars of the skateboard with their left hand and have their right hand in the air. The background is a city street with tall buildings and a busy intersection. There are several cars and buses passing by on the road."}, "5352": {"image_id": 5352, "Bleu_1": 0.3870967741810615, "Bleu_2": 0.3407771005370629, "Bleu_3": 0.2715423701702676, "Bleu_4": 0.2152132515932983, "METEOR": 0.30921610971493535, "ROUGE_L": 0.3125533731853117, "CIDEr": 0.0068419596583985266, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.2222222222222222, "f": 0.1904761904761905, "fn": 14.0, "numImages": 1.0, "fp": 20.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2857142857142857, "f": 0.30769230769230765, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.6666666666666666, "re": 0.6666666666666666, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 2.0}, "Object": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 2.0}}, "caption": "The image is a plate with a slice of pizza on it. The pizza has cheese and pepperoni on top, and there are two beers on the table next to it."}, "236023": {"image_id": 236023, "Bleu_1": 0.31818181817699726, "Bleu_2": 0.19789097796424007, "Bleu_3": 0.12244219036745456, "Bleu_4": 0.0734705312586394, "METEOR": 0.1821764313782462, "ROUGE_L": 0.22283105022831048, "CIDEr": 3.212327447635458e-17, "SPICE": {"All": {"pr": 0.09523809523809523, "re": 0.07407407407407407, "f": 0.08333333333333333, "fn": 25.0, "numImages": 1.0, "fp": 19.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.18181818181818182, "re": 0.2222222222222222, "f": 0.19999999999999998, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 2.0}}, "caption": "The image shows a group of people standing in a kitchen. They are all wearing green shirts and black pants. One person is holding a cup of coffee, while another person is holding a plate of food. There are several bottles of alcohol on the counter, as well as some glasses and plates. The walls are painted white and there are some windows in the background."}, "339943": {"image_id": 339943, "Bleu_1": 0.24999999999519235, "Bleu_2": 0.19802950859148932, "Bleu_3": 0.14639174990113343, "Bleu_4": 0.10637655718853216, "METEOR": 0.24437039797271012, "ROUGE_L": 0.2610024449877751, "CIDEr": 2.318950168768889e-11, "SPICE": {"All": {"pr": 0.1111111111111111, "re": 0.21052631578947367, "f": 0.14545454545454545, "fn": 15.0, "numImages": 1.0, "fp": 32.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.16666666666666666, "f": 0.1818181818181818, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 0.3333333333333333, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.17647058823529413, "re": 0.6, "f": 0.2727272727272727, "fn": 2.0, "numImages": 1.0, "fp": 14.0, "tp": 3.0}}, "caption": "This is a bathroom with a sink, toilet, and bathtub. The walls are made of wood and the floor is made of hardwood. There is a window on one side of the room and a door on the other. The room has a lot of natural light coming in through the window."}, "491755": {"image_id": 491755, "Bleu_1": 0.26229508196291323, "Bleu_2": 0.19835388146890715, "Bleu_3": 0.1587548060745315, "Bleu_4": 0.10837913035765115, "METEOR": 0.2123034350344847, "ROUGE_L": 0.2857886517438834, "CIDEr": 1.4447510720021916e-15, "SPICE": {"All": {"pr": 0.20689655172413793, "re": 0.2, "f": 0.20338983050847456, "fn": 24.0, "numImages": 1.0, "fp": 23.0, "tp": 6.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.09090909090909091, "f": 0.09999999999999999, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.2857142857142857, "re": 0.3333333333333333, "f": 0.30769230769230765, "fn": 8.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image is a living room with a couch, coffee table, and television. There are two windows on the wall opposite the couch and a ceiling fan in the center of the room. The floor is made of hardwood and there are no rugs or carpets. The walls are painted white and there are no curtains or blinds on the windows."}, "375405": {"image_id": 375405, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.17131872291347974, "Bleu_3": 0.10412270358781489, "Bleu_4": 1.2197379410072965e-05, "METEOR": 0.22072963028819584, "ROUGE_L": 0.25326215895610915, "CIDEr": 2.721962974177937e-12, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.2857142857142857, "f": 0.2105263157894737, "fn": 15.0, "numImages": 1.0, "fp": 30.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.15384615384615385, "re": 0.5, "f": 0.23529411764705882, "fn": 2.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2857142857142857, "re": 0.6666666666666666, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 10.0, "tp": 4.0}}, "caption": "The image shows two white birds perched on a branch of a tree. They are facing each other and appear to be in love. The birds have soft, fluffy feathers and are standing on their hind legs, with their heads tilted towards each other. The background is a green tree with leaves and branches."}, "253624": {"image_id": 253624, "Bleu_1": 0.35616438355676483, "Bleu_2": 0.2332680926372891, "Bleu_3": 0.14526827284605595, "Bleu_4": 0.08134925308630969, "METEOR": 0.2580693321999035, "ROUGE_L": 0.29171376402051985, "CIDEr": 1.3445192427911782e-21, "SPICE": {"All": {"pr": 0.16, "re": 0.19047619047619047, "f": 0.17391304347826086, "fn": 17.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.125, "f": 0.15384615384615385, "fn": 7.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.5, "f": 0.37499999999999994, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image is of a cat sitting on top of a wooden chair with its front paws resting on the edge of the seat. The cat's back legs are stretched out and its tail is hanging down behind it. The chair appears to be in a dining room setting, with a table and chairs nearby. The cat's fur is a light brown color with white patches on its face and paws."}, "510122": {"image_id": 510122, "Bleu_1": 0.16666666666414143, "Bleu_2": 0.1240347345873147, "Bleu_3": 0.08967594763235774, "Bleu_4": 0.0581663542105731, "METEOR": 0.2353016081683156, "ROUGE_L": 0.2427860696517413, "CIDEr": 1.609056943133384e-19, "SPICE": {"All": {"pr": 0.2631578947368421, "re": 0.21739130434782608, "f": 0.23809523809523808, "fn": 18.0, "numImages": 1.0, "fp": 14.0, "tp": 5.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.125, "f": 0.11764705882352941, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4444444444444444, "f": 0.4444444444444444, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a group of people standing on a platform overlooking a body of water. They are all looking up at something in the sky, possibly a bird or a kite. The sky is clear and blue with some clouds scattered about. There are trees and buildings visible in the background. The people are dressed in casual clothing and appear to be enjoying the view."}, "484225": {"image_id": 484225, "Bleu_1": 0.20652173912818997, "Bleu_2": 0.11669112204596546, "Bleu_3": 0.05328574482991642, "Bleu_4": 6.421119891496208e-06, "METEOR": 0.17932263190501418, "ROUGE_L": 0.13948170731707316, "CIDEr": 1.8087663678888203e-41, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.125, "f": 0.12000000000000001, "fn": 21.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.23076923076923078, "re": 0.3, "f": 0.2608695652173913, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 3.0}}, "caption": "The image is of a train traveling down the tracks in the snow. The train is made up of several cars, including a locomotive and several passenger cars. The locomotive is yellow and black, with the words \"Union Pacific\" written on the side. The passenger cars are also yellow and black, with windows on the sides and tops. The train is moving quickly through the snowy landscape, with steam coming out of the locomotive's stack. There are trees and buildings visible in the background, and the sky is cloudy and gray."}, "271772": {"image_id": 271772, "Bleu_1": 0.18181818181542703, "Bleu_2": 0.1295500551242812, "Bleu_3": 0.08064455973560596, "Bleu_4": 0.05371507285498268, "METEOR": 0.20052099755060138, "ROUGE_L": 0.21631205673758866, "CIDEr": 1.2784013194675489e-19, "SPICE": {"All": {"pr": 0.09859154929577464, "re": 0.4117647058823529, "f": 0.1590909090909091, "fn": 10.0, "numImages": 1.0, "fp": 64.0, "tp": 7.0}, "Relation": {"pr": 0.034482758620689655, "re": 0.1111111111111111, "f": 0.05263157894736842, "fn": 8.0, "numImages": 1.0, "fp": 28.0, "tp": 1.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.07692307692307693, "re": 0.3333333333333333, "f": 0.125, "fn": 2.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Object": {"pr": 0.1724137931034483, "re": 1.0, "f": 0.29411764705882354, "fn": 0.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}}, "caption": "The image shows a man standing on a beach with his arms outstretched, holding a kite in the air. The kite is made of white and red fabric with a yellow tail and has a sun symbol on it. The man is wearing a white shirt and shorts and has a hat on his head. The sky is blue and there are clouds in the background."}, "264013": {"image_id": 264013, "Bleu_1": 0.3399999999932, "Bleu_2": 0.276272565791757, "Bleu_3": 0.1995881547631518, "Bleu_4": 0.1500917846172271, "METEOR": 0.29766598334923616, "ROUGE_L": 0.3696969696969697, "CIDEr": 1.3965322659963291e-09, "SPICE": {"All": {"pr": 0.22916666666666666, "re": 0.3793103448275862, "f": 0.28571428571428575, "fn": 18.0, "numImages": 1.0, "fp": 37.0, "tp": 11.0}, "Relation": {"pr": 0.058823529411764705, "re": 0.09090909090909091, "f": 0.07142857142857142, "fn": 10.0, "numImages": 1.0, "fp": 16.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.6666666666666666, "f": 0.4444444444444444, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Object": {"pr": 0.4375, "re": 0.7777777777777778, "f": 0.56, "fn": 2.0, "numImages": 1.0, "fp": 9.0, "tp": 7.0}}, "caption": "The image shows a blue and yellow train with the words \"British Rail\" written on the side. The train is parked on a set of tracks next to a building with a clock tower. There are other trains in the background, some of which are also parked on the tracks."}, "138644": {"image_id": 138644, "Bleu_1": 0.2105263157857803, "Bleu_2": 0.10619884880883856, "Bleu_3": 5.896925524327534e-07, "Bleu_4": 1.3959529865204302e-09, "METEOR": 0.1696394012551977, "ROUGE_L": 0.17805020431990656, "CIDEr": 1.0520198565805105e-14, "SPICE": {"All": {"pr": 0.1, "re": 0.17647058823529413, "f": 0.12765957446808512, "fn": 14.0, "numImages": 1.0, "fp": 27.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.25, "f": 0.15384615384615383, "fn": 3.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.2222222222222222, "f": 0.1904761904761905, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a woman holding a banana in her hand while standing in front of a kitchen counter. She has long, straight brown hair and is wearing a black shirt with white stripes. The background is a dark brown color and there are some utensils on the counter, such as a knife and a cutting board."}, "525208": {"image_id": 525208, "Bleu_1": 0.23999999999520005, "Bleu_2": 0.12121830534381622, "Bleu_3": 6.739562828886045e-07, "Bleu_4": 1.59753113202508e-09, "METEOR": 0.1556324334646878, "ROUGE_L": 0.2434435575826682, "CIDEr": 1.6039642559685825e-09, "SPICE": {"All": {"pr": 0.07407407407407407, "re": 0.09090909090909091, "f": 0.08163265306122448, "fn": 20.0, "numImages": 1.0, "fp": 25.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.14285714285714285, "re": 0.25, "f": 0.18181818181818182, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 2.0}}, "caption": "This is an image of a busy intersection at night. There are several cars driving through the intersection, and the streetlights are on. The buildings around the intersection are lit up, and there are people walking on the sidewalk. The sky is dark and there are no clouds in sight."}, "548713": {"image_id": 548713, "Bleu_1": 0.10429447852696752, "Bleu_2": 0.06713085386959489, "Bleu_3": 0.03036263709743101, "Bleu_4": 3.6368435666802648e-06, "METEOR": 0.163678294072008, "ROUGE_L": 0.14637414690392347, "CIDEr": 9.37338148240223e-134, "SPICE": {"All": {"pr": 0.36363636363636365, "re": 0.27586206896551724, "f": 0.3137254901960784, "fn": 21.0, "numImages": 1.0, "fp": 14.0, "tp": 8.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.14285714285714285, "f": 0.15384615384615383, "fn": 6.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3333333333333333, "re": 0.2, "f": 0.25, "fn": 8.0, "numImages": 1.0, "fp": 4.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 0.3333333333333333, "f": 0.5, "fn": 4.0, "numImages": 1.0, "fp": 0.0, "tp": 2.0}, "Object": {"pr": 0.5, "re": 0.4166666666666667, "f": 0.45454545454545453, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 5.0}}, "caption": "The image shows two horses pulling a carriage down the street. There are people walking on the sidewalk and cars parked along the road. The sky is clear and blue with some clouds in the distance.\n\nThe carriage is black and has white wheels. It has a white canopy over it and a black seat inside. The horses are brown and have white manes and tails. They are wearing harnesses and have reins attached to their heads.\n\nThere are people standing on the sidewalk, looking at the carriage as it passes by. Some of them are taking pictures with their phones or cameras. There are also cars parked along the road, with people getting out of them to watch the carriage go by.\n\nThe buildings on either side of the street are tall and made of glass and steel. They have large windows and balconies on the upper floors. There are also trees and planters on the sidewalks, adding greenery to the area."}, "552186": {"image_id": 552186, "Bleu_1": 0.18518518518175586, "Bleu_2": 0.10238251947041137, "Bleu_3": 5.863398711091723e-07, "Bleu_4": 1.410001343225771e-09, "METEOR": 0.18130744523525397, "ROUGE_L": 0.1783625730994152, "CIDEr": 6.061267746787503e-13, "SPICE": {"All": {"pr": 0.08333333333333333, "re": 0.12, "f": 0.09836065573770492, "fn": 22.0, "numImages": 1.0, "fp": 33.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.42857142857142855, "f": 0.2857142857142857, "fn": 4.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image shows a man skateboarding on a concrete skate park. He is wearing blue jeans and a white shirt, and has his arms outstretched as he jumps off the ramp. There are several other people in the background watching him skate. The sky is cloudy and there are some trees in the distance."}, "245513": {"image_id": 245513, "Bleu_1": 0.15999999999786668, "Bleu_2": 0.11389895948877098, "Bleu_3": 0.08108601722276382, "Bleu_4": 0.052164656611632484, "METEOR": 0.1977734193459994, "ROUGE_L": 0.19417917235106869, "CIDEr": 5.083483948203681e-25, "SPICE": {"All": {"pr": 0.15625, "re": 0.2, "f": 0.17543859649122806, "fn": 20.0, "numImages": 1.0, "fp": 27.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.125, "f": 0.14285714285714288, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.26666666666666666, "re": 0.4, "f": 0.32, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "The image shows a giraffe standing in the middle of a grassy area surrounded by trees. The giraffe is looking up at something in the sky, possibly a bird or plane. There are no other animals or people visible in the image. The sky is blue and there are some clouds in it. The grass is green and there are some small plants growing in it. The trees are tall and have leaves on them."}, "156999": {"image_id": 156999, "Bleu_1": 0.2553191489307379, "Bleu_2": 0.1490022319427778, "Bleu_3": 0.07901769277795581, "Bleu_4": 1.0290348647814142e-05, "METEOR": 0.16578455285980265, "ROUGE_L": 0.19869706840390877, "CIDEr": 1.773294695058466e-08, "SPICE": {"All": {"pr": 0.020833333333333332, "re": 0.037037037037037035, "f": 0.02666666666666667, "fn": 26.0, "numImages": 1.0, "fp": 47.0, "tp": 1.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Object": {"pr": 0.05, "re": 0.09090909090909091, "f": 0.06451612903225806, "fn": 10.0, "numImages": 1.0, "fp": 19.0, "tp": 1.0}}, "caption": "The image shows a giraffe standing on the ground, looking at something in its mouth. The giraffe is brown with white spots and has long legs and a long neck. It is standing next to a wooden fence post. There are trees and bushes in the background."}, "406211": {"image_id": 406211, "Bleu_1": 0.2077922077895092, "Bleu_2": 0.15686609568275003, "Bleu_3": 0.09947319224185736, "Bleu_4": 0.060390915246975596, "METEOR": 0.2243506541627439, "ROUGE_L": 0.25007320644216685, "CIDEr": 4.3916001088642676e-24, "SPICE": {"All": {"pr": 0.07142857142857142, "re": 0.1, "f": 0.08333333333333333, "fn": 18.0, "numImages": 1.0, "fp": 26.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2857142857142857, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image is of a living room with a cat sitting on top of a blue cushion in front of a television. There are two windows on the left side of the room, one of which has curtains pulled back to let in natural light. The floor is made of hardwood and there is a rug in the center of the room. The walls are painted white and there are several pieces of artwork hanging on them."}}}