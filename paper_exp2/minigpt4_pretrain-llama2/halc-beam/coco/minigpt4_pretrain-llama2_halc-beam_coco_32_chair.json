{"overall": {"Bleu_1": 0.24040511727066077, "Bleu_2": 0.16467325234898395, "Bleu_3": 0.10749510172921023, "Bleu_4": 0.07175999583934477, "METEOR": 0.21438639139699936, "ROUGE_L": 0.24364140647678262, "CIDEr": 1.3746751234498029e-06, "SPICE": 0.19363194778393722}, "imgToEval": {"281533": {"image_id": 281533, "Bleu_1": 0.22857142856816326, "Bleu_2": 0.1726663389182017, "Bleu_3": 0.12059301315684869, "Bleu_4": 0.09413545432752378, "METEOR": 0.2528561282652983, "ROUGE_L": 0.23529411764705882, "CIDEr": 4.309228397003151e-20, "SPICE": {"All": {"pr": 0.2631578947368421, "re": 0.19230769230769232, "f": 0.2222222222222222, "fn": 21.0, "numImages": 1.0, "fp": 14.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.4444444444444444, "f": 0.47058823529411764, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 4.0}}, "caption": "The image shows a small brown dog sitting in front of a television. The dog is looking up at the television with its ears perked up and tail wagging. The television is showing a nature documentary with images of animals in the wild. There are plants in pots on the floor and a wooden coffee table in front of the television. The walls are painted white and there are no"}, "397773": {"image_id": 397773, "Bleu_1": 0.22916666666189242, "Bleu_2": 0.13965509692979164, "Bleu_3": 0.10834918977671004, "Bleu_4": 0.0867108855424312, "METEOR": 0.2477379122320002, "ROUGE_L": 0.3202099737532808, "CIDEr": 3.161801796038632e-09, "SPICE": {"All": {"pr": 0.19444444444444445, "re": 0.3181818181818182, "f": 0.2413793103448276, "fn": 15.0, "numImages": 1.0, "fp": 29.0, "tp": 7.0}, "Relation": {"pr": 0.08333333333333333, "re": 0.1111111111111111, "f": 0.09523809523809525, "fn": 8.0, "numImages": 1.0, "fp": 11.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1111111111111111, "re": 0.2, "f": 0.14285714285714285, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 1.0, "f": 0.5, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.3333333333333333, "re": 0.625, "f": 0.43478260869565216, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 5.0}}, "caption": "The image is a plate of pasta with meatballs, broccoli, and red sauce. The pasta is made with a mixture of flour, eggs, and water, and is shaped into small, curved noodles. The meatballs are made with ground beef, breadcrumbs, and spices, and are cooked until they are"}, "181859": {"image_id": 181859, "Bleu_1": 0.253731343279795, "Bleu_2": 0.19607178002098768, "Bleu_3": 0.10575857911999645, "Bleu_4": 1.1659816403474453e-05, "METEOR": 0.2198835647219836, "ROUGE_L": 0.23859747153733327, "CIDEr": 4.631828248413754e-19, "SPICE": {"All": {"pr": 0.14285714285714285, "re": 0.23809523809523808, "f": 0.17857142857142855, "fn": 16.0, "numImages": 1.0, "fp": 30.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 13.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.2857142857142857, "f": 0.26666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.21428571428571427, "re": 0.6, "f": 0.3157894736842105, "fn": 2.0, "numImages": 1.0, "fp": 11.0, "tp": 3.0}}, "caption": "The image is of a gray and white cat lying on top of a marble sink in a bathroom. The cat is looking up at the camera with its green eyes. The counter sink has a faucet on the right side and a mirror on the left side. There is a towel hanging on the towel rack next to the sink. The floor is made of white"}, "119939": {"image_id": 119939, "Bleu_1": 0.17741935483584811, "Bleu_2": 0.1078612815361425, "Bleu_3": 0.057879748301363096, "Bleu_4": 7.571500798403399e-06, "METEOR": 0.14184601139785746, "ROUGE_L": 0.13275299238302501, "CIDEr": 4.7411808591736966e-17, "SPICE": {"All": {"pr": 0.10344827586206896, "re": 0.12, "f": 0.11111111111111112, "fn": 22.0, "numImages": 1.0, "fp": 26.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Attribute": {"pr": 0.125, "re": 0.1111111111111111, "f": 0.11764705882352941, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.15384615384615385, "re": 0.2222222222222222, "f": 0.18181818181818185, "fn": 7.0, "numImages": 1.0, "fp": 11.0, "tp": 2.0}}, "caption": "The image shows a group of people skateboarding on the sidewalk in front of a building. The skateboarders are wearing black and white clothing and have their boards in hand. The building is a large, modern structure with large windows and a flat roof. There are trees and other plants growing in pots on the sidewalk. The sky is clear and blue"}, "385320": {"image_id": 385320, "Bleu_1": 0.23333333332944448, "Bleu_2": 0.1886616557167528, "Bleu_3": 0.14531254128144008, "Bleu_4": 0.1127298484678521, "METEOR": 0.24672788289359304, "ROUGE_L": 0.25258799171842644, "CIDEr": 6.443419330394156e-15, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.2631578947368421, "f": 0.2040816326530612, "fn": 14.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.35714285714285715, "re": 1.0, "f": 0.5263157894736842, "fn": 0.0, "numImages": 1.0, "fp": 9.0, "tp": 5.0}}, "caption": "The image shows a young girl sitting on the floor, holding a toothbrush in her mouth. She is wearing a striped shirt and white shorts. The room appears to be a living room with a couch, coffee table, and chairs. There are several toys and books scattered around the room. The girl is smiling and looking directly at the camera."}, "490415": {"image_id": 490415, "Bleu_1": 0.16176470587997407, "Bleu_2": 0.13897909469233014, "Bleu_3": 0.09575440425889309, "Bleu_4": 1.0780545324458568e-05, "METEOR": 0.22941983644461997, "ROUGE_L": 0.26592395253087914, "CIDEr": 3.0238338597844316e-20, "SPICE": {"All": {"pr": 0.14814814814814814, "re": 0.18181818181818182, "f": 0.16326530612244897, "fn": 18.0, "numImages": 1.0, "fp": 23.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.16666666666666666, "f": 0.2, "fn": 5.0, "numImages": 1.0, "fp": 3.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.25, "re": 0.42857142857142855, "f": 0.3157894736842105, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a man standing in a park, holding a camera in his hand. He is facing the sky and appears to be flying a kite. The background is a clear blue sky with some clouds in the distance. The man is wearing a black shirt and jeans, and has a baseball cap on his head. There are several people in the background, some of whom are"}, "256301": {"image_id": 256301, "Bleu_1": 0.253731343279795, "Bleu_2": 0.15187654773650588, "Bleu_3": 0.08920024026570453, "Bleu_4": 0.05770713650297164, "METEOR": 0.19993227398385788, "ROUGE_L": 0.20381861575178994, "CIDEr": 8.522993620376207e-17, "SPICE": {"All": {"pr": 0.18181818181818182, "re": 0.12903225806451613, "f": 0.1509433962264151, "fn": 27.0, "numImages": 1.0, "fp": 18.0, "tp": 4.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.1111111111111111, "f": 0.13333333333333333, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 12.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.3, "f": 0.28571428571428564, "fn": 7.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image is a large neon sign that reads \"Public Time\" in red and white letters. The sign is mounted on a black background and has a clock face in the center with hands pointing to 12 o'clock. There are people standing in front of the sign, looking at it. The image appears to be taken in a city or urban setting, possibly in a public square"}, "448320": {"image_id": 448320, "Bleu_1": 0.2741935483826743, "Bleu_2": 0.20113375108690426, "Bleu_3": 0.12646780187951195, "Bleu_4": 0.07651947377928889, "METEOR": 0.23547577140933712, "ROUGE_L": 0.2821171634121274, "CIDEr": 1.8139822370300715e-14, "SPICE": {"All": {"pr": 0.05128205128205128, "re": 0.08333333333333333, "f": 0.06349206349206349, "fn": 22.0, "numImages": 1.0, "fp": 37.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 11.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Object": {"pr": 0.11764705882352941, "re": 0.25, "f": 0.15999999999999998, "fn": 6.0, "numImages": 1.0, "fp": 15.0, "tp": 2.0}}, "caption": "The image is a bathroom with a sink, toilet, and a shower. The sink is made of white porcelain and has a large mirror above it. The toilet is a standard size and has a seat and lid. The shower is tiled in white and has a shower curtain that is open. There is a window on the wall opposite the sink"}, "373713": {"image_id": 373713, "Bleu_1": 0.21874999999658204, "Bleu_2": 0.16666666666404187, "Bleu_3": 0.11035932143591104, "Bleu_4": 0.08147639524124843, "METEOR": 0.1857604241197933, "ROUGE_L": 0.1840305711987128, "CIDEr": 5.2520464298769715e-18, "SPICE": {"All": {"pr": 0.20588235294117646, "re": 0.30434782608695654, "f": 0.2456140350877193, "fn": 16.0, "numImages": 1.0, "fp": 27.0, "tp": 7.0}, "Relation": {"pr": 0.2, "re": 0.3333333333333333, "f": 0.25, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 3.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.26666666666666666, "re": 0.4444444444444444, "f": 0.33333333333333337, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "The image shows a group of people sitting around a table in a conference room. They are all wearing casual clothing and are engaged in a discussion. One person is standing in front of the table, holding a laptop, while the others are seated and looking at him. The background of the image is a white wall with a large screen displaying a presentation."}, "20059": {"image_id": 20059, "Bleu_1": 0.19672131147218494, "Bleu_2": 0.16195526603310612, "Bleu_3": 0.11007443464912503, "Bleu_4": 0.08235033082562919, "METEOR": 0.24671603777336756, "ROUGE_L": 0.2588859416445623, "CIDEr": 1.290799823758595e-16, "SPICE": {"All": {"pr": 0.1875, "re": 0.4, "f": 0.25531914893617025, "fn": 9.0, "numImages": 1.0, "fp": 26.0, "tp": 6.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 6.0, "tp": 2.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.6666666666666666, "re": 1.0, "f": 0.8, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 2.0}, "Object": {"pr": 0.3076923076923077, "re": 0.8, "f": 0.4444444444444444, "fn": 1.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a group of zebras standing in a grassy area. They are grazing on the green grass and appear to be enjoying themselves. The zebras are black and white with distinctive stripes on their backs. They are standing in a fenced enclosure with rocks and trees in the background. There is a small pond in the foreground with water"}, "117337": {"image_id": 117337, "Bleu_1": 0.17910447760926712, "Bleu_2": 0.12760182301581513, "Bleu_3": 0.09091604859111013, "Bleu_4": 0.05853766845419571, "METEOR": 0.16885220165779988, "ROUGE_L": 0.2061805890873974, "CIDEr": 1.6728940308670133e-18, "SPICE": {"All": {"pr": 0.25925925925925924, "re": 0.25, "f": 0.2545454545454545, "fn": 21.0, "numImages": 1.0, "fp": 20.0, "tp": 7.0}, "Relation": {"pr": 0.2222222222222222, "re": 0.2, "f": 0.2105263157894737, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 2.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.38461538461538464, "f": 0.4, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "This is a collage of images that show different shades of orange and blue. The images are arranged in a collage style, with different shapes and sizes of orange and blue objects in the foreground and background. The images are arranged in a collage style, with different shapes and sizes of orange and blue objects in the foreground and background. The images are arranged in a collage"}, "265472": {"image_id": 265472, "Bleu_1": 0.1960784313687044, "Bleu_2": 0.1252448582145496, "Bleu_3": 6.840816017013768e-07, "Bleu_4": 1.6070175637240875e-09, "METEOR": 0.182096882587512, "ROUGE_L": 0.23416506717850286, "CIDEr": 3.204419107992669e-11, "SPICE": {"All": {"pr": 0.30434782608695654, "re": 0.30434782608695654, "f": 0.30434782608695654, "fn": 16.0, "numImages": 1.0, "fp": 16.0, "tp": 7.0}, "Relation": {"pr": 0.16666666666666666, "re": 0.1, "f": 0.125, "fn": 9.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5, "re": 0.6666666666666666, "f": 0.5714285714285715, "fn": 3.0, "numImages": 1.0, "fp": 6.0, "tp": 6.0}}, "caption": "The image shows a plate with a stack of pancakes topped with bananas, bacon, and maple syrup. The pancakes are golden brown and fluffy, and the bacon is crispy and cooked to perfection. The maple syrup is drizzled over the top of the pancakes and bacon, adding a sweet and sticky"}, "285258": {"image_id": 285258, "Bleu_1": 0.24999999999632352, "Bleu_2": 0.14962640041392813, "Bleu_3": 0.08786869413347967, "Bleu_4": 1.0107582386846654e-05, "METEOR": 0.1841520418259455, "ROUGE_L": 0.17766990291262133, "CIDEr": 3.1239286743764765e-21, "SPICE": {"All": {"pr": 0.25925925925925924, "re": 0.25, "f": 0.2545454545454545, "fn": 21.0, "numImages": 1.0, "fp": 20.0, "tp": 7.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.3, "re": 0.2727272727272727, "f": 0.28571428571428564, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}, "Size": {"pr": 1.0, "re": 0.3333333333333333, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": 0.3333333333333333, "re": 0.3333333333333333, "f": 0.3333333333333333, "fn": 2.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.36363636363636365, "re": 0.4444444444444444, "f": 0.39999999999999997, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 4.0}}, "caption": "The image shows a group of dogs playing in a fenced area. There are five dogs in the image, all of them are small to medium sized and have different colored fur. They are all standing on their hind legs and looking at each other. One of the dogs is wearing a collar and tag. The background of the image is a green grassy area with some trees"}, "574454": {"image_id": 574454, "Bleu_1": 0.19999999999714288, "Bleu_2": 0.10767638041008373, "Bleu_3": 5.545116325017944e-07, "Bleu_4": 1.2630318292495086e-09, "METEOR": 0.1292897231897005, "ROUGE_L": 0.17958783120706576, "CIDEr": 8.282481294496377e-23, "SPICE": {"All": {"pr": 0.21052631578947367, "re": 0.14285714285714285, "f": 0.1702127659574468, "fn": 24.0, "numImages": 1.0, "fp": 15.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 13.0, "numImages": 1.0, "fp": 4.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4444444444444444, "re": 0.4444444444444444, "f": 0.4444444444444444, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 4.0}}, "caption": "The image shows a beach with a large mountain in the background. There are several people on the beach, including one person flying a kite and another person standing on the sand. The sky is clear and blue, with a few clouds scattered across it. The water is calm and clear, with a few waves breaking on the shore. The beach is lined with rocks and there are some trees"}, "541474": {"image_id": 541474, "Bleu_1": 0.2608695652117203, "Bleu_2": 0.17025130614800751, "Bleu_3": 0.08701138391057635, "Bleu_4": 1.1125382292156774e-05, "METEOR": 0.2595163673158783, "ROUGE_L": 0.24238410596026488, "CIDEr": 5.939743308360565e-09, "SPICE": {"All": {"pr": 0.21212121212121213, "re": 0.3684210526315789, "f": 0.2692307692307693, "fn": 12.0, "numImages": 1.0, "fp": 26.0, "tp": 7.0}, "Relation": {"pr": 0.07692307692307693, "re": 0.14285714285714285, "f": 0.1, "fn": 6.0, "numImages": 1.0, "fp": 12.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.2, "f": 0.16666666666666666, "fn": 4.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Object": {"pr": 0.38461538461538464, "re": 0.7142857142857143, "f": 0.5, "fn": 2.0, "numImages": 1.0, "fp": 8.0, "tp": 5.0}}, "caption": "The image shows a person skiing down a snowy slope. The person is wearing a blue and white helmet, black pants, and black boots. They are holding a snowboard and riding down the mountain at high speed. The sky is clear and blue in the background."}, "96241": {"image_id": 96241, "Bleu_1": 0.28070175438104034, "Bleu_2": 0.1415984650784514, "Bleu_3": 0.07143616536435347, "Bleu_4": 9.06442292469654e-06, "METEOR": 0.21928449736302974, "ROUGE_L": 0.23448654585392636, "CIDEr": 1.8234639718977702e-13, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.2, "f": 0.12500000000000003, "fn": 12.0, "numImages": 1.0, "fp": 30.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.09090909090909091, "re": 0.125, "f": 0.10526315789473685, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.16666666666666666, "re": 0.5, "f": 0.25, "fn": 2.0, "numImages": 1.0, "fp": 10.0, "tp": 2.0}}, "caption": "The image shows a group of people standing next to a train that is parked on the tracks. The train is black and has a number 8 painted on the side. There are people standing on both sides of the train, looking at it. The background is a rural area with trees and buildings in the distance."}, "326911": {"image_id": 326911, "Bleu_1": 0.22727272726756204, "Bleu_2": 0.14540168172223333, "Bleu_3": 0.11472822500028991, "Bleu_4": 0.0926433478217715, "METEOR": 0.2562074608630539, "ROUGE_L": 0.285427807486631, "CIDEr": 1.0047831695545988e-07, "SPICE": {"All": {"pr": 0.09090909090909091, "re": 0.1, "f": 0.09523809523809525, "fn": 18.0, "numImages": 1.0, "fp": 20.0, "tp": 2.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.16666666666666666, "f": 0.15384615384615383, "fn": 5.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.3333333333333333, "re": 0.5, "f": 0.4, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 1.0}, "Object": {"pr": 0.1, "re": 0.16666666666666666, "f": 0.125, "fn": 5.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}}, "caption": "The image shows a small black and white dog standing on the sidewalk next to a bicycle. The dog is looking at the camera with its tail wagging. The background of the image is a cobblestone street with trees and buildings in the distance."}, "85292": {"image_id": 85292, "Bleu_1": 0.2656249999958496, "Bleu_2": 0.15905225256758304, "Bleu_3": 0.07417017771502782, "Bleu_4": 9.043562221156522e-06, "METEOR": 0.222572723969875, "ROUGE_L": 0.24448897795591185, "CIDEr": 4.18215647832437e-18, "SPICE": {"All": {"pr": 0.16, "re": 0.23529411764705882, "f": 0.19047619047619052, "fn": 13.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.16666666666666666, "f": 0.16666666666666666, "fn": 5.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 1.0, "f": 1.0, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.6, "f": 0.4, "fn": 2.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a train traveling along a set of train tracks. The train is made up of several cars, including one that is carrying cargo. The train is traveling at a moderate speed and appears to be on a straight section of track. There are trees and buildings in the background, as well as a small amount of cloud cover in the sky."}, "527529": {"image_id": 527529, "Bleu_1": 0.2340425531865098, "Bleu_2": 0.10087498788996685, "Bleu_3": 0.06092349727774221, "Bleu_4": 8.466919880732135e-06, "METEOR": 0.17595075114998024, "ROUGE_L": 0.20847573479152426, "CIDEr": 1.8604145555748739e-09, "SPICE": {"All": {"pr": 0.3448275862068966, "re": 0.35714285714285715, "f": 0.3508771929824561, "fn": 18.0, "numImages": 1.0, "fp": 19.0, "tp": 10.0}, "Relation": {"pr": 0.1111111111111111, "re": 0.09090909090909091, "f": 0.09999999999999999, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.2, "re": 0.25, "f": 0.22222222222222224, "fn": 3.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Size": {"pr": 1.0, "re": 0.5, "f": 0.6666666666666666, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 1.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.5333333333333333, "re": 0.6153846153846154, "f": 0.5714285714285715, "fn": 5.0, "numImages": 1.0, "fp": 7.0, "tp": 8.0}}, "caption": "The image shows a white cat sitting on top of a black bag that is hanging from the handle of a door. The cat is looking up at the camera with its eyes. There are several other bags and boxes on the floor next to the door."}, "152785": {"image_id": 152785, "Bleu_1": 0.22580645160926122, "Bleu_2": 0.17208707350147742, "Bleu_3": 0.12544934931070312, "Bleu_4": 0.0904473413998449, "METEOR": 0.20489851454040772, "ROUGE_L": 0.2596928690892504, "CIDEr": 5.830518413475662e-17, "SPICE": {"All": {"pr": 0.1724137931034483, "re": 0.22727272727272727, "f": 0.19607843137254902, "fn": 17.0, "numImages": 1.0, "fp": 24.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 10.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 3.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.4166666666666667, "re": 0.625, "f": 0.5, "fn": 3.0, "numImages": 1.0, "fp": 7.0, "tp": 5.0}}, "caption": "The image shows a group of elephants walking across a dry, dusty field at sunset. The sky is orange and pink, with clouds in the distance. The elephants are walking in a line, with their trunks held high and their ears flapping in the wind. They are all facing forward, with their eyes fixed on something in the distance. The ground is"}, "516212": {"image_id": 516212, "Bleu_1": 0.23333333332944448, "Bleu_2": 0.19886684640115515, "Bleu_3": 0.17603360743951676, "Bleu_4": 0.15479815867447325, "METEOR": 0.34735941070817583, "ROUGE_L": 0.35390295358649787, "CIDEr": 7.192945566140035e-14, "SPICE": {"All": {"pr": 0.16, "re": 0.14285714285714285, "f": 0.15094339622641512, "fn": 24.0, "numImages": 1.0, "fp": 21.0, "tp": 4.0}, "Relation": {"pr": 0.2, "re": 0.07692307692307693, "f": 0.1111111111111111, "fn": 12.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 5.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.3, "f": 0.2727272727272727, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 3.0}}, "caption": "The image shows a cat sitting on top of a microwave oven in a kitchen. The cat is looking directly at the camera with its eyes. The microwave oven has a plate of food on top of it, and there are several other kitchen appliances in the background, including a refrigerator, stove, and sink. The walls are white and there"}, "565776": {"image_id": 565776, "Bleu_1": 0.2968749999953613, "Bleu_2": 0.19416079083384802, "Bleu_3": 0.10673834308041524, "Bleu_4": 1.1882501667396768e-05, "METEOR": 0.23557816058342554, "ROUGE_L": 0.23597678916827852, "CIDEr": 3.462494315945832e-17, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.21739130434782608, "f": 0.19607843137254902, "fn": 18.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.26666666666666666, "re": 0.4444444444444444, "f": 0.33333333333333337, "fn": 5.0, "numImages": 1.0, "fp": 11.0, "tp": 4.0}}, "caption": "This is an image of a kitchen with white and pink accents. The walls are white and the countertops are made of wood. There is a large island in the center of the room with a sink and stove on it. The floor is made of hardwood and there are two chairs at the island. The cabinets are white and the drawers are pink."}, "208132": {"image_id": 208132, "Bleu_1": 0.2592592592544582, "Bleu_2": 0.17131872291347974, "Bleu_3": 0.1413162504531272, "Bleu_4": 0.12197379410072966, "METEOR": 0.2915219081315538, "ROUGE_L": 0.27949599083619703, "CIDEr": 1.6230220665739442e-11, "SPICE": {"All": {"pr": 0.16666666666666666, "re": 0.17857142857142858, "f": 0.17241379310344826, "fn": 23.0, "numImages": 1.0, "fp": 25.0, "tp": 5.0}, "Relation": {"pr": 0.1, "re": 0.08333333333333333, "f": 0.0909090909090909, "fn": 11.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.4, "f": 0.34782608695652173, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a table with a plate of food on it. There are several glasses of red wine on the table, as well as a bottle of ketchup and a bottle of mustard. There is also a knife and a fork on the table. The background of the image is dark and blurry."}, "289264": {"image_id": 289264, "Bleu_1": 0.19999999999555562, "Bleu_2": 0.11677484162160401, "Bleu_3": 0.0681935615181132, "Bleu_4": 9.321701825196602e-06, "METEOR": 0.15781010830518202, "ROUGE_L": 0.25258799171842644, "CIDEr": 3.924443092021351e-08, "SPICE": {"All": {"pr": 0.10526315789473684, "re": 0.16, "f": 0.12698412698412698, "fn": 21.0, "numImages": 1.0, "fp": 34.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 14.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 10.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Object": {"pr": 0.25, "re": 0.5, "f": 0.3333333333333333, "fn": 4.0, "numImages": 1.0, "fp": 12.0, "tp": 4.0}}, "caption": "The image shows a brown dog sitting on a windowsill looking out at the view. The dog is wearing a collar and appears to be looking out at the rain outside. The background is a dark green color with a few clouds in the sky."}, "18014": {"image_id": 18014, "Bleu_1": 0.3888888888780865, "Bleu_2": 0.23570226038887507, "Bleu_3": 0.11778306710176518, "Bleu_4": 1.4917074526131141e-05, "METEOR": 0.20306147868680155, "ROUGE_L": 0.25673400673400676, "CIDEr": 4.377381896911509e-05, "SPICE": {"All": {"pr": 0.15, "re": 0.13043478260869565, "f": 0.13953488372093023, "fn": 20.0, "numImages": 1.0, "fp": 17.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 9.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 3.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image is a pizza box with a slice of pizza inside. The pizza has a crispy crust and toppings such as vegetables, meat, and cheese. The box is open and the pizza is visible inside."}, "497348": {"image_id": 497348, "Bleu_1": 0.3617021276518787, "Bleu_2": 0.25080812955550935, "Bleu_3": 0.1612614844344404, "Bleu_4": 0.09880634191227361, "METEOR": 0.25005301235805133, "ROUGE_L": 0.30367143746110764, "CIDEr": 5.761817752056652e-08, "SPICE": {"All": {"pr": 0.17857142857142858, "re": 0.38461538461538464, "f": 0.24390243902439027, "fn": 8.0, "numImages": 1.0, "fp": 23.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 4.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.5, "f": 0.25, "fn": 1.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3076923076923077, "re": 0.5714285714285714, "f": 0.4, "fn": 3.0, "numImages": 1.0, "fp": 9.0, "tp": 4.0}}, "caption": "The image shows a street with a sign that reads \"no through traffic\". The road is lined with trees on both sides and there are houses on the left and right sides of the road. The sky is cloudy and there are some clouds in the background."}, "413404": {"image_id": 413404, "Bleu_1": 0.21212121211799817, "Bleu_2": 0.1399300524541518, "Bleu_3": 0.0848966731474613, "Bleu_4": 9.927339321042857e-06, "METEOR": 0.19069363629328312, "ROUGE_L": 0.2150050352467271, "CIDEr": 5.186175352063095e-19, "SPICE": {"All": {"pr": 0.125, "re": 0.17647058823529413, "f": 0.14634146341463414, "fn": 14.0, "numImages": 1.0, "fp": 21.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 6.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 8.0, "tp": 0.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 2.0, "tp": 0.0}, "Color": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.75, "f": 0.4285714285714285, "fn": 1.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a park with several benches and trees. There are people walking on the sidewalks and some are sitting on the benches. The park has a paved path that leads to a building in the background. The building appears to be a restaurant or cafe. There are cars parked on the street in front of the building. The sky is cloudy and there is"}, "530624": {"image_id": 530624, "Bleu_1": 0.1969696969667126, "Bleu_2": 0.12309149097745338, "Bleu_3": 0.06186220039404225, "Bleu_4": 7.829497418405435e-06, "METEOR": 0.20816459438154838, "ROUGE_L": 0.21243781094527364, "CIDEr": 1.1826872913262414e-19, "SPICE": {"All": {"pr": 0.17391304347826086, "re": 0.19047619047619047, "f": 0.1818181818181818, "fn": 17.0, "numImages": 1.0, "fp": 19.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 8.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.16666666666666666, "re": 0.25, "f": 0.2, "fn": 3.0, "numImages": 1.0, "fp": 5.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.3, "re": 0.3333333333333333, "f": 0.3157894736842105, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows a brown and white dog lying under a floral patterned blanket on a bed. The dog is curled up and appears to be sleeping. The bed is covered with a floral patterned quilt and there are pillows on either side of the dog. The room appears to be a bedroom with a window on the left side of the image and a door"}, "234676": {"image_id": 234676, "Bleu_1": 0.2786885245855953, "Bleu_2": 0.22603737782316946, "Bleu_3": 0.15130600583081896, "Bleu_4": 0.10454243843462924, "METEOR": 0.23300959123040266, "ROUGE_L": 0.2857886517438834, "CIDEr": 2.8319102450227573e-15, "SPICE": {"All": {"pr": 0.13333333333333333, "re": 0.2857142857142857, "f": 0.18181818181818182, "fn": 10.0, "numImages": 1.0, "fp": 26.0, "tp": 4.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.1, "re": 1.0, "f": 0.18181818181818182, "fn": 0.0, "numImages": 1.0, "fp": 9.0, "tp": 1.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": 0.5, "re": 1.0, "f": 0.6666666666666666, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 1.0}, "Object": {"pr": 0.2727272727272727, "re": 0.42857142857142855, "f": 0.33333333333333326, "fn": 4.0, "numImages": 1.0, "fp": 8.0, "tp": 3.0}}, "caption": "The image shows a group of people standing on a beach next to a surfboard. The board is lying on the sand, and the people are standing around it, looking at it. There are some waves in the distance, and the sky is clear and blue. The beach is covered in sand, and there are some rocks and shells scattered around."}, "43448": {"image_id": 43448, "Bleu_1": 0.3134328358162174, "Bleu_2": 0.20673858190240743, "Bleu_3": 0.13803733525913842, "Bleu_4": 0.09521599948403056, "METEOR": 0.2192489434007332, "ROUGE_L": 0.2561829211385907, "CIDEr": 1.961618279299402e-12, "SPICE": {"All": {"pr": 0.11538461538461539, "re": 0.2, "f": 0.14634146341463417, "fn": 12.0, "numImages": 1.0, "fp": 23.0, "tp": 3.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 6.0, "numImages": 1.0, "fp": 7.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 2.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 5.0, "tp": 0.0}, "Object": {"pr": 0.3, "re": 0.42857142857142855, "f": 0.3529411764705882, "fn": 4.0, "numImages": 1.0, "fp": 7.0, "tp": 3.0}}, "caption": "The image shows two elephants standing on a rocky beach with trees in the background. The elephants are brown and have large ears and tusks. They are standing next to each other and appear to be looking at something in the distance. The sky is blue and there are clouds in the background. The image is taken from a low angle, giving the impression of being on"}, "37616": {"image_id": 37616, "Bleu_1": 0.25423728813128416, "Bleu_2": 0.20936574503555552, "Bleu_3": 0.1664801848859185, "Bleu_4": 0.13473794001466768, "METEOR": 0.233277018201874, "ROUGE_L": 0.28586305649570426, "CIDEr": 7.135361027201427e-14, "SPICE": {"All": {"pr": 0.26666666666666666, "re": 0.27586206896551724, "f": 0.2711864406779661, "fn": 21.0, "numImages": 1.0, "fp": 22.0, "tp": 8.0}, "Relation": {"pr": 0.14285714285714285, "re": 0.14285714285714285, "f": 0.14285714285714285, "fn": 6.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Attribute": {"pr": 0.14285714285714285, "re": 0.1111111111111111, "f": 0.125, "fn": 8.0, "numImages": 1.0, "fp": 6.0, "tp": 1.0}, "Size": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 1.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": 0.2, "re": 0.5, "f": 0.28571428571428575, "fn": 1.0, "numImages": 1.0, "fp": 4.0, "tp": 1.0}, "Object": {"pr": 0.375, "re": 0.46153846153846156, "f": 0.41379310344827586, "fn": 7.0, "numImages": 1.0, "fp": 10.0, "tp": 6.0}}, "caption": "The image shows a man standing in a living room with a couch, coffee table, and chairs. There is a large window on the wall behind him, and a mirror on the wall opposite him. The man is wearing a pair of sunglasses and a white shirt. He is looking at something on the floor in front of him."}, "516508": {"image_id": 516508, "Bleu_1": 0.23913043477741028, "Bleu_2": 0.12626174790809142, "Bleu_3": 0.07129027762233402, "Bleu_4": 9.580884740728586e-06, "METEOR": 0.1685714285714286, "ROUGE_L": 0.2121001390820584, "CIDEr": 7.431506780818802e-09, "SPICE": {"All": {"pr": 0.1388888888888889, "re": 0.3125, "f": 0.1923076923076923, "fn": 11.0, "numImages": 1.0, "fp": 31.0, "tp": 5.0}, "Relation": {"pr": 0.0, "re": 0.0, "f": 0.0, "fn": 7.0, "numImages": 1.0, "fp": 9.0, "tp": 0.0}, "Cardinality": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 0.0, "tp": 0.0}, "Attribute": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 11.0, "tp": 0.0}, "Size": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Color": {"pr": NaN, "re": NaN, "f": NaN, "fn": 0.0, "numImages": 1.0, "fp": 1.0, "tp": 0.0}, "Object": {"pr": 0.3125, "re": 0.5555555555555556, "f": 0.39999999999999997, "fn": 4.0, "numImages": 1.0, "fp": 11.0, "tp": 5.0}}, "caption": "This image shows a large, ornate clock hanging on the wall of a church. The clock has two hands and is surrounded by intricate carvings on the walls. The room is dimly lit, with only a small amount of light coming from the stained glass windows."}}}