<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Wie konvertiert man ein ü§ó Transformers-Modell in TensorFlow?

Die Tatsache, dass mehrere Frameworks f√ºr die Verwendung mit ü§ó Transformers zur Verf√ºgung stehen, gibt Ihnen die Flexibilit√§t, deren St√§rken beim Entwurf Ihrer Anwendung auszuspielen.
Ihre Anwendung zu entwerfen, aber das bedeutet auch, dass die Kompatibilit√§t f√ºr jedes Modell einzeln hinzugef√ºgt werden muss. Die gute Nachricht ist, dass
das Hinzuf√ºgen von TensorFlow-Kompatibilit√§t zu einem bestehenden Modell einfacher ist als [das Hinzuf√ºgen eines neuen Modells von Grund auf](add_new_model)!
Ob Sie ein tieferes Verst√§ndnis f√ºr gro√üe TensorFlow-Modelle haben m√∂chten, einen wichtigen Open-Source-Beitrag leisten oder
TensorFlow f√ºr das Modell Ihrer Wahl aktivieren wollen, dieser Leitfaden ist f√ºr Sie.

Dieser Leitfaden bef√§higt Sie, ein Mitglied unserer Gemeinschaft, TensorFlow-Modellgewichte und/oder
Architekturen beizusteuern, die in ü§ó Transformers verwendet werden sollen, und zwar mit minimaler Betreuung durch das Hugging Face Team. Das Schreiben eines neuen Modells
ist keine Kleinigkeit, aber ich hoffe, dass dieser Leitfaden dazu beitr√§gt, dass es weniger eine Achterbahnfahrt üé¢ und mehr ein Spaziergang im Park üö∂ ist.
Die Nutzung unserer kollektiven Erfahrungen ist absolut entscheidend, um diesen Prozess immer einfacher zu machen, und deshalb m√∂chten wir
ermutigen Sie daher, Verbesserungsvorschl√§ge f√ºr diesen Leitfaden zu machen!

Bevor Sie tiefer eintauchen, empfehlen wir Ihnen, die folgenden Ressourcen zu lesen, wenn Sie neu in ü§ó Transformers sind:
- [Allgemeiner √úberblick √ºber ü§ó Transformers](add_new_model#general-overview-of-transformers)
- [Die TensorFlow-Philosophie von Hugging Face](https://huggingface.co/blog/tensorflow-philosophy)

Im Rest dieses Leitfadens werden Sie lernen, was n√∂tig ist, um eine neue TensorFlow Modellarchitektur hinzuzuf√ºgen, die
Verfahren zur Konvertierung von PyTorch in TensorFlow-Modellgewichte und wie Sie Unstimmigkeiten zwischen ML
Frameworks. Legen Sie los!

<Tip>

Sind Sie unsicher, ob das Modell, das Sie verwenden m√∂chten, bereits eine entsprechende TensorFlow-Architektur hat?

&nbsp;

√úberpr√ºfen Sie das Feld `model_type` in der `config.json` des Modells Ihrer Wahl
([Beispiel](https://huggingface.co/bert-base-uncased/blob/main/config.json#L14)). Wenn der entsprechende Modellordner in
ü§ó Transformers eine Datei hat, deren Name mit "modeling_tf" beginnt, bedeutet dies, dass es eine entsprechende TensorFlow
Architektur hat ([Beispiel](https://github.com/huggingface/transformers/tree/main/src/transformers/models/bert)).

</Tip>


## Schritt-f√ºr-Schritt-Anleitung zum Hinzuf√ºgen von TensorFlow-Modellarchitektur-Code

Es gibt viele M√∂glichkeiten, eine gro√üe Modellarchitektur zu entwerfen, und viele M√∂glichkeiten, diesen Entwurf zu implementieren. Wie auch immer,
Sie erinnern sich vielleicht an unseren [allgemeinen √úberblick √ºber ü§ó Transformers](add_new_model#general-overview-of-transformers)
wissen, dass wir ein meinungsfreudiger Haufen sind - die Benutzerfreundlichkeit von ü§ó Transformers h√§ngt von konsistenten Designentscheidungen ab. Aus
Erfahrung k√∂nnen wir Ihnen ein paar wichtige Dinge √ºber das Hinzuf√ºgen von TensorFlow-Modellen sagen:

- Erfinden Sie das Rad nicht neu! In den meisten F√§llen gibt es mindestens zwei Referenzimplementierungen, die Sie √ºberpr√ºfen sollten: das
PyTorch-√Ñquivalent des Modells, das Sie implementieren, und andere TensorFlow-Modelle f√ºr dieselbe Klasse von Problemen.
- Gute Modellimplementierungen √ºberleben den Test der Zeit. Dies geschieht nicht, weil der Code h√ºbsch ist, sondern eher
sondern weil der Code klar, einfach zu debuggen und darauf aufzubauen ist. Wenn Sie den Maintainern das Leben mit Ihrer
TensorFlow-Implementierung leicht machen, indem Sie die gleichen Muster wie in anderen TensorFlow-Modellen nachbilden und die Abweichung
zur PyTorch-Implementierung minimieren, stellen Sie sicher, dass Ihr Beitrag lange Bestand haben wird.
- Bitten Sie um Hilfe, wenn Sie nicht weiterkommen! Das ü§ó Transformers-Team ist da, um zu helfen, und wir haben wahrscheinlich L√∂sungen f√ºr die gleichen
Probleme gefunden, vor denen Sie stehen.

Hier finden Sie einen √úberblick √ºber die Schritte, die zum Hinzuf√ºgen einer TensorFlow-Modellarchitektur erforderlich sind:
1. W√§hlen Sie das Modell, das Sie konvertieren m√∂chten
2. Bereiten Sie die Transformers-Entwicklungsumgebung vor.
3. (Optional) Verstehen Sie die theoretischen Aspekte und die bestehende Implementierung
4. Implementieren Sie die Modellarchitektur
5. Implementieren Sie Modelltests
6. Reichen Sie den Pull-Antrag ein
7. (Optional) Erstellen Sie Demos und teilen Sie diese mit der Welt

### 1.-3. Bereiten Sie Ihren Modellbeitrag vor

**1. W√§hlen Sie das Modell, das Sie konvertieren m√∂chten**

Beginnen wir mit den Grundlagen: Als erstes m√ºssen Sie die Architektur kennen, die Sie konvertieren m√∂chten. Wenn Sie
Sie sich nicht auf eine bestimmte Architektur festgelegt haben, ist es eine gute M√∂glichkeit, das ü§ó Transformers-Team um Vorschl√§ge zu bitten.
Wir werden Sie zu den wichtigsten Architekturen f√ºhren, die auf der TensorFlow-Seite noch fehlen.
Seite fehlen. Wenn das spezifische Modell, das Sie mit TensorFlow verwenden m√∂chten, bereits eine Implementierung der TensorFlow-Architektur in
ü§ó Transformers, aber es fehlen Gewichte, k√∂nnen Sie direkt in den
Abschnitt [Gewichtskonvertierung](#adding-tensorflow-weights-to-hub)
auf dieser Seite.

Der Einfachheit halber wird im Rest dieser Anleitung davon ausgegangen, dass Sie sich entschieden haben, mit der TensorFlow-Version von
*BrandNewBert* (dasselbe Beispiel wie in der [Anleitung](add_new_model), um ein neues Modell von Grund auf hinzuzuf√ºgen).

<Tip>

Bevor Sie mit der Arbeit an einer TensorFlow-Modellarchitektur beginnen, sollten Sie sich vergewissern, dass es keine laufenden Bem√ºhungen in dieser Richtung gibt.
Sie k√∂nnen nach `BrandNewBert` auf der
[pull request GitHub page](https://github.com/huggingface/transformers/pulls?q=is%3Apr), um zu best√§tigen, dass es keine
TensorFlow-bezogene Pull-Anfrage gibt.

</Tip>


**2. Transformers-Entwicklungsumgebung vorbereiten**

Nachdem Sie die Modellarchitektur ausgew√§hlt haben, √∂ffnen Sie einen PR-Entwurf, um Ihre Absicht zu signalisieren, daran zu arbeiten. Folgen Sie den
Anweisungen, um Ihre Umgebung einzurichten und einen PR-Entwurf zu √∂ffnen.

1. Forken Sie das [repository](https://github.com/huggingface/transformers), indem Sie auf der Seite des Repositorys auf die Schaltfl√§che 'Fork' klicken.
   Seite des Repositorys klicken. Dadurch wird eine Kopie des Codes unter Ihrem GitHub-Benutzerkonto erstellt.

2. Klonen Sie Ihren `transformers` Fork auf Ihre lokale Festplatte und f√ºgen Sie das Basis-Repository als Remote hinzu:

```bash
git clone https://github.com/[your Github handle]/transformers.git
cd transformers
git remote add upstream https://github.com/huggingface/transformers.git
```

3. Richten Sie eine Entwicklungsumgebung ein, indem Sie z.B. den folgenden Befehl ausf√ºhren:

```bash
python -m venv .env
source .env/bin/activate
pip install -e ".[dev]"
```

Abh√§ngig von Ihrem Betriebssystem und da die Anzahl der optionalen Abh√§ngigkeiten von Transformers w√§chst, kann es sein, dass Sie bei diesem Befehl einen
Fehler mit diesem Befehl erhalten. Wenn das der Fall ist, stellen Sie sicher, dass Sie TensorFlow installieren und dann ausf√ºhren:

```bash
pip install -e ".[quality]"
```

**Hinweis:** Sie m√ºssen CUDA nicht installiert haben. Es reicht aus, das neue Modell auf der CPU laufen zu lassen.

4. Erstellen Sie eine Verzweigung mit einem beschreibenden Namen von Ihrer Hauptverzweigung

```bash
git checkout -b add_tf_brand_new_bert
```

5. Abrufen und zur√ºcksetzen auf die aktuelle Hauptversion

```bash
git fetch upstream
git rebase upstream/main
```

6. F√ºgen Sie eine leere `.py` Datei in `transformers/src/models/brandnewbert/` mit dem Namen `modeling_tf_brandnewbert.py` hinzu. Dies wird
Ihre TensorFlow-Modelldatei sein.

7. √úbertragen Sie die √Ñnderungen auf Ihr Konto mit:

```bash
git add .
git commit -m "initial commit"
git push -u origin add_tf_brand_new_bert
```

8. Wenn Sie zufrieden sind, gehen Sie auf die Webseite Ihrer Abspaltung auf GitHub. Klicken Sie auf "Pull request". Stellen Sie sicher, dass Sie das
   GitHub-Handle einiger Mitglieder des Hugging Face-Teams als Reviewer hinzuzuf√ºgen, damit das Hugging Face-Team √ºber zuk√ºnftige √Ñnderungen informiert wird.
   zuk√ºnftige √Ñnderungen benachrichtigt wird.

9. √Ñndern Sie den PR in einen Entwurf, indem Sie auf der rechten Seite der GitHub-Pull-Request-Webseite auf "In Entwurf umwandeln" klicken.


Jetzt haben Sie eine Entwicklungsumgebung eingerichtet, um *BrandNewBert* nach TensorFlow in ü§ó Transformers zu portieren.


**3. (Optional) Verstehen Sie die theoretischen Aspekte und die bestehende Implementierung**

Sie sollten sich etwas Zeit nehmen, um die Arbeit von *BrandNewBert* zu lesen, falls eine solche Beschreibung existiert. M√∂glicherweise gibt es gro√üe
Abschnitte des Papiers, die schwer zu verstehen sind. Wenn das der Fall ist, ist das in Ordnung - machen Sie sich keine Sorgen! Das Ziel ist
ist es nicht, ein tiefes theoretisches Verst√§ndnis des Papiers zu erlangen, sondern die notwendigen Informationen zu extrahieren, um
das Modell mit Hilfe von TensorFlow effektiv in ü§ó Transformers neu zu implementieren. Das hei√üt, Sie m√ºssen nicht zu viel Zeit auf die
viel Zeit auf die theoretischen Aspekte verwenden, sondern sich lieber auf die praktischen Aspekte konzentrieren, n√§mlich auf die bestehende Modelldokumentation
Seite (z.B. [model docs for BERT](model_doc/bert)).

Nachdem Sie die Grundlagen der Modelle, die Sie implementieren wollen, verstanden haben, ist es wichtig, die bestehende
Implementierung zu verstehen. Dies ist eine gute Gelegenheit, sich zu vergewissern, dass eine funktionierende Implementierung mit Ihren Erwartungen an das
Modell entspricht, und um technische Herausforderungen auf der TensorFlow-Seite vorauszusehen.

Es ist ganz nat√ºrlich, dass Sie sich von der Menge an Informationen, die Sie gerade aufgesogen haben, √ºberw√§ltigt f√ºhlen. Es ist
Es ist definitiv nicht erforderlich, dass Sie in dieser Phase alle Facetten des Modells verstehen. Dennoch empfehlen wir Ihnen dringend
ermutigen wir Sie, alle dringenden Fragen in unserem [Forum](https://discuss.huggingface.co/) zu kl√§ren.


### 4. Implementierung des Modells

Jetzt ist es an der Zeit, endlich mit dem Programmieren zu beginnen. Als Ausgangspunkt empfehlen wir die PyTorch-Datei selbst: Kopieren Sie den Inhalt von
modeling_brand_new_bert.py` in `src/transformers/models/brand_new_bert/` nach
modeling_tf_brand_new_bert.py`. Das Ziel dieses Abschnitts ist es, die Datei zu √§ndern und die Importstruktur von
ü§ó Transformers zu aktualisieren, so dass Sie `TFBrandNewBert` und
`TFBrandNewBert.from_pretrained(model_repo, from_pt=True)` erfolgreich ein funktionierendes TensorFlow *BrandNewBert* Modell l√§dt.

Leider gibt es kein Rezept, um ein PyTorch-Modell in TensorFlow zu konvertieren. Sie k√∂nnen jedoch unsere Auswahl an
Tipps befolgen, um den Prozess so reibungslos wie m√∂glich zu gestalten:
- Stellen Sie `TF` dem Namen aller Klassen voran (z.B. wird `BrandNewBert` zu `TFBrandNewBert`).
- Die meisten PyTorch-Operationen haben einen direkten TensorFlow-Ersatz. Zum Beispiel entspricht `torch.nn.Linear` der Klasse
  `tf.keras.layers.Dense`, `torch.nn.Dropout` entspricht `tf.keras.layers.Dropout`, usw. Wenn Sie sich nicht sicher sind
  √ºber eine bestimmte Operation nicht sicher sind, k√∂nnen Sie die [TensorFlow-Dokumentation](https://www.tensorflow.org/api_docs/python/tf)
  oder die [PyTorch-Dokumentation](https://pytorch.org/docs/stable/).
- Suchen Sie nach Mustern in der Codebasis von ü§ó Transformers. Wenn Sie auf eine bestimmte Operation sto√üen, f√ºr die es keinen direkten Ersatz gibt
   Ersatz hat, stehen die Chancen gut, dass jemand anderes bereits das gleiche Problem hatte.
- Behalten Sie standardm√§√üig die gleichen Variablennamen und die gleiche Struktur wie in PyTorch bei. Dies erleichtert die Fehlersuche, die Verfolgung von
   Probleme zu verfolgen und sp√§tere Korrekturen vorzunehmen.
- Einige Ebenen haben in jedem Framework unterschiedliche Standardwerte. Ein bemerkenswertes Beispiel ist die Schicht f√ºr die Batch-Normalisierung
   epsilon (`1e-5` in [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d)
   und `1e-3` in [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)).
   Pr√ºfen Sie die Dokumentation genau!
- Die Variablen `nn.Parameter` von PyTorch m√ºssen in der Regel innerhalb von TF Layer's `build()` initialisiert werden. Siehe das folgende
   Beispiel: [PyTorch](https://github.com/huggingface/transformers/blob/655f72a6896c0533b1bdee519ed65a059c2425ac/src/transformers/models/vit_mae/modeling_vit_mae.py#L212) /
   [TensorFlow](https://github.com/huggingface/transformers/blob/655f72a6896c0533b1bdee519ed65a059c2425ac/src/transformers/models/vit_mae/modeling_tf_vit_mae.py#L220)
- Wenn das PyTorch-Modell ein `#copied from ...` am Anfang einer Funktion hat, stehen die Chancen gut, dass Ihr TensorFlow-Modell diese Funktion auch
   diese Funktion von der Architektur ausleihen kann, von der sie kopiert wurde, vorausgesetzt, es hat eine TensorFlow-Architektur.
- Die korrekte Zuweisung des Attributs `name` in TensorFlow-Funktionen ist entscheidend, um das `from_pt=True` Gewicht zu erreichen
   Cross-Loading. Name" ist fast immer der Name der entsprechenden Variablen im PyTorch-Code. Wenn `name` nicht
   nicht richtig gesetzt ist, sehen Sie dies in der Fehlermeldung beim Laden der Modellgewichte.
- Die Logik der Basismodellklasse, `BrandNewBertModel`, befindet sich in `TFBrandNewBertMainLayer`, einer Keras
   Schicht-Unterklasse ([Beispiel](https://github.com/huggingface/transformers/blob/4fd32a1f499e45f009c2c0dea4d81c321cba7e02/src/transformers/models/bert/modeling_tf_bert.py#L719)).
   TFBrandNewBertModel" ist lediglich ein Wrapper f√ºr diese Schicht.
- Keras-Modelle m√ºssen erstellt werden, um die vorher trainierten Gewichte zu laden. Aus diesem Grund muss `TFBrandNewBertPreTrainedModel`
   ein Beispiel f√ºr die Eingaben in das Modell enthalten, die `dummy_inputs`
   ([Beispiel](https://github.com/huggingface/transformers/blob/4fd32a1f499e45f009c2c0dea4d81c321cba7e02/src/transformers/models/bert/modeling_tf_bert.py#L916)).
- Wenn Sie nicht weiterkommen, fragen Sie nach Hilfe - wir sind f√ºr Sie da! ü§ó

Neben der Modelldatei selbst m√ºssen Sie auch die Verweise auf die Modellklassen und die zugeh√∂rigen
Dokumentationsseiten hinzuf√ºgen. Sie k√∂nnen diesen Teil ganz nach den Mustern in anderen PRs erledigen
([Beispiel](https://github.com/huggingface/transformers/pull/18020/files)). Hier ist eine Liste der erforderlichen manuellen
√Ñnderungen:
- F√ºgen Sie alle √∂ffentlichen Klassen von *BrandNewBert* in `src/transformers/__init__.py` ein.
- F√ºgen Sie *BrandNewBert* Klassen zu den entsprechenden Auto Klassen in `src/transformers/models/auto/modeling_tf_auto.py` hinzu.
- F√ºgen Sie die *BrandNewBert* zugeh√∂rigen Klassen f√ºr tr√§ges Laden in `src/transformers/utils/dummy_tf_objects.py` hinzu.
- Aktualisieren Sie die Importstrukturen f√ºr die √∂ffentlichen Klassen in `src/transformers/models/brand_new_bert/__init__.py`.
- F√ºgen Sie die Dokumentationszeiger auf die √∂ffentlichen Methoden von *BrandNewBert* in `docs/source/de/model_doc/brand_new_bert.md` hinzu.
- F√ºgen Sie sich selbst zur Liste der Mitwirkenden an *BrandNewBert* in `docs/source/de/model_doc/brand_new_bert.md` hinzu.
- F√ºgen Sie schlie√ülich ein gr√ºnes H√§kchen ‚úÖ in der TensorFlow-Spalte von *BrandNewBert* in `docs/source/de/index.md` hinzu.

Wenn Sie mit Ihrer Implementierung zufrieden sind, f√ºhren Sie die folgende Checkliste aus, um zu best√§tigen, dass Ihre Modellarchitektur
fertig ist:
1. Alle Schichten, die sich zur Trainingszeit anders verhalten (z.B. Dropout), werden mit einem `Training` Argument aufgerufen, das
von den Top-Level-Klassen weitergegeben wird
2. Sie haben `#copied from ...` verwendet, wann immer es m√∂glich war.
3. Die Funktion `TFBrandNewBertMainLayer` und alle Klassen, die sie verwenden, haben ihre Funktion `call` mit `@unpack_inputs` dekoriert
4. TFBrandNewBertMainLayer` ist mit `@keras_serializable` dekoriert
5. Ein TensorFlow-Modell kann aus PyTorch-Gewichten mit `TFBrandNewBert.from_pretrained(model_repo, from_pt=True)` geladen werden.
6. Sie k√∂nnen das TensorFlow Modell mit dem erwarteten Eingabeformat aufrufen


### 5. Modell-Tests hinzuf√ºgen

Hurra, Sie haben ein TensorFlow-Modell implementiert! Jetzt ist es an der Zeit, Tests hinzuzuf√ºgen, um sicherzustellen, dass sich Ihr Modell wie erwartet verh√§lt.
erwartet. Wie im vorigen Abschnitt schlagen wir vor, dass Sie zun√§chst die Datei `test_modeling_brand_new_bert.py` in
`tests/models/brand_new_bert/` in die Datei `test_modeling_tf_brand_new_bert.py` zu kopieren und dann die notwendigen
TensorFlow-Ersetzungen vornehmen. F√ºr den Moment sollten Sie in allen Aufrufen von `.from_pretrained()` das Flag `from_pt=True` verwenden, um die
die vorhandenen PyTorch-Gewichte zu laden.

Wenn Sie damit fertig sind, kommt der Moment der Wahrheit: F√ºhren Sie die Tests durch! üò¨

```bash
NVIDIA_TF32_OVERRIDE=0 RUN_SLOW=1 RUN_PT_TF_CROSS_TESTS=1 \
py.test -vv tests/models/brand_new_bert/test_modeling_tf_brand_new_bert.py
```

Das wahrscheinlichste Ergebnis ist, dass Sie eine Reihe von Fehlern sehen werden. Machen Sie sich keine Sorgen, das ist zu erwarten! Das Debuggen von ML-Modellen ist
notorisch schwierig, und der Schl√ºssel zum Erfolg ist Geduld (und `breakpoint()`). Nach unserer Erfahrung sind die schwierigsten
Probleme aus subtilen Unstimmigkeiten zwischen ML-Frameworks, zu denen wir am Ende dieses Leitfadens ein paar Hinweise geben.
In anderen F√§llen kann es sein, dass ein allgemeiner Test nicht direkt auf Ihr Modell anwendbar ist; in diesem Fall empfehlen wir eine √úberschreibung
auf der Ebene der Modelltestklasse. Z√∂gern Sie nicht, in Ihrem Entwurf einer Pull-Anfrage um Hilfe zu bitten, wenn
Sie nicht weiterkommen.

Wenn alle Tests erfolgreich waren, k√∂nnen Sie Ihr Modell in die ü§ó Transformers-Bibliothek aufnehmen! üéâ

### 6.-7. Stellen Sie sicher, dass jeder Ihr Modell verwenden kann

**6. Reichen Sie den Pull Request ein**

Sobald Sie mit der Implementierung und den Tests fertig sind, ist es an der Zeit, eine Pull-Anfrage einzureichen. Bevor Sie Ihren Code einreichen,
f√ºhren Sie unser Dienstprogramm zur Codeformatierung, `make fixup` ü™Ñ, aus. Damit werden automatisch alle Formatierungsfehler behoben, die dazu f√ºhren w√ºrden, dass
unsere automatischen Pr√ºfungen fehlschlagen w√ºrden.

Nun ist es an der Zeit, Ihren Entwurf einer Pull-Anfrage in eine echte Pull-Anfrage umzuwandeln. Klicken Sie dazu auf die Schaltfl√§che "Bereit f√ºr
Review" und f√ºgen Sie Joao (`@gante`) und Matt (`@Rocketknight1`) als Reviewer hinzu. Eine Modell-Pull-Anfrage ben√∂tigt
mindestens 3 Reviewer, aber sie werden sich darum k√ºmmern, geeignete zus√§tzliche Reviewer f√ºr Ihr Modell zu finden.

Nachdem alle Gutachter mit dem Stand Ihres PR zufrieden sind, entfernen Sie als letzten Aktionspunkt das Flag `from_pt=True` in
.from_pretrained()-Aufrufen zu entfernen. Da es keine TensorFlow-Gewichte gibt, m√ºssen Sie sie hinzuf√ºgen! Lesen Sie den Abschnitt
unten, um zu erfahren, wie Sie dies tun k√∂nnen.

Wenn schlie√ülich die TensorFlow-Gewichte zusammengef√ºhrt werden, Sie mindestens 3 Genehmigungen von Pr√ºfern haben und alle CI-Checks gr√ºn sind
gr√ºn sind, √ºberpr√ºfen Sie die Tests ein letztes Mal lokal

```bash
NVIDIA_TF32_OVERRIDE=0 RUN_SLOW=1 RUN_PT_TF_CROSS_TESTS=1 \
py.test -vv tests/models/brand_new_bert/test_modeling_tf_brand_new_bert.py
```

und wir werden Ihren PR zusammenf√ºhren! Herzlichen Gl√ºckwunsch zu dem Meilenstein üéâ.

**7. (Optional) Erstellen Sie Demos und teilen Sie sie mit der Welt**

Eine der schwierigsten Aufgaben bei Open-Source ist die Entdeckung. Wie k√∂nnen die anderen Benutzer von der Existenz Ihres
fabelhaften TensorFlow-Beitrags erfahren? Mit der richtigen Kommunikation, nat√ºrlich! üì£

Es gibt vor allem zwei M√∂glichkeiten, Ihr Modell mit der Community zu teilen:
- Erstellen Sie Demos. Dazu geh√∂ren Gradio-Demos, Notebooks und andere unterhaltsame M√∂glichkeiten, Ihr Modell vorzuf√ºhren. Wir raten Ihnen
   ermutigen Sie, ein Notizbuch zu unseren [community-driven demos](https://huggingface.co/docs/transformers/community) hinzuzuf√ºgen.
- Teilen Sie Geschichten in sozialen Medien wie Twitter und LinkedIn. Sie sollten stolz auf Ihre Arbeit sein und sie mit der
   Ihre Leistung mit der Community teilen - Ihr Modell kann nun von Tausenden von Ingenieuren und Forschern auf der ganzen Welt genutzt werden
   der Welt genutzt werden üåç! Wir werden Ihre Beitr√§ge gerne retweeten und Ihnen helfen, Ihre Arbeit mit der Community zu teilen.


## Hinzuf√ºgen von TensorFlow-Gewichten zum ü§ó Hub

Unter der Annahme, dass die TensorFlow-Modellarchitektur in ü§ó Transformers verf√ºgbar ist, ist die Umwandlung von PyTorch-Gewichten in
TensorFlow-Gewichte ist ein Kinderspiel!

Hier sehen Sie, wie es geht:
1. Stellen Sie sicher, dass Sie in Ihrem Terminal bei Ihrem Hugging Face Konto angemeldet sind. Sie k√∂nnen sich mit dem folgenden Befehl anmelden
   `huggingface-cli login` (Ihre Zugangstoken finden Sie [hier](https://huggingface.co/settings/tokens))
2. F√ºhren Sie `transformers-cli pt-to-tf --model-name foo/bar` aus, wobei `foo/bar` der Name des Modell-Repositorys ist
   ist, das die PyTorch-Gewichte enth√§lt, die Sie konvertieren m√∂chten.
3. Markieren Sie `@joaogante` und `@Rocketknight1` in dem ü§ó Hub PR, den der obige Befehl gerade erstellt hat

Das war's! üéâ


## Fehlersuche in verschiedenen ML-Frameworks üêõ

Irgendwann, wenn Sie eine neue Architektur hinzuf√ºgen oder TensorFlow-Gewichte f√ºr eine bestehende Architektur erstellen, werden Sie
sto√üen Sie vielleicht auf Fehler, die sich √ºber Unstimmigkeiten zwischen PyTorch und TensorFlow beschweren. Sie k√∂nnten sich sogar dazu entschlie√üen, den
Modellarchitektur-Code f√ºr die beiden Frameworks zu √∂ffnen, und stellen fest, dass sie identisch aussehen. Was ist denn da los? ü§î

Lassen Sie uns zun√§chst dar√ºber sprechen, warum es wichtig ist, diese Diskrepanzen zu verstehen. Viele Community-Mitglieder werden ü§ó
Transformers-Modelle und vertrauen darauf, dass sich unsere Modelle wie erwartet verhalten. Wenn es eine gro√üe Diskrepanz gibt
zwischen den beiden Frameworks auftritt, bedeutet dies, dass das Modell nicht der Referenzimplementierung f√ºr mindestens eines der Frameworks folgt.
der Frameworks folgt. Dies kann zu stillen Fehlern f√ºhren, bei denen das Modell zwar l√§uft, aber eine schlechte Leistung aufweist. Dies ist
wohl schlimmer als ein Modell, das √ºberhaupt nicht l√§uft! Aus diesem Grund streben wir an, dass die Abweichung zwischen den Frameworks kleiner als
1e-5" in allen Phasen des Modells.

Wie bei anderen numerischen Problemen auch, steckt der Teufel im Detail. Und wie bei jedem detailorientierten Handwerk ist die geheime
Zutat hier Geduld. Hier ist unser Vorschlag f√ºr den Arbeitsablauf, wenn Sie auf diese Art von Problemen sto√üen:
1. Lokalisieren Sie die Quelle der Abweichungen. Das Modell, das Sie konvertieren, hat wahrscheinlich bis zu einem gewissen Punkt nahezu identische innere Variablen.
   bestimmten Punkt. Platzieren Sie `Breakpoint()`-Anweisungen in den Architekturen der beiden Frameworks und vergleichen Sie die Werte der
   numerischen Variablen von oben nach unten, bis Sie die Quelle der Probleme gefunden haben.
2. Nachdem Sie nun die Ursache des Problems gefunden haben, setzen Sie sich mit dem ü§ó Transformers-Team in Verbindung. Es ist m√∂glich
   dass wir ein √§hnliches Problem schon einmal gesehen haben und umgehend eine L√∂sung anbieten k√∂nnen. Als Ausweichm√∂glichkeit k√∂nnen Sie beliebte Seiten
   wie StackOverflow und GitHub-Probleme.
3. Wenn keine L√∂sung in Sicht ist, bedeutet das, dass Sie tiefer gehen m√ºssen. Die gute Nachricht ist, dass Sie das Problem gefunden haben.
   Problem ausfindig gemacht haben, so dass Sie sich auf die problematische Anweisung konzentrieren und den Rest des Modells ausblenden k√∂nnen! Die schlechte Nachricht ist
   dass Sie sich in die Quellimplementierung der besagten Anweisung einarbeiten m√ºssen. In manchen F√§llen finden Sie vielleicht ein
   Problem mit einer Referenzimplementierung - verzichten Sie nicht darauf, ein Problem im Upstream-Repository zu √∂ffnen.

In einigen F√§llen k√∂nnen wir nach R√ºcksprache mit dem ü§ó Transformers-Team zu dem Schluss kommen, dass die Behebung der Abweichung nicht machbar ist.
Wenn die Abweichung in den Ausgabeschichten des Modells sehr klein ist (aber m√∂glicherweise gro√ü in den versteckten Zust√§nden), k√∂nnen wir
k√∂nnten wir beschlie√üen, sie zu ignorieren und das Modell zu verteilen. Die oben erw√§hnte CLI `pt-to-tf` hat ein `--max-error`
Flag, um die Fehlermeldung bei der Gewichtskonvertierung zu √ºberschreiben.
